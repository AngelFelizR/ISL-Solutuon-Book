---
format:
  html:
    number-depth: 3
    css: summary-format.css
---
# Non-parametric Methods

## K-nearest neighbors (KNN) 

It uses the principle of nearest neighbors to classify unlabeled examples by using the **Euclidean Distance** to calculate distance between the point we want to predict and $k$ closest neighbors on the training data.

$$
 d\left( a,b\right)   = \sqrt {\sum _{i=1}^{p}  \left( a_{i}-b_{i}\right)^2 }
$$

KNN unlike parametric models does not tell us which predictors are important, making it hard to make inferences using this model.

This method performs worst than a parametric as we starting adding *noise* predictors. In fact, we will get in the situation where for a given observation has no *nearby neighbors*, known as **curse of dimensionality** and leading to a very poor prediction of $f(x_{0})$.

### Classiﬁer

The next function estimates the conditional probability for class $j$ as the fraction of points in $N_{0}$ whose response values equal $j$.

$$
\text{Pr}(Y = j|X = x_{0}) = \frac{1}{K} 
                      \displaystyle\sum_{i \in N_{0}} I(y_{i} = j)
$$

- Where
  - $j$ response value to test
  - $x_{0}$ is the test observation
  - $K$ the number of points in the training data that are closest to $x_{0}$ and reduce the model flexibility
  - $N_{0}$ points in the training data that are closest to $x_{0}$
  
Then KNN classiﬁes the test observation $x_{0}$ to the class with the largest probability.

![](img/08-knn-classifier.png){fig-align="center"}

### Regression

KNN regression estimates $f(x_{0})$ using the average of all the training responses in $N_{0}$.
 
$$
\hat{f}(x_{0}) = \frac{1}{K} 
                      \displaystyle\sum_{i \in N_{0}} y_{i}
$$

- Where
  - $x_{0}$ is the test observation
  - $K$ the number of points in the training data that are closest to $x_{0}$ and reduce the model flexibility
  - $N_{0}$ points in the training data that are closest to $x_{0}$
  
### Pre-processing

To use this method we need to make sure that all our variables are numeric. If one our variables is a factor we need to perform a dummy transformation of that variable with the `recipes::step_dummy` function.

On the other hand, as this model uses distances to make predicts it's important to check that each feature of the input data is measured with **the same range of values** with the `recipes::step_range` function which normalize from 0 to 1 as happens with the dummy function.

$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

Another normalization alternative is centering the predictors in $\overline{x} = 0$ with $S = 0$ with the function `recipes::step_normalize` or the function `scale()` which apply the [z-score normalization](https://developers.google.com/machine-learning/data-prep/transform/normalization).

$$
x' = \frac{x - \mu}{\sigma}
$$

### Coding example

To perform **K-Nearest Neighbors** we just need to create the model specification by using **kknn** engine.

```{r}
library(tidymodels)
library(ISLR2)

Smarket_train <- 
  Smarket %>%
  filter(Year != 2005)

Smarket_test <- 
  Smarket %>%
  filter(Year == 2005)

knn_spec <- nearest_neighbor(neighbors = 3) %>%
  set_mode("classification") %>%
  set_engine("kknn")

SmarketKnnPredictions <-
  knn_spec %>%
  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |>
  augment(new_data = Smarket_test) 

conf_mat(SmarketKnnPredictions, truth = Direction, estimate = .pred_class) 

accuracy(SmarketKnnPredictions, truth = Direction, estimate = .pred_class) 

```


## Tree-Based Methods

These methods involve **stratifying the predictor** space into a number of *simple regions* and then use mean or the mode response value for the training observations in the region to which it belongs.

![](img/48-Hitters-salary-regression-tree-regions.png){fig-align="center"}

As these results can be summarized in a tree, these types of approaches are known as **decision tree** methods, we have some important parts:

- *Terminal nodes* (*leaves*) are represented by $R_1$, $R_2$ and $R_3$.
- *Internal nodes* refers to the points along the tree where the predictor space is split.
- *Branches* refer to the segments of the trees that connect the nodes.

![](img/47-Hitters-salary-regression-tree.png){fig-align="center"}

It's important to take in consideration that the order in which is presented each predictors also explain the level of importance of each variable. For example, the number of `Years` has a higher effect over the player's salary than the number of `Hits`.

### Simple trees

#### Advantages and disadvantages

|Advantages|Disadvantages|
|:---------|:------------|
|Simpler to explain than linear regression thanks to its  graphical representation|Small change in the data can cause a large change in the final estimated tree|
|It doesn't need much preprocessing as: <br> - It handles qualitative predictors. <br> - It doesn't require feature scaling or normalization. <br> - It can handle missing values and outliers|They aren't so very good predicting results as: <br> - It prones to overfitting. <br> - It presents low accuracy.|
|It can be used for **feature selection** by defining the importance of a feature based on **how early it appears** in the tree and **how often** it is used for splitting|They can be biased towards the majority class in imbalanced datasets|

#### Regression

To create a decision tree we need to find the regions $R_1, \dots, R_j$ that minimize the RSS where $\hat{y}_{R_j}$ represent the mean response for the training observations within the *j*th box:

$$
RSS = \sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
$$

To define the regions we use the **recursive binary splitting**, which consist the predictor $X_j$ and the cutpoint $s$ leads to the greatest possible reduction in RSS. Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. The process continues until a *stopping criterion is reached* (no region contains more than five observations).

This method is:

- *Top-down*: It begins at the *top of the tree* (where all observations belong to a single region) and then successively splits the predictor space.

- *Greedy*: At each step of the tree-building process, **the best split is made at that particular step**, rather than looking ahead and picking a split that will lead to a better tree in some future step.

As result, we could end with a very complex tree that **overfits** the data. To solve this, we need **prune** the original tree until getting a **subtree** that leads to the lowest test error rate by using the **cost complexity pruning** approach which creates differente trees based on $\alpha$.


$$
\sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_m}) ^2  + \alpha|T|
$$

Where:

- $\alpha$: Tunning parameter $[0,\infty]$ selected using *k-cross validation*
- $|T|$: Number of terminal nodes of the tree $T$.
- $R_m$: The subset of predictor space corresponding to the $m$th terminal node
- $\hat{y}_{R_m}$: Predicted response associated with $R_m$


![](img/49-tree-best-number-leaves.png){fig-align="center"}

#### Classification

For a classification tree, we predict that each observation belongs to the most *commonly occurring class* of training observations in the region to which it belongs.

As we can not use RSS as a criterion for making the binary splits, the **classification error rate** could the fraction of the training observations in that region that do not belong to the most common class ($1 - \max_k(\hat{p}_{mk})$), but it turns out that classification error is not sufficiently sensitive for tree-growing and we use the next metrics as they are more sensitive to **node purity** (*proportion of the main class on each terminal node*):

|Name|Formula|
|:---|:-----:|
|**Gini index**| $G = \sum_{k = 1}^K 1 - \hat{p}_{mk}  (1 -\hat{p}_{mk})$|
|**Entropy**| $D = -\sum_{k = 1}^K 1 - \hat{p}_{mk}  \log \hat{p}_{mk}$|

![](img/50-tree-classification-example.png){fig-align="center"}

#### Coding example

https://app.datacamp.com/learn/tutorials/decision-trees-R

```{r}
# For data maninulation
library(data.table)

# For modeling and visualization
library(tidymodels)

#
Boston <- as.data.table(MASS::Boston)

pillar::glimpse(Boston)
```


### Bagging (bootstrap aggregation)

As we said before, simple trees has a *high variance* problem $Var(\hat{f}(x_{0}))$ and **bagging** can help to mitigate this problem.

We know from the *Central Limit Theorem* a natural way to *reduce the variance* and *increase the test set accuracy* is taking many **training sets from the population**, build a separate prediction model using each training set, and average the resulting prediction, as for a given a set of $n$ **independent observations** $Z_1, \dots, Z_n$, each with variance $\sigma^2$, the variance of the mean $\overline{Z}$ of the observations is given by $\sigma^2/n$.

![](img/66-bagging-concept.png){fig-align="center"}


As we generally do not have access to multiple training sets we use **bootstrap** to take repeated samples from the one training data set, train $B$ **not pruned regression trees** and **average** the resulting predictions or select the most commonly occurring class among the $B$ predictions in classification settings.

$$
\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)
$$

The  **number of trees is not a critical** as $B$ will not lead to overfitting. Using $B = 100$ is sufficient to achieve good performance in this example.

#### Out-of-bag error estimation

To estimate the test error as an approximation of the *Leave-one-out cross validation* when $B$ sufficiently large, we can take advantage of the $1/3$ of observation that were **out-of-bag** (OOB) on each re-sample and  predict the response for the $i$th observation using each of the trees in which that observation was OOB. 

This will yield around $B/3$ predictions for each of the $n$ observation that we can *average* or take a *majority* vote to calculate the *test error*.


#### Variable importance measures

After using this method, we can’t represent the statistical learning procedure using a single tree, instead we can use the *RSS* (or the *Gini index*) to record the total amount that the RSS is decreased due to splits over a given predictor, averaged(or added) over all $B$ trees where a large value indicates an important predictor.

![](img/51-bagging-variable-importance.png){fig-align="center"}

### Random Forests

Predictions from the bagged trees, has a big problem, there are **highly correlated** and averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many **uncorrelated quantities** (*independent*).

To solve this problem **Random Forests**  provide an improvement over bagged trees by **decorrelates the trees**. As in bagging, we build many trees based on bootstrapped training samples. But when building these decision trees *random forest* sample $m \approx \sqrt{p}$ predictors to create $B$ *independent trees, making the average of the resulting trees less variable and hence more reliable.

![](img/52-random-forest-effect-over-bagging.png){fig-align="center"}

#### Parameters to tune

Random forests have the least variability in their prediction accuracy when tuning, as the **default values tend to produce good results**.

1. **Number of trees** ($B$): A good rule of thumb is to **start with 10 times the number of features** as the error estimate converges after some trees and computation time increases linearly with the number of trees.

![](img/71-ramdom-forest-tuning-number-of-trees.png){fig-align="center"}

2. **Number of predictors** ($m_{try}$): In `ranger`, $m_{try} = \text{floor} \left( \frac{p}{3} \right)$ in regression problems and $m_{try} = \text{floor} \left( \sqrt{p} \right)$ in classifications problems, but we can explore in the range $[2,p]$.
  - With **few** relevant predictors (e.g., noisy data) a **higher number tends to perform better** because it makes it more likely to *select those features with the strongest signal*.
  - With **many** relevant predictors a **lower number might perform better**.

![](img/72-ramdom-forest-tuning-number-of-predictors.png){fig-align="center"}

3. **Tree complexity** (*node size*): The default values of $1$ for classification and $5$ for regression as these values tend to produce good results, but:
  - If your data has **many noisy predictors** and a **high number of trees**, then performance may improve by **increasing node size** (i.e., decreasing tree depth and complexity).
  - If **computation time is a concern** then you can often decrease run time substantially by **increasing the node size**.

Start with three values between 1–10 and adjust depending on impact to accuracy and run time.

![](img/73-ramdom-forest-tuning-node-size.png){fig-align="center"}


### Boosting

As well as **bagging**, **boosting** is a general approach that can be be applied to many statistical learning methods for regression or classification. Both methods create many models in order to create a single prediction $\hat{f}^1, \dots, \hat{f}^B$.

To create boosting trees, in general, we need to create then ***sequentially*** by using information from prior models and fit a new model with a modified version of the original data set. To be more specific we need to flow the following steps:

1. Set $\hat{f}(x) = 0$ and $r_i=y_i$ for all $i$ in the training set.

2. For $b = 1, 2, \dots, B$, repeat the next process:
  - Fit a tree $\hat{f}^b$ with d splits (d + 1 terminal nodes) to the training data ($X,r$). In this step is very import to see that we are fitting the new model based on the residuals.
  - Update $\hat{f}$ by adding in a shrunken version of the new tree:
  
$$
\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)
$$

  - Update the residuals

$$
r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)
$$

3. Calculate the output of the boosting model by:

$$
\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)
$$

As result, our model *learn slowly* by adding new decision trees into the fitted function in order to update the residuals on each step. As we are going to use many models, each individual model can be small by using a low $d$ parameter. In general, statistical learning approaches that learn slowly tend to
perform well.

#### Parameters to tune

- **Number of trees** ($B$): Unlike bagging and random forests, boosting can overfit if B is too large.

- **shrinkage** ($\lambda$): Controls the rate at which boosting learns, it should be a positive value its values are $0.01$ or $0.001$. Very small $\lambda$ can require using a very large value of B in order to achieve good performance.

- **Number of splits or interaction depth** ($d$): It controls the complexity of the boosted ensemble. When $d=1$ is known as a **stump tree** as each term involves only *a single variable*. Some times, stump tree works well and are eraser to interpret, but as $d$ increases the *number of variables* used by each model increases, with $d$ as limit.

![](img/53-boosting-vs-random-forest.png){fig-align="center"}

### Bayesian additive regression trees (BART)

This method constructs trees in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.

To understand the method works we need to define some important notation:

- $K$: Number of regression trees. For example $K$ could be $100$
- $B$: Number of iterations. For example $B$ could be $1000$
- $\hat{f}_k^b(x)$: The prediction at $x$ for the $k$th regression tree used in the $b$th iteration
- $\hat{f}^b(x) = \sum_{k=1}^K \hat{f}_k^b(x)$: Summed of the $K$ at the end of each iteration.

To apply this method we need to follow the below steps:

1. In the first iteration all trees are initialized to have a single root node,*the mean of the response values divided by the total number of trees*, in other to predict the mean of $y$ in the first iteration $\hat{f}^1(x)$.

$$
\hat{f}_k^1(x) = \frac{1}{nK} \sum_{i=1}^n y_i
$$

2. Compute the predictions of the first iteration.

$$
\hat{f}^1(x) = \sum_{k=1}^K \hat{f}_k^1(x) = \frac{1}{n} \sum_{i=1}^n y_i
$$

3. For each of the following iterations $b = 2, \dots, B$.

  - Update each tree $k = 1, 2, \dots, K$ by:

    - Computing a **partial residual** for each tree with all trees but the $k$th tree.
  
$$
r_i = y_i - \sum_{k'<k} \hat{f}_{k'}^b(x_i) - \sum_{k'>k} \hat{f}_{k'}^{b-1}(x_i)
$$

  -
    - Based on the *partial residual* BART **randomly choosing a perturbation to the tree** from the previous iteration $\hat{f}_k^{b-1}$ from a set of possible perturbations (adding branches, prunning branches or changing the prediction of terminal nodes) favoring ones that improve the fit to the partial residual. This guards against overfitting since it limits how “hard” we fit the data in each iteration.
    
    
  - Compute $\hat{f}^b(x) = \sum_{k=1}^K \hat{f}_k^b(x)$
  
4. Compute the *mean* or a *percentile* after $L$ burn-in iterations that don't provide good results. For example $L$ could be $200$

$$
\hat{f}(x) = \frac{1}{B-L} \sum_{b=L+1}^B \hat{f}(x)
$$

This models works really well even without a tuning process and the random process modifications protects the metho to overfit as we increase the number of iterations, as we can see in the next chart.

![](img/54-bart-vs-boosting-trees.png){fig-align="center"}


## Support Vector Machine



### Maximal Margin Classifier

In a p-dimensional space, a **hyperplane** is a flat affine subspace of hyperplane dimension $p − 1$.

$$
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0
$$

But if a point $X = (X_1, X_3, \dots, X_p)^T$ doesn't satisfy that equation that equation the point would lies to one or other side the equation:
  - Over the *hyperplane* if $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p > 0$
  - Under the *hyperplane* if $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p < 0$
  
But as we can see below there are several possible *hyperplane* that could do the job.

![](img/97-hyperplane.png){fig-align="center"}

To solve this problem, we need to find the **maximal margin hyperplane** by computing the perpendicular distance from each training observation to a given separating hyperplane to select the farthest *hyperplane* from the training observations.

![](img/98-maximal-margin-hyperplane.png){fig-align="center"}

In the last example the two blue points and the purple point that lie on the dashed lines are the **support vectors** as they "support" the maximal margin hyperplane.

The maximal margin hyperplane is the solution to the next optimization problem where the **associated class labels** $y_1, \dots, y_n \in \{-1, 1\}$:

$$
\begin{split}
\underset{\beta_0, \beta_1, \dots, \beta_p, M}{\text{maximize}} & \; M \\
\text{subject to } \sum_{j=1}^p \beta_{j}^2  & = 1 , \\
y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) & \geq M \; \; \forall \; i = 1, \dots, n
\end{split}
$$

This method has two disadvantages:

1. Sometimes there isn't any possible *separating hyperplane*.

![](img/99-no-posible-hyperplane.png){fig-align="center"}

2. Classifying correctly all of the training can lead to sensitivity to individual observations, returning as a result a overfitted model.

![](img/100-hyperplane-sensitivity.png){fig-align="center"}

### Support Vector Classifiers (soft margin classifier)

To solve this problem we need to allow that:

- Some observations will be on the incorrect side of the *margin* like observations 1 and 8.
- Some observations will be on the incorrect side of the *hyperplane* like observations 11 and 12.

![](img/101-soft-margin-classifier.png){fig-align="center"}

To get that result, we need to solve the next problem:

$$
\begin{split}
\underset{\beta_0, \beta_1, \dots, \beta_p, M}{\text{maximize}} & \; M \\
\text{subject to } \sum_{j=1}^p \beta_{j}^2  & = 1 , \\
y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) & \geq M (1 - \epsilon_i), \\
\epsilon_i \geq 0, \; \sum_{i=1}^n \epsilon_i \leq C,
\end{split}
$$

- Where:
  - $M$ is the width of the margin.
  - $\epsilon_1, \dots, \epsilon_n$ are slack variables that allow individual observations to be on the wrong side of the margin or the hyperplane. 
    - If $\epsilon_i = 0$ then the *i*th observation is on the **correct side of the margin**.
    - If $\epsilon_i > 0$ then the *i*th observation is on the **wrong side of the margin**.
    - If $\epsilon_i > 1$ then the *i*th observation is on the **wrong side of the hyperplane**.
  - $C$ is a nonnegative tuning parameter that represents the **budget for the amount that the margin can be violated** by the n observations. For $C > 0$ no more than $C$ observations can be on the wrong side of the hyperplane.

It's important to point that only observations that either **lie on the margin** or **violate the margin** will **affect the hyperplane**.
  
![](img/102-effects-of-changing-C.png){fig-align="center"}

### Non-linear boundaries

To extend the *Support Vector Classifier* to non-lineal settings we need to use functions that quantifies the similarity of two observations, known as **kernels** $K(x_i, x_{i'})$ and implement it to the **hyperplane** function for the *support vectors* $\mathcal{S}$.

$$
f(x) = \beta_0 + \sum_{i \in \mathcal{S}} \alpha_i K(x_i, x_{i'})
$$
And depending on the shape that we want to use there are 2 types of kernels to use.

#### Polynomial

As the degree $d$ increases the fit becomes more non-linear

$$
K(x_i, x_{i'}) = (1 + \sum_{j=1}^p x_{ij} x_{i'j})^d
$$

#### Radial

As $\gamma$ increases the fit becomes more non-linear.

$$
K(x_i, x_{i'}) = \exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i'j})^2)
$$

### Extending SVMs to the K-class case

To extend this method we have 2 alternatives

- **One-Versus-One Classification**: It constructs $\left( \begin{array}{c} K \\ 2 \end{array} \right)$, tallys the number of times that the test observation is assigned to each of the $K$. The final classification is performed by assigning the test observation to the class to which it was most frequently assigned.

- **One-Versus-All Classification**: We fit $K$ SVMs, each time comparing one of the $K$ classes to the remaining $K − 1$ classes. Let $\beta = \beta_{0k}, \beta_{1k}, \dots, \beta_{pk}$  denote the parameters that result from fitting an SVM comparing the $k$th class (coded as $+1$) to the others (coded as $−1$). We assign the observation to the class for which the lineal combination of coefficients and the test observation $\beta x^*$ is largest.


### Coding Example

1. Load libraries

```{r}
library(tidymodels)
library(ISLR)
library(kernlab)
theme_set(theme_light())

set.seed(1)
sim_data <- tibble(
  x1 = rnorm(40),
  x2 = rnorm(40),
  y  = factor(rep(c(-1, 1), 20))
) %>%
  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),
         x2 = ifelse(y == 1, x2 + 1.5, x2))

ggplot(sim_data, aes(x1, x2, color = y)) +
  geom_point()
```


2. Define model specification

```{r}
svm_linear_spec <- svm_poly(degree = 1) %>%
  set_mode("classification") %>%
  # We don't need scaling for now
  set_engine("kernlab", scaled = FALSE)
```

3. Fitting the model and checking results

```{r}
svm_linear_fit <- svm_linear_spec %>% 
  set_args(cost = 10) %>%
  fit(y ~ ., data = sim_data)

svm_linear_fit %>%
  extract_fit_engine() %>%
  plot()
```





