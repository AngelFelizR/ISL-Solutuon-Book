[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Statistical Learning (Non Official Solution Book)",
    "section": "",
    "text": "The purpose of creating this book is share the lessons learnt by reading and completing the exercises in the book.\nIt also could work as summary of the main point of the book."
  },
  {
    "objectID": "model-performance.html",
    "href": "model-performance.html",
    "title": "Understanding model performance",
    "section": "",
    "text": "The goal when we are analyzing data is to find a function that based on some Predictors and some random noise could explain the Response variable.\n\\[\nY = f(X) + \\epsilon\n\\]\n\\(\\epsilon\\) represent the random error and correspond to the irreducible error as it cannot be predicted using the Predictors in regression models. It would have a mean of 0 unless are missing some relevant Predictors.\nIn classification models, the irreducible error is represented by the Bayes Error Rate.\n\\[\n1 -  E\\left(\n     \\underset{j}{max}Pr(Y = j|X)\n     \\right)\n\\]\nAn error is reducible if we can improve the accuracy of \\(\\hat{f}\\) by using a most appropriate statistical learning technique to estimate \\(f\\).\nThe challenge to achieve that goal it’s that we don’t at the beginning how much of the error correspond to each type.\n\\[\n\\begin{split}\nE(Y-\\hat{Y})^2 & = E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\\n               & = \\underbrace{[f(X)- \\hat{f}(X)]^2}_\\text{Reducible} +\n                   \\underbrace{Var(\\epsilon)}_\\text{Irredicible}\n\\end{split}\n\\]\nThe reducible error can be also spitted in two parts:\n\nVariance refers to the amount by which \\(\\hat{f}\\) would change if we estimate it using a different training data set. If a method has high variance then small changes in the training data can result in large changes of \\(\\hat{f}\\).\nSquared bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model as for example a linear model.\n\n\\[\nE(y_{0} - \\hat{f}(x_{0}))^2 =\nVar(\\hat{f}(x_{0})) +\n[Bias(\\hat{f}(x_{0}))]^2 +\nVar(\\epsilon)\n\\]\n\n\n\n\n\n\nOur challenge lies in ﬁnding a method for which both the variance and the squared bias are low."
  },
  {
    "objectID": "model-performance.html#types-of-models",
    "href": "model-performance.html#types-of-models",
    "title": "Understanding model performance",
    "section": "Types of models",
    "text": "Types of models\n\nParametric methods\n\nMake an assumption about the functional form. For example, assuming linearity.\nEstimate a small number parameters based on training data.\nAre easy to interpret.\nTend to outperform non-parametric approaches when there is a small number of observations per predictor.\n\nNon-parametric methods\n\nDon’t make an assumption about the functional form, to accurately ﬁt a wider range of possible shapes for \\(f\\).\nNeed a large number of observations in order to obtain an accurate estimate for \\(f\\).\nThe data analyst must select a level of smoothness (degrees of freedom)."
  },
  {
    "objectID": "model-performance.html#evaluating-model-performance",
    "href": "model-performance.html#evaluating-model-performance",
    "title": "Understanding model performance",
    "section": "Evaluating model performance",
    "text": "Evaluating model performance\nTo evaluate how good works a models we need to split the available data in two parts.\n\nTraining data: Used to fit the model.\nTest data: Used to confirm how well the model works with new data.\n\nSome measurements to evaluate our test data are:\n\nTest mean squared error (MSE)\n\n\\[\nAve(y_{0}-\\hat{f}(x_{0}))^2\n\\]\n\nTest error rate\n\n\\[\nI(y_{0} \\neq \\hat{y}_{0}) =\n\\begin{cases}\n    1 & \\text{If } y_{0} \\neq \\hat{y}_{0} \\\\\n    0 & \\text{If } y_{0} = \\hat{y}_{0}\n\\end{cases}\n\\]\n\\[\nAve(I(y_{0} \\neq \\hat{y}_{0}))\n\\]\n\nConfusion Matrix\n\nIt contracts the model’s outputs with real values and makes easier to calculate more metrics to validate the model.\n\n\n\n\n\n\nSome metrics related with the confussion matrix are:\n\nSensitivity: Represents the percentage of positive values that have been correctly identiﬁed \\(\\text{TP}/\\text{P}\\).\nSpecificity: Represents the percentage of negative values that have been correctly identiﬁed \\(\\text{TN}/\\text{N}\\).\n\n\nYou can more metrics in the next table.\n\n\n\n\n\nThe ROC (receiver operating characteristics) Curve displays the two types of errors for all possible thresholds. The area under the curve (AUC) the represent overall performance of a classiﬁer, summarized over all possible thresholds and as larger the AUC the better the classiﬁer."
  },
  {
    "objectID": "generalized-linear-models.html",
    "href": "generalized-linear-models.html",
    "title": "Generalized linear models (GLM)",
    "section": "",
    "text": "As the population regression line is unobserved the least squares line of a sample is a good estimation. To get it we need to follow the next steps:\n\nDefine the function to fit.\n\n\\[\n\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x\n\\]\n\nDefine how to calculate residuals.\n\n\\[\ne_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nDefine the residual sum of squares (RSS).\n\n\\[\nRSS = e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\n\\]\n\nUse calculus or make estimation with a computer to find the coefficients that minimize the RSS.\n\n\\[\n\\hat{\\beta}_{1} = \\frac{\\Sigma_{i=1}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})}\n                       {\\Sigma_{i=1}^{n}(x_{i}-\\overline{x})}\n, \\quad\n\\hat{\\beta}_{0} = \\overline{y} - \\hat{\\beta}_{1}\\overline{x}\n\\]\n\n\n\nTo estimate the population regression line we can calculate conﬁdence intervals for sample coefficients, to define a range where we can find the population values with a defined confidence level.\n\nIf we want to use 95% of confidence we need to know that after taking many samples only 95% of the intervals produced with this confident level would have the true value (parameter).\n\nTo generate confident intervals we would need to calculate the variance of the random error.\n\\[\n\\sigma^2 = Var(\\epsilon)\n\\]\nBut as we can not calculate that variance an alternative can be to estimate it based on residuals if they meet the next conditions:\n\nEach residual have common variance \\(\\sigma^2\\), so the variances of the error terms shouldn’t have any relation with the value of the response.\nResiduals are uncorrelated. For example, if \\(\\epsilon_{i}\\) is positive, that provides little or no information about the sign of \\(\\epsilon_{i+1}\\).\n\nIf not, we would end underestimating the true standard errors, reducing the probability a given confident level to contain the true value of the parameter and underrating the p-values associated with the model.\n\\[\n\\sigma \\approx RSE = \\sqrt{\\frac{RSS}{(n-p-1)}}\n\\]\nNow we can calculate the standard error of each coefficient and calculate the confident intervals.\n\\[\nSE(\\hat{\\beta_{0}})^2 = \\sigma^2\n                       \\left[\\frac{1}{n}+\n                             \\frac{\\overline{x}^2}\n                                  {\\Sigma_{i=1}^{n} (x_{i}-\\overline{x})^2}\n                       \\right]\n\\]\n\\[\nSE(\\hat{\\beta_{1}})^2 = \\frac{\\sigma^2}\n                             {\\Sigma_{i=1}^{n} (x_{i} - \\overline{x})^2}\n\\]\n\\[  \n\\hat{\\beta_{1}} \\pm 2 \\cdot SE(\\hat{\\beta_{1}}), \\quad \\hat{\\beta_{0}} \\pm 2 \\cdot SE(\\hat{\\beta_{0}})\n\\]\n\n\n\n\n\nUse the regression overall P-value (based on the F-statistic) to confirm that at least one predictor is related with the Response and avoid interpretative problems associated with the number of observations (n) or predictors (p).\n\\[\nH_{0}: \\beta_{1} = \\beta_{2} = \\dots = \\beta_{p} = 0\n\\]\n\\[\nH_{a}: \\text{at least one } \\beta_{j} \\text{ is non-zero}\n\\]\n\n\n\nIf we want to know how well the model fits to the data we have two options:\n\nResidual standard error (RSE): Even if the model were correct, the actual values of \\(\\hat{y}\\) would differ from the true regression line by approximately this units, on average. To get the percentage error we can calculate \\(RSE/\\overline{x}\\)\nThe \\(R^2\\) statistic: The proportion of variance explained by taking as a reference the total sum of squares (TSS).\n\n\\[\nTSS = \\Sigma(y_{i} - \\overline{y})^2\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS}\n\\]\n\\[\nR^2 =\n\\begin{cases}\n    Cor(X, Y)^2  & \\text{Simple Lineal Regresion} \\\\\n    Cor(Y,\\hat{Y})^2 & \\text{Multipline Lineal Regresion}\n\\end{cases}\n\\]\n\n\n\nTo answer that we can test if a particular subset of q of the coefficients are zero.\n\\[\nH_{0}: \\beta_{p-q+1} = \\beta_{p-q+2} = \\dots = \\beta_{p} = 0\n\\]\nIn this case, F-statistic reports the partial eﬀect of adding a extra variable to the model (the order matters) to apply a variable selection technique. The classical approach is to:\n\nFit a model for each variable combination \\(2^p\\).\nSelect the best model based on Mallow’s Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted \\(R^2\\) or plot various model outputs, such as the residuals, in order to search for patterns.\n\nBut just think that if we have \\(p = 30\\) we will have \\(2^{30} = =1,073,741,824\\ models\\) to fit, that it’s too much. Some alternative approaches for this task:\n\nForward selection\nBackward selection (cannot be used if p >n)\nMixed selection\n\n\n\n\nTo check that we need to see the \\(\\hat{\\beta}_{j}\\) confident intervals as the real \\(\\beta_{j}\\) is in that range.\n\n\n\nIf we want to predict the average response \\(f(X)\\) we can use the confident intervals, but if we want to predict an individual response \\(Y = f(X) + \\epsilon\\) we need to use prediction intervals as they account for the uncertainty associated with \\(\\epsilon\\), the irreducible error.\n\n\n\n\n\nThe additivity assumption means that the association between a predictor \\(X_{j}\\) and the response \\(Y\\) does not depend on the values of the other predictors, as it happens when there is a interaction (synergy) effect\nThe linearity assumption states that the change in the response Y associated with a one-unit change in \\(X_{j}\\) is constant, regardless of the value of \\(X_{j}\\).\n\n\n\nThis approach relax the additivity assumption that models usually have.\n\n2 quantitative variables\n\nIt consist in adding an extra coefficient which multiplies two or more variables.\n\\[\n\\begin{split}\nY & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\beta_{3} X_{1} X_{2} + \\epsilon \\\\\n  & = \\beta_{0} + (\\beta_{1} + \\beta_{3} X_{2}) X_{1} + \\beta_{2} X_{2} + \\epsilon \\\\\n  & = \\beta_{0} + \\tilde{\\beta}_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon\n\\end{split}\n\\]\nAfter adding the interaction term we could interpret the change as making one of the original coefficient a function of the another variable. Now we could say that \\(\\beta_{3}\\) represent the change of \\(X_{1}\\) effectiveness associated with a one-unit increase in \\(X_{2}\\).\nIt very important that we keep hierarchical principle, which states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant as it would alter the meaning of the interaction.\n\n1 quantitative and 1 qualitative variable\n\nIf \\(X_{1}\\) is quantitative and \\(X_{2}\\) is qualitative:\n\\[\n\\hat{Y} =\n\\begin{cases}\n    (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3})X_{1} & \\text{if }X_{2} \\text{ is TRUE}\\\\\n    \\beta_{0} + \\beta_{1}X_{1}                             & \\text{if }X_{2} \\text{ is FALSE}\n\\end{cases}\n\\]\nAdding the \\(\\beta_{3}\\) interaction allow the line to change the line slope based on \\(X_{2}\\) and not just a different intercept.\n\n\n\n\n\n\n\n\nThis approach relax the linearity assumption that models usually have. It consist in including transformed versions of the predictors.\n\\[\n\\begin{split}\nY & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} \\\\\n  & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{1}^2\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n\n\nPlot the residuals versus predicted values \\(\\hat{y}_{i}\\). Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\nA simple approach is to use non-linear transformations of the predictors, such as \\(\\log{X}\\), \\(\\sqrt{X}\\), and \\(X^2\\), in the regression model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n\n\n1. Plot the residuals from our model as a function of time or execution order. If the errors are uncorrelated, then there should be no discernible pattern.   2. Check if some observation have been exposed to the same environmental factors\nGood experimental design is crucial in order to mitigate these problems\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n\n\nPlot the residual plot en check if you can see a funnel shape\nWe can transform the response using a concave function such as \\(\\log{Y}\\) or \\(\\sqrt{Y}\\)\n\n\n\n\n\n\n\n\n\n\n\nAn outlier is a point for which \\(y_{i}\\) is far from the value predicted by the model. Sometimes, they have little effect on the least squares line, but over estimate the RSE making bigger p-values of the model and under estimate the \\(R^2\\).\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n\n\nPlot the studentized residuals, computed by dividing each residual \\(e_{i}\\) by its estimated standard error. Then search for points which absolute value is greater than 3\nThey can be removed if it has occurred due to an error in data collection. Otherwise, they may indicate a deficiency with the model, such as a missing predictor.\n\n\n\n\n\n\n\n\n\n\n\nObservations with high leverage have an unusual value for \\(x_{i}\\). High leverage observations tend to have a sizable impact on the estimated regression line and any problems with these points may invalidate the entire fit.\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n\n\nCompute the leverage statistic. Find an observation with higher value than mean, represented by \\((p + 1)/n\\). Leverage values are always between \\(1/n\\) and \\(1\\)\nMake sure that the value is correct and not a data collection problem\n\n\n\n\\[\nh_{i} = \\frac{1}{n} +\n        \\frac{(x_{i} - \\overline{x})^2}\n              {\\Sigma_{i'=1}^n(x_{i'} - \\overline{x})^2}\n\\]\nIn a multiple linear regression, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors.\n\n\n\n\n\n\n\n\nCollinearity refers to the situation in which two or more predictor variables are closely related (highly correlated) to one another. It reduces the accuracy of the estimates of the regression coeﬃcients and causes the standard error for \\(\\hat{\\beta}_{j}\\) to grow. That reduce the power of the hypothesis test,that is, the probability of correctly detecting a non-zero coeﬃcient.\nLooking at the correlation matrix of the predictors could be usefull, but it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation (multicollinearity).\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n\n\nThe best way to assess multicollinearity is to compute the variance inﬂation factor (VIF), which is the ratio of the variance of \\(\\hat{\\beta}_{j}\\) when ﬁtting the full model divided by the variance of \\(\\hat{\\beta}_{j}\\) if ﬁt on its own with 1 as its lowest value and 5 or 10 as problematic values of collinearity\n1. Drop one of the problematic variables from the regression.   2. Combine the collinear variables together into a single predictor\n\n\n\n\\[\n\\text{VIF}(\\hat{\\beta}_{j}) = \\frac{1}\n                                   {1 - R_{X_{j}|X_{-j}}^2}\n\\]\nWhere \\(R_{X_{j}|X_{-j}}^2\\) is the \\(R^2\\) from a regression of \\(X_{j}\\) onto all of the other predictors.\n\n\n\n\nThere are better model to achieve that kind of situation. For example, he linear discriminant analysis (LDA) procedure the same response of a linear regression for a binary problem. Other reasons are:\n\nA regression method cannot accommodate a qualitative response with more than two classes.\nA regression method will not provide meaningful estimates of \\(Pr(Y|X)\\) as some of our estimates might be outside the [0, 1] interval."
  },
  {
    "objectID": "generalized-linear-models.html#poisson-regression",
    "href": "generalized-linear-models.html#poisson-regression",
    "title": "Generalized linear models (GLM)",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nIf \\(Y \\in \\{ 0, 1, 2, 3, \\dots \\}\\) that could be the result after counting a particular event the linear regression might not meet our needs as it could bring negative numbers. The Poisson Distribution follow the next function:\n\\[\nPr(Y = k) = \\frac{e^{-\\lambda} \\lambda^{k}}\n                 {k!}\n\\]\nWhere: - \\(\\lambda\\) must be greater than 0. It represents the expected number of events \\(E(Y)\\) and variance related \\(Var(Y)\\) - \\(k\\) represent the number of events that we want to evaluate base of \\(\\lambda\\). Its numbers should be greater or equal to 0.\nSo, it makes sense that the value that we want to predict with our regression would be \\(\\lambda\\), by using next structure:\n\\[\n\\log{ \\left( \\lambda(X_1, X_2, \\dots , X_p)  \\right)} = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\nIf select this model we need to be aware how to interpret the coefficients. For example, if \\(\\beta_1 = -0.08\\) for a categorical variable, we can conclude by calculating \\(e^{-0.08}\\) that 92.31% of events of the base line related to \\(\\beta_0\\) would happen."
  },
  {
    "objectID": "generalized-linear-models.html#logistic-regression",
    "href": "generalized-linear-models.html#logistic-regression",
    "title": "Generalized linear models (GLM)",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIt models the probability (\\(p(X) = Pr(Y=1|X)\\)) that Y belongs to a particular category given some predictors by assuming that \\(Y\\) follows a Bernoulli Distribution. This model calculates the probability using the logistic function which produce a S form between 0 and 1:\n\\[\np(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X}}\n            {1+e^{\\beta_{0}+\\beta_{1}X}}\n\\]\n\n\n\n\n\nAs the functions returns probabilities is responsibility of the analyst to define a threshold to make classifications.\n\nEstimating coefficients\nTo estimate \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) the method used is called as maximum likelihood which consists in maximizing the likelihood function. It is important to clarify that the least squares approach is in fact a special case of maximum likelihood.\n\\[\n\\ell(\\beta_{0}, \\beta_{1}) = \\prod_{i:y_{i} = 1}p(x_{i})\\prod_{i':y_{i'} = 0}p(1-x_{i'})\n\\]\n\n\nMultiple regression\nWe also can generalize the logistic function as you can see bellow.\n\\[\np(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}\n            {1+e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}\n\\]\n\n\nInterpreting the model\nTo understand how each variable influence the probability \\(p(X)\\), we need to manipulate the logistic function until having a lineal combination on the right site.\n\\[\n\\underbrace{ \\log{ \\left( \\overbrace{\\frac{p(X)}{1 - p(X)}}^\\text{odds ratio} \\right)} }_\\text{log odds or logit} = \\beta_{0}+\\beta_{1}X\n\\]\nAs we can see, the result of the linear combination is the \\(\\log\\) of the odds ratio, known as log odd or logit.\nAn odds ratio of an event presents the likelihood that the event will occur as a proportion of the likelihood that the event won’t occur. It can take any value between \\(0\\) and \\(\\infty\\), where low probabilities are close to \\(0\\), higher to \\(\\infty\\) and equivalents ones are equals to 1. For example, if we have an \\(\\text{odds ratio} = 2\\), we can say that it’s 2 times more likely that the event happens rather than not.\nApplying \\(\\log{(\\text{odds ratio})}\\) makes easier to compare the effect of variables as values below 1 become negative numbers of the scale of possible numbers and 1 becomes 0 for non-significant ones. To have an idea, an odds ratio of 2 has the same effect as 0.5, which it’s hard to see at first hand, but if we apply the \\(\\log\\) to each value we can see that \\(\\log{(2)} = 0.69\\) and \\(\\log{(0.5)} = -0.69\\).\nAt end, \\(p(X)\\) will increase as \\(X\\) increases if \\(\\beta_{1}\\) is positive despite the relationship between each other isn’t a linear one.\n\nUnderstanding a confounding paradox\n\n\n\n\n\n\n\nSimple Regression\nMultiple Regression\n\n\n\n\n\n\n\n\nThe positive coeﬃcient for student indicates that for over all values of balance and income, a student is more likely to default than a non-student.\nThe negative coeﬃcient for student indicates that for a ﬁxed value of balance and income, a student is less likely to default than a non-student.\n\n\n\n\n\n\n\n\nThe problem relays on the fact that student and balance are correlated. In consequence, a student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!\n\n\n\n\n\n\n\n\nMultinomial Logistic Regression\nWe also can generalize the logistic function to support more than 2 categories (\\(K > 2\\)) by defining by convention the last category \\(K\\) as a baseline.\nFor \\(k = 1, \\dotsc,K-1\\) we use function.\n\\[\nPr(Y = k|X= x) = \\frac{e^{\\beta_{k0}+\\beta_{k1}x_{1}+\\dots+\\beta_{kp}x_{p}}}\n                      {1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_{l1}x_{1}+\\dots+\\beta_{lp}x_{p}}}\n\\]\nFor \\(k=K\\), we use the function.\n\\[\nPr(Y = K|X= x) = \\frac{1}\n                      {1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_{l1}x_{1}+\\dots+\\beta_{lp}x_{p}}}\n\\] And after some manipulations we can show that \\(\\log\\) of the probability of getting \\(k\\) divided by the probability of the baseline is equivalent to a linear combinations of the functions parameters.\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|X= x)}{Pr(Y = K|X= x)} \\right)} = \\beta_{k0}+\\beta_{k1}x_{1}+\\dots+\\beta_{kp}x_{p}\n\\]\nIn consequence, each coefficient represent a measure of how much change the probability from the baseline probability.\n\n\nModel limitatios\nThere are models that could make better classifications when:\n\nThere is a substantial separation between the \\(Y\\) classes.\nThe predictors \\(X\\) are approximately normal in each class and the sample size is small.\nWhen the decision boundary is not lineal."
  },
  {
    "objectID": "generative-classification-models.html",
    "href": "generative-classification-models.html",
    "title": "Generative Models for Classiﬁcation",
    "section": "",
    "text": "These models instead of trying to predict the posterior probability ( \\(Pr(Y=k|X=x)\\) ) directly, they try to estimate the distribution of the predictors \\(X\\) separately in each of the response classes \\(Y\\) ( \\(f_{k}(X) = Pr(X|Y=k)\\) ). Then, they use the Bayes’ Theorem and the overall or prior probability \\(\\pi_{k}\\) (probability of a randomly chosen observation comes from the \\(k\\)th class) to flip these around into estimates for \\(Pr(Y=k|X=x)\\) by approximating the Bayes Classifier, which has the lowest total error rate.\n\\[\np_{k}(x) = Pr(Y = k | X = x) = \\frac{\\pi_{k} f_{k}(x)} {\\sum_{l=1}^{K} \\pi_{l} f_{l}(x)}\n\\]\nEstimating the prior probability can be as easy calculate \\(\\hat{\\pi}_{k} = n_{k}/ n\\) for each \\(Y\\) class by assuming that the trainning data its representative of the population, but estimating the density function of \\(X\\) for each class \\(f_{k}\\) it’s more challenging, so models need to make more simplifying assumptions to estimate it."
  },
  {
    "objectID": "generative-classification-models.html#linear-discriminant-analysis-lda",
    "href": "generative-classification-models.html#linear-discriminant-analysis-lda",
    "title": "Generative Models for Classiﬁcation",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\nThis model assumes that:\n\nThe density function of \\(X\\) for each \\(Y\\) class \\(f_{k}\\) follows a Normal (Gaussian) distribution within each class. Even though, it is often remarkably robust to model violations like Boolean variables.\n\\(X\\) has a different mean across all \\(Y\\) classes \\(\\mu_{1}^2 \\neq \\dots \\neq \\mu_{k}^2\\).\n\\(X\\) has a common variance across all \\(Y\\) classes \\(\\sigma_{1}^2 = \\dots = \\sigma_{k}^2\\).\n\nTo understand how the model calculates its parameters, let’s see the discriminant function when the number of predictors is \\(p=1\\) and the number of \\(Y\\) classes is \\(K=2\\).\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{ \\left( p_{x}(x) \\right)} \\\\\n              & = \\log{(\\pi_{k})}\n                - \\frac{\\mu_{k}^2}{2\\sigma^2}\n                + x \\cdot \\frac{\\mu_{k}}{\\sigma^2}\n\\end{split}\n\\]\nIn this function, it’s clear that a class \\(k\\) has more possibilities to be selected as mean of \\(x\\) for that particular class increases and its variance decreases. It is also important to take in consideration the effect of \\(\\log{(\\pi_{k})}\\), in consequence the proportion of classes also influence the results.\nIf we want to extend the model to work with \\(p \\geq 1\\) we also need to consider that:\n\nEach individual predictor follows a one-dimensional normal distribution\nThere is some correlation between each pair of predictors\n\nAs result, the discriminant function is:\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{\\pi_{k}}  - \\frac{1}{2} \\mu_{k}^T \\Sigma^{-1} \\mu_{k} \\\\\n                & \\quad + x^T \\Sigma^{-1} \\mu_{k}\n\\end{split}                      \n\\]\n\nWhere:\n\n\\(x\\) refers to a vector the current value of each \\(p\\) element.\n\\(\\mu\\) refers to a vector with the mean of each predictor.\n\\(\\Sigma\\) refers to the covariance matrix \\(p \\times p\\) of \\(\\text{Cov}(X)\\).\n\n\nThe model also can be extended to handle \\(K > 2\\) after defining the \\(K\\) class as the baseline, we can extend the discriminant function to have the next form:\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{ \\left(\n                        \\frac{Pr(Y = k|K=x)}\n                             {Pr(Y=K|X=x)}\n                      \\right)} \\\\\n              & = \\log{ \\left( \\frac{\\pi_{k}}{\\pi_{K}} \\right)}\n                  - \\frac{1}{2} (\\mu_{k} + \\mu_{K})^T \\Sigma^{-1} (\\mu_{k} - \\mu_{K}) \\\\\n              & \\quad + x^{T} \\Sigma^{-1} (\\mu_{k} - \\mu_{K})\n\\end{split}\n\\]"
  },
  {
    "objectID": "generative-classification-models.html#quadratic-discriminant-analysis-qda",
    "href": "generative-classification-models.html#quadratic-discriminant-analysis-qda",
    "title": "Generative Models for Classiﬁcation",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nLike LDA, the QDA classiﬁer plugs estimates for the parameters into Bayes’ theorem in order to perform prediction results and assumes that:\n\nThe observations from each class are drawn from a Gaussian distribution\nEach class has its own covariance matrix, \\(X \\sim N(\\mu_{k}, \\Sigma_{k})\\)\n\nUnder this assumption, the Bayes classiﬁer assigns an observation \\(X = x\\) to the class for which \\(\\delta_{k}(x)\\) is largest.\n\\[\n\\begin{split}\n\\delta_{k}(x) = & \\quad \\log{\\pi_{k}}\n                - \\frac{1}{2} \\log{|\\Sigma_{k}|}\n                - \\frac{1}{2} \\mu_{k}^T \\Sigma_{k}^{-1}\\mu_{k} \\\\\n              & + x^T \\Sigma_{k}^{-1} \\mu_{k} \\\\\n              & - \\frac{1}{2} x^T \\Sigma_{k}^{-1} x\n\\end{split}                  \n\\]\nIn consequence, QDA is more flexible than LDA and has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes or when we need non-linear decision boundaries.\nThe model also can be extended to handle \\(K > 2\\) after defining the \\(K\\) class as the baseline, we can extend the discriminant function to have the next form:\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|K=x)}{Pr(Y=K|X=x)} \\right)} =\na_k + \\sum_{j=1}^{p}b_{kj}x_{j} +\n      \\sum_{j=1}^{p} \\sum_{l=1}^{p} c_{kjl} x_{j}x_{l}\n\\]\nWhere \\(a_k\\), \\(b_{kj}\\) and \\(c_{kjl}\\) are functions of \\(\\pi_{k}\\), \\(\\pi_{K}\\), \\(\\mu_{k}\\), \\(\\mu_{K}\\), \\(\\Sigma_{k}\\) and \\(\\Sigma_{K}\\)"
  },
  {
    "objectID": "generative-classification-models.html#naive-bayes",
    "href": "generative-classification-models.html#naive-bayes",
    "title": "Generative Models for Classiﬁcation",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nTo estimate \\(f_{k}(X)\\) this model assumes that Within the kth class, the p predictors are independent (correlation = 0) and as consequence:\n\\[\nf_{k}(x) = f_{k1}(x_{1}) \\times f_{k2}(x_{2}) \\times \\dots \\times f_{kp}(x_{p})\n\\]\nEven thought the assumption might not be true, the model often leads to pretty decent results, especially in settings where \\(n\\) is not large enough relative to \\(p\\) for us to eﬀectively estimate the joint distribution of the predictors within each class.\nTo estimate the one-dimensional density function \\(f_{kj}\\) using training data we have the following options:\n\nWe can assume that \\(X_{j}|Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2)\\)\nWe can estimate the distribution by defining bins and creating a histogram\nWe can estimate the distribution by use a kernel density estimator\nIf \\(X_{j}\\) is qualitative, we can count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\n\nThe model also can be extended to handle \\(K > 2\\) after defining the \\(K\\) class as the baseline, we can extend the function to have the next form:\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|K=x)}{Pr(Y=K|X=x)} \\right)} =\n\\log{ \\left(\n        \\frac{\\pi_{k}}\n             {\\pi_{K}}\n      \\right)}\n+\n\\log{ \\left(\n        \\frac{\\prod_{j=1}^{p} f_{kj}(x_{j}) }\n             {\\prod_{j=1}^{p} f_{Kj}(x_{j}) }\n      \\right)}\n\\]"
  },
  {
    "objectID": "non-parametric-models.html",
    "href": "non-parametric-models.html",
    "title": "Non-parametric Methods",
    "section": "",
    "text": "This method performs worst than a parametric as we starting adding noise predictors. In fact, we will get in the situation where for a given observation has no nearby neighbors, known as curse of dimensionality and leading to a very poor prediction of \\(f(x_{0})\\).\nKNN unlike parametric models does not tell us which predictors are important, making it hard to make inferences using this model.\nIt also sensible the scale of the variables. Variables that are on a large scale will have a much larger eﬀect on the distance between the observations, and hence on the KNN classiﬁer, than variables that are on a small scale. To solve that problem we can standardize the data so that all variables are given a mean of zero and a standard deviation of one with the function scale().\n\n\nThe next function estimates the conditional probability for class \\(j\\) as the fraction of points in \\(N_{0}\\) whose response values equal \\(j\\).\n\\[\n\\text{Pr}(Y = j|X = x_{0}) = \\frac{1}{K}\n                      \\displaystyle\\sum_{i \\in N_{0}} I(y_{i} = j)\n\\]\n\nWhere\n\n\\(j\\) response value to test\n\\(x_{0}\\) is the test observation\n\\(K\\) the number of points in the training data that are closest to \\(x_{0}\\) and reduce the model flexibility\n\\(N_{0}\\) points in the training data that are closest to \\(x_{0}\\)\n\n\nThen KNN classiﬁes the test observation \\(x_{0}\\) to the class with the largest probability.\n\n\n\n\n\n\n\n\nKNN regression estimates \\(f(x_{0})\\) using the average of all the training responses in \\(N_{0}\\).\n\\[\n\\hat{f}(x_{0}) = \\frac{1}{K}\n                      \\displaystyle\\sum_{i \\in N_{0}} y_{i}\n\\]\n\nWhere\n\n\\(x_{0}\\) is the test observation\n\\(K\\) the number of points in the training data that are closest to \\(x_{0}\\) and reduce the model flexibility\n\\(N_{0}\\) points in the training data that are closest to \\(x_{0}\\)"
  },
  {
    "objectID": "02-execises.html",
    "href": "02-execises.html",
    "title": "02 - Statistical Learning",
    "section": "",
    "text": "For each of parts (a) through (d), indicate whether we would generally expect the performance of a ﬂexible statistical learning method to be better or worse than an inﬂexible method. Justify your answer.\n\n\nThe sample size n is extremely large, and the number of predictors p is small.\n\nBetter, flexible models reduce bias and quit the variance low when having a large n.\n\nThe number of predictors p is extremely large, and the number of observations n is small.\n\nWorse, a flexible model could increase its variance very high when having small n.\n\nThe relationship between the predictors and response is highly non-linear.\n\nBetter, a lineal model would have a very high bias in this case.\n\nThe variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high.\n\nWorse, a flexible model would over-fit trying to follow the irreducible error.\n\nExplain whether each scenario is a classiﬁcation or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\n\n\nWe collect a set of data on the top 500 ﬁrms in the US. For each ﬁrm we record proﬁt, number of employees, industry and the CEO salary. We are interested in understanding which factors aﬀect CEO salary.\n\nRegression, inference, 500, 4\n\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n\nClassification, prediction, 20, 14\n\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n\nRegression, prediction, 52, 4\n\nWe now revisit the bias-variance decomposition.\n\n\nProvide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less ﬂexible statistical learning methods towards more ﬂexible approaches. The x-axis should represent the amount of ﬂexibility in the method, and the y-axis should represent the values for each curve. There should be ﬁve curves. Make sure to label each one.\n\n\n\nExplain why each of the ﬁve curves has the shape displayed in part (a).\n\nIn the example f isn’t lineal, so the the test error lower as we add flexibility until the point the models starts to overfit. The training error always goes down as we increase the flexibility. As we make the model more flexible variance always increase as the model is more likely to change as we change the training data and the bias always goes down as a more flexible model has fewer assumptions. The Bayes error is the irreducible error we can not change it.\n\nYou will now think of some real-life applications for statistical learning.\n\n\nDescribe three real-life applications in which classiﬁcation might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n\n\n\n\n\n\n\nGoal\nResponse\nPredictors\n\n\n\n\nInference\nCustomer Tower Churn (0 or 1)\nAnnual Rent, Tower Lat, Tower Log, Tower Type, Number of sites around 10 km, Population around 10 km, Average Annual Salary in the city, contract Rent increases, customer technology\n\n\nInference\nEmployee Churn (0 or 1)\nMonths in company, Salary, Number of positions, Major, Sex, Total Salary Change, Bono, Wellness Expend, Number of depends, Home location\n\n\nInference\nAbsent (0 or 1)\nSalary, Rain?, Holiday?, Number of uniforms, distance from home to work place, Months in company, Neighborhood median Salary, number of depends, number of marriage, Work start, Work End, Free day\n\n\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n\n\n\n\n\n\n\nGoal\nResponse\nPredictors\n\n\n\n\nInference\nNumber of likes\nWords, Has a video?, Has a picture?, Post time, hashtag used\n\n\nInference\nVelocidad de picheo\nEdad, Altura, Peso, Horas de corrida, Cantidad de sentadillas, cantidad de practicas por semana, Años practicando el deporte\n\n\nInference\nFood Satisfaction level (0 to 10)\nCountry, City, Height, Weight, Salary (US $), Salt, Spacy Level, Sugar (gr), Meat Type, Cheese (gr), Cheese Type\n\n\n\n\nDescribe three real-life applications in which cluster analysis might be useful.\n\n\n\n\n\n\n\n\nGoal\nPredictors\n\n\n\n\nClassify costumer to improve advertising\nWords searched, products clicked, Explored image, Seconds spent on each product, start time, end time, customer location\n\n\nClassify company towers to see patterns in customers\nTower Lat, Tower Log, Tower Type, Number of sites around 10 km, Population around 10 km, Average Annual Salary in the city, BTS?, start date, Height\n\n\nClassify football players check which players have similar results\nNumber of passes on each game, Number of meters run on each game, Position Played, Number of goals, Number of stolen balls, total time played\n\n\n\n\nWhat are the advantages and disadvantages of a very ﬂexible (versus a less ﬂexible) approach for regression or classiﬁcation? Under what circumstances might a more ﬂexible approach be preferred to a less ﬂexible approach? When might a less ﬂexible approach be preferred?\n\n\nFlexible model advantages\n\nThey have the potential to accurately ﬁt a wider range of possible shapes for f\n\nFlexible model disadvantages\n\nThey do not reduce the problem of estimating f to a small number of parameters.\nA very large number of observations is required in order to obtain an accurate estimate for f.\nThey are harder to interpret\n\n\nIt is preferred when we have a lot of data to train the model and the goal is to get accurate predictions rather than good interpretations.\nA less flexible approach is preferred when we don’t have a lot data to train the model or when the main goal is to make inferences to understand business rules.\n\nDescribe the diﬀerences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classiﬁcation (as opposed to a nonparametric approach)? What are its disadvantages?\n\n\n\n\n\n\n\n\nParametric\nNon-parametric\n\n\n\n\nMake an assumption about the functional form\nDon’t make an assumption about the functional form, to accurately ﬁt a wider range of possible shapes for \\(f\\)\n\n\nEstimates a small number parameters based on training data\nEstimates a large number parameters based on training data\n\n\nCan be trained with few examples\nNeeds many examples to be trained\n\n\nSmoothness level is fixed\nData analyst needs define a level of smoothness\n\n\n\n\nParametric model advantages\n\nReduce the problem of estimating f to a small number of parameters.\nCan be trained with few examples.\nThey are easy to interpret.\n\nParametric model disadvantages\n\nIn many times \\(f\\) doesn’t have the assumed shape adding a lot of bias to the model.\n\n\n\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable.\n\n\nDF_07 <-\n  data.frame(X1 = c(0,2,0,0,-1,1),\n             X2 = c(3,0,1,1,0,0),\n             X3 = c(0,0,3,2,1,1),\n             Y = c(\"Red\",\"Red\",\"Red\",\"Green\",\"Green\",\"Red\"))\n\nDF_07\n\n  X1 X2 X3     Y\n1  0  3  0   Red\n2  2  0  0   Red\n3  0  1  3   Red\n4  0  1  2 Green\n5 -1  0  1 Green\n6  1  0  1   Red\n\n\nSuppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.\n\nCompute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.\n\n\n# As (C - 0)^2 = C^2\n\nDF_07 <- transform(DF_07, dist = sqrt(X1^2+X2^2+X3^2))\n\n\nWhat is our prediction with K = 1? Why?\n\n\nDF_07[order(DF_07$dist),]\n\n  X1 X2 X3     Y     dist\n5 -1  0  1 Green 1.414214\n6  1  0  1   Red 1.414214\n2  2  0  0   Red 2.000000\n4  0  1  2 Green 2.236068\n1  0  3  0   Red 3.000000\n3  0  1  3   Red 3.162278\n\n\nIn the is case, the point would be in the Bayes decision boundary as there are two points of different colors at the same distance.\n\nWhat is our prediction with K = 3? Why?\n\nIn this case, the point would be a Red one as 2 of 3 of them are from that color.\n\nIf the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?\n\nAs flexibility decrease as K gets bigger, for highly nonlinear Bayes decision boundary the best K value should be a small one."
  },
  {
    "objectID": "02-execises.html#applied",
    "href": "02-execises.html#applied",
    "title": "02 - Statistical Learning",
    "section": "Applied",
    "text": "Applied\n\nThis exercise relates to the College data set, which can be found in the ﬁle College.csv on the book website. It contains a number of variables for 777 diﬀerent universities and colleges in the US\n\nBefore reading the data into R, it can be viewed in Excel or a text editor.\n\nUse the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. You should notice that the ﬁrst column is just the name of each university. We don’t really want R to treat this as data.\n\n\ncollege <- \n  here::here(\"data/College.csv\") |>\n  read.csv(row.names = 1, stringsAsFactors = TRUE)\n\n\nUse the summary() function to produce a numerical summary of the variables in the data set.\n\n\nsummary(college)\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\nUse the pairs() function to produce a scatterplot matrix of the ﬁrst ten columns or variables of the data. \n\n\npairs(college[,1:10])\n\n\n\n\n\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private.\n\n\nplot(college$Private, college$Outstate)\n\n\n\n\n\nCreate a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10 % of their high school classes exceeds 50 %.\n\n\ncollege$Elite <- ifelse(college$Top10perc > 50, \"Yes\", \"No\") |> as.factor()\n\n\nThen Use the summary() function to see how many elite universities there are.\n\n\nsummary(college$Elite)\n\n No Yes \n699  78 \n\n\n\nNow use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\n\nplot(college$Elite, college$Outstate)\n\n\n\n\n\nUse the hist() function to produce some histograms with diﬀering numbers of bins for a few of the quantitative variables. You may ﬁnd the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\n\n\npar(mfrow = c(3, 1))\n\nhist(college$Apps)\nhist(college$Accept)\nhist(college$Enroll)\n\n\n\n\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\npar(mfrow = c(2, 2))\n\nplot(college$S.F.Ratio, college$Expend)\nplot(college$S.F.Ratio, college$Outstate)\nplot(college$Top10perc, college$Terminal)\nplot(college$Top10perc, college$Room.Board)\n\n\n\npar(mfrow = c(1, 1))\n\nAs students have more resources like teaching, supervision, curriculum development, and pastoral support institutions tend to expend less on each student and quest less money from out state students.\nWe also can see that students from top 10 % of high school class tend to go to universities where most the professors have the highest academic level available for each field or the highest room and board costs\n\nThis exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n\n\nWhich of the predictors are quantitative, and which are qualitative?\n\n\nlibrary(ISLR2)\n\nquantitative_vars <-\n  sapply(Auto, is.numeric) |>\n  (\\(x) names(x)[x])()\n\nqualitative_vars <- setdiff(names(Auto), quantitative_vars)\n\n\nWhat is the range of each quantitative predictor? You can answer this using the range() function.\n\n\nsapply(quantitative_vars, \\(var) range(Auto[[var]]))\n\n      mpg cylinders displacement horsepower weight acceleration year origin\n[1,]  9.0         3           68         46   1613          8.0   70      1\n[2,] 46.6         8          455        230   5140         24.8   82      3\n\n\n\nWhat is the mean and standard deviation of each quantitative predictor?\n\n\nsapply(quantitative_vars, \\(var) c(mean(Auto[[var]]), sd(Auto[[var]])))\n\n           mpg cylinders displacement horsepower    weight acceleration\n[1,] 23.445918  5.471939      194.412  104.46939 2977.5842    15.541327\n[2,]  7.805007  1.705783      104.644   38.49116  849.4026     2.758864\n          year    origin\n[1,] 75.979592 1.5765306\n[2,]  3.683737 0.8055182\n\n\n\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\n\nAutoFiltered <-Auto[-c(10:85),]\n\nsapply(quantitative_vars, \\(var) c(mean(AutoFiltered[[var]]), sd(AutoFiltered[[var]])))\n\n           mpg cylinders displacement horsepower    weight acceleration\n[1,] 24.404430  5.373418    187.24051  100.72152 2935.9715    15.726899\n[2,]  7.867283  1.654179     99.67837   35.70885  811.3002     2.693721\n          year   origin\n[1,] 77.145570 1.601266\n[2,]  3.106217 0.819910\n\n\n\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your ﬁndings.\n\nCars with 4 or 5 cylinders are more efficient than others.\n\nplot(factor(Auto$cylinders) ,Auto$mpg,\n     xlab = \"cylinders\", ylab = \"mpg\", col = 2)\n\n\n\n\nCars have improved their efficiency each year.\n\nplot(factor(Auto$year) ,Auto$mpg,\n     xlab = \"year\", ylab = \"mpg\", col = \"blue\")\n\n\n\n\n\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\npairs(Auto[, quantitative_vars])\n\n\n\n\ncylinders, horsepower and year are good candidates as they have correlations with the mpg variable.\n\nThis exercise involves the Boston housing data set.\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\nA data frame with 506 rows and 13 variables. Suburbs are represented as rows and columns represent different indicators.\n\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your ﬁndings.\n\nAs all the data is numeric, let’s get the highest correlations.\n\nBostonRelations <-\n  cor(Boston) |>\n  (\\(m) data.frame(row = rownames(m)[row(m)[upper.tri(m)]], \n                   col = colnames(m)[col(m)[upper.tri(m)]], \n                   cor = m[upper.tri(m)]))() |>\n  (\\(DF) DF[order(-abs(DF$cor)),])() \n\nhead(BostonRelations,3)\n\n     row col        cor\n45   rad tax  0.9102282\n26   nox dis -0.7692301\n9  indus nox  0.7636514\n\n\nAs we can see, if a suburb has high accessibility to radial highways the house value also increase.\n\nwith(Boston, plot(rad, tax, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(dis, nox, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(indus, nox, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\nwith(BostonRelations, BostonRelations[row == \"crim\", ])\n\n    row     col         cor\n29 crim     rad  0.62550515\n37 crim     tax  0.58276431\n56 crim   lstat  0.45562148\n7  crim     nox  0.42097171\n2  crim   indus  0.40658341\n67 crim    medv -0.38830461\n22 crim     dis -0.37967009\n16 crim     age  0.35273425\n46 crim ptratio  0.28994558\n11 crim      rm -0.21924670\n1  crim      zn -0.20046922\n4  crim    chas -0.05589158\n\n\n\nwith(Boston, plot(rad, crim, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(tax, crim, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\n\npar(mfrow=c(3,1))\n\nboxplot(Boston$crim, horizontal = TRUE)\nhist(Boston$tax)\nhist(Boston$ptratio)\n\n\n\npar(mfrow=c(1,1))\n\n\nHow many of the census tracts in this data set bound the Charles river?\n\n\nsum(Boston$chas)\n\n[1] 35\n\n\n\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\n\nmedian(Boston$ptratio)\n\n[1] 19.05\n\n\n\nWhich census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your ﬁndings.\n\n\nwhich.min(Boston$age)\n\n[1] 42\n\nMinOwnerOccupiedHomes <- Boston[which.min(Boston$age),]\n\nMinOwnerOccupiedHomes\n\n      crim zn indus chas   nox   rm age    dis rad tax ptratio lstat medv\n42 0.12744  0  6.91    0 0.448 6.77 2.9 5.7209   3 233    17.9  4.84 26.6\n\nVarsToPlot <-\n  names(Boston) |>\n  setdiff(\"crim\")\n\nfor(variable in VarsToPlot){\n  \n  hist(Boston[[variable]],\n       main = paste(\"Histogram of\" , variable), xlab = variable)\n  abline(v=MinOwnerOccupiedHomes[[variable]],col=\"blue\",lwd=2)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n\nsum(Boston$rm > 7)\n\n[1] 64\n\nsum(Boston$rm > 8)\n\n[1] 13\n\nBostonRelations[BostonRelations$row == \"rm\" | BostonRelations$col == \"rm\",]\n\n     row     col         cor\n72    rm    medv  0.69535995\n61    rm   lstat -0.61380827\n13 indus      rm -0.39167585\n51    rm ptratio -0.35550149\n12    zn      rm  0.31199059\n15   nox      rm -0.30218819\n42    rm     tax -0.29204783\n21    rm     age -0.24026493\n11  crim      rm -0.21924670\n34    rm     rad -0.20984667\n27    rm     dis  0.20524621\n14  chas      rm  0.09125123\n\npar(mfrow=c(1,2))\nwith(Boston, plot(rm, medv, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\nabline(v=8,col=\"red\",lwd=2)\n \n\nwith(Boston, plot(rm, lstat, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\nabline(v=8,col=\"red\",lwd=2)\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "03-execises.html",
    "href": "03-execises.html",
    "title": "03 - Linear Regression",
    "section": "",
    "text": "Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\n\nNull hypotheses for each predictor each coefficient is 0. We can see in the table that we can reject the null hypotheses for TV and radio but there isn’t enough evidence to reject the null hypotheses for newspaper.\n\nCarefully explain the differences between the KNN classifier and KNN regression methods.\n\nThe classifier assigns classes based on the most often class of the closest \\(K\\) elements, on the other hand the regression estimate each value taking the mean of the closest \\(K\\) elements.\n\nSuppose we have a data set with five predictors to predict the starting salary after graduation (in thousands of dollars) and after using least squares we fitted the next model:\n\n\n\n\nVariable\nCoefficient\n\n\n\n\nLevel (High School)\n\\(\\hat{\\beta}_{0} = 50\\)\n\n\n\\(X_{1}\\) = GPA\n\\(\\hat{\\beta}_{1} = 20\\)\n\n\n\\(X_{2}\\) = IQ\n\\(\\hat{\\beta}_{2} = 0.07\\)\n\n\n\\(X_{3}\\) = Level (College)\n\\(\\hat{\\beta}_{3} = 35\\)\n\n\n\\(X_{4}\\) = Interaction between GPA and IQ\n\\(\\hat{\\beta}_{4} = 0.01\\)\n\n\n\\(X_{5}\\) = Interaction between GPA and Level\n\\(\\hat{\\beta}_{5} = −10\\)\n\n\n\n\nWhich answer is correct, and why?\n\nBased on this information we can say that:\n\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduate.\n\nAs High School students earn on average \\(\\hat{\\beta}_{0} = 50\\) College students earn |\\(\\hat{\\beta}_{0} + \\hat{\\beta}_{3} = 85\\)\n\nPredict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n\n\\[\n\\begin{split}\n\\hat{Y} & = 35 + 20 (4) + 0.07 (110) + 35 + 0.01(4)(110) - 10 (4) \\\\\n        & = 122.1\n\\end{split}\n\\]\n\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n\nFALSE, we can not make conclusions about the significance of any tern about checking the the standard error of each term. The coefficient might small because the IQ has very high values if we contrast the GPA ones.\n\nI collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^2 + \\beta_{3}x^3 + \\epsilon\\).\n\n\nSuppose that the true relationship between X and Y is linear, i.e. \\(Y = \\beta_{0} + \\beta_{1}x + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\nAs the training RSS always gets lower as we increase the flexibility the cubic regression would have a lower RSS.\n\nAnswer (a) using test rather than training RSS.\n\nThe linear regression would have a lower test RSS, as it reduces de scare bias of the model.\n\nSuppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\nAs the training RSS always gets lower as we increase the flexibility the cubic regression would have a lower RSS.\n\nAnswer (c) using test rather than training RSS.\n\nThe cubic regression would have a lower test RSS, as it reduces de scare bias of the model.\n\nConsider the fitted values that result from performing linear regression without an intercept. In this setting, the \\(i\\)th fitted value takes the form.\n\n\\[\n\\hat{y}_{i} = x_{i}\\hat{\\beta}\n\\]\nWhere\n\\[\n\\hat{\\beta}= \\left( \\sum_{i=1}^{n}{x_{i}y_{i}}  \\right) /\n             \\left( \\sum_{i'=1}^{n}{x_{i'}^2}  \\right)\n\\]\n\nShow that we can write\n\n\\[\n\\hat{y}_{i} = \\sum_{i'=1}^{n}{a_{i'}y_{i'}}\n\\] I am not sure about this execise as I don’t understand the difference between \\(i\\) and \\(i'\\).\n\\[\n\\begin{split}\n\\sum_{i'=1}^{n}{a_{i'}y_{i'}} & = x_{i}\\hat{\\beta} \\\\\n\\sum_{i'=1}^{n}{a_{i'}y_{i'}} & = x_{i}\\frac{\\sum_{i=1}^{n}{x_{i}y_{i}}}\n                                            {\\sum_{i'=1}^{n}{x_{i'}^2} } \\\\\n\\sum_{i'=1}^{n}{a_{i'}} \\sum_{i'=1}^{n}{y_{i'}} & = \\frac{x_{i}\\sum_{i=1}^{n}{x_{i}}}\n                                                         {\\sum_{i'=1}^{n}{x_{i'}^2} }\n                                                    \\sum_{i=1}^{n} {y_{i}} \\\\\n\\sum_{i'=1}^{n}{a_{i'}} & = \\frac{x_{i}\\sum_{i=1}^{n}{x_{i}}}\n                                                         {\\sum_{i'=1}^{n}{x_{i'}^2} }\n\\end{split}\n\\]\n\nUsing (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \\((\\overline{x},\\overline{x})\\).\n\nAs you can see bellow the intercept it’s the responsible for that property.\n\\[\n\\begin{split}\n\\hat{y} & = \\left( \\hat{\\beta}_{0} \\right) + \\hat{\\beta}_{1} \\overline{x} \\\\\n\\hat{y} & = \\overline{y} - \\hat{\\beta}_{1}\\overline{x} + \\hat{\\beta}_{1} \\overline{x} \\\\\n\\hat{y} & = \\overline{y}\n\\end{split}\n\\]"
  },
  {
    "objectID": "03-execises.html#applied",
    "href": "03-execises.html#applied",
    "title": "03 - Linear Regression",
    "section": "Applied",
    "text": "Applied\n\nThis question involves the use of simple linear regression on the Auto data set.\n\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.\n\n\nlibrary(ISLR2)\n\nAutoSimpleModel <- lm(mpg ~ horsepower, data = Auto)\n\nsummary(AutoSimpleModel)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\nAs we see the regression p-value is much lower than 0.05 and we can reject the null hypotheses to conclude that there is a strong relationship between the response en the predictor. The coefficient of horsepower is negative, so we know that as the predictor increase the response decrease.\n\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals.\n\n\npredict(AutoSimpleModel, newdata = data.frame(horsepower = 98), interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\npredict(AutoSimpleModel, newdata = data.frame(horsepower = 98), interval = \"prediction\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\n\n\nPlot the response and the predictor. Use the abline() function to display the least squares regression line.\n\n\nplot(Auto$horsepower,Auto$mpg)\nabline(AutoSimpleModel)\n\n\n\n\n\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\n\n\npar(mfrow = c(2, 2))\nplot(AutoSimpleModel)\n\n\n\n\nThe Residuals vs Fitted shows that the relation is not linear and variance isn’t constant.\n\nThis question involves the use of multiple linear regression on the Auto data set.\n\n\nProduce a scatterplot matrix which includes all of the variables in the data set.\n\n\npairs(Auto)\n\n\n\n\n\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.\n\n\nAuto |>\n  subset(select = -name)|>\n  cor()\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\n\n\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance\n\n\nAutoModelNoInteraction <- \n  lm(mpg ~ . -name, data = Auto)\n\nAutoModelNoInteractionummary <- \n  summary(AutoModelNoInteraction)\n\nAutoModelNoInteractionummary\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n\n\nIs there a relationship between the predictors and the response?\n\nAs the regression p-value is bellow 0.05 we can reject the null hypothesis and conclude that at least one of the predictors have a relation with the response.\n\nWhich predictors appear to have a statistically significant relationship to the response?\n\n\nAutoModelNoInteractionummary |>\n  coefficients() |>\n  as.data.frame() |>\n  subset(`Pr(>|t|)` < 0.05)\n\n                  Estimate   Std. Error   t value     Pr(>|t|)\n(Intercept)  -17.218434622 4.6442941494 -3.707438 2.401841e-04\ndisplacement   0.019895644 0.0075150792  2.647430 8.444649e-03\nweight        -0.006474043 0.0006520478 -9.928787 7.874953e-21\nyear           0.750772678 0.0509731223 14.728795 3.055983e-39\norigin         1.426140495 0.2781360924  5.127492 4.665681e-07\n\n\n\nWhat does the coefficient for the year variable suggest?\n\nIt suggests that cars in average cars can drive 0.75 more miles per gallon every year.\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n\nNon-linearity of the response-predictor relationships\nNon-constant variance\nHigh-leverage points\n\n\n\npar(mfrow = c(2, 2))\nplot(AutoModelNoInteraction)\n\n\n\n\n\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\n\nremove_rownames <- function(DF){\n  \n  DF <- cbind(name = row.names(DF), DF)\n  rownames(DF) <- NULL\n  return(DF)\n  \n}\n\n\nnames(Auto) |>\n  setdiff(c(\"mpg\",\"name\")) |>\n  (\\(x) c(x,\n          combn(x, m = 2, \n                FUN = \\(y) paste0(y,collapse =\":\"))))() |>\n  paste0(collapse = \" + \") |>\n  paste0(\"mpg ~ \", predictors = _) |>\n  lm(data = Auto) |>\n  summary() |>\n  coef() |>\n  as.data.frame() |>\n  remove_rownames() |>\n  subset(`Pr(>|t|)` < 0.05 | name == \"year\")\n\n                  name      Estimate  Std. Error   t value    Pr(>|t|)\n3         displacement  -0.478538689 0.189353429 -2.527225 0.011920695\n6         acceleration  -5.859173212 2.173621188 -2.695582 0.007353578\n7                 year   0.697430284 0.609670317  1.143947 0.253399572\n8               origin -20.895570401 7.097090511 -2.944245 0.003445892\n18   displacement:year   0.005933802 0.002390716  2.482019 0.013515633\n27   acceleration:year   0.055621508 0.025581747  2.174265 0.030330641\n28 acceleration:origin   0.458316099 0.156659694  2.925552 0.003654670\n\n\n\nTry a few different transformations of the variables, such as \\(\\log{x}\\), \\(\\sqrt{x}\\), \\(x^2\\). Comment on your findings.\n\nAs we can see bellow we can explain 3% more of the variability by applying log to some variables.\n\nlibrary(data.table)\n\napply_fun_lm <- function(FUN,DF, trans_vars, remove_vars){\n  \n    as.data.table(DF\n    )[, (trans_vars) := lapply(.SD, FUN), .SDcols = trans_vars\n    ][, !remove_vars, with = FALSE\n    ][, lm(mpg ~ . , data = .SD)] |>\n    summary() |>\n    (\\(x) data.table(adj.r.squared = x$adj.r.squared,\n                     sigma = x$sigma,\n                     p.value = pf(x$fstatistic[\"value\"], \n                                  x$fstatistic[\"numdf\"], \n                                  x$fstatistic[\"dendf\"], \n                                  lower.tail = FALSE)))()\n  \n}\n\n\ndata.table(function_name = c(\"original\",\"log\", \"sqrt\",\"x^2\"),\n           function_list = list(\\(x) x,log, sqrt, \\(x) x^2)\n)[, data :=  \n    lapply(function_list, \n           FUN = apply_fun_lm,\n           DF = Auto, \n           trans_vars = c(\"displacement\", \"horsepower\", \n                          \"weight\", \"acceleration\"),\n           remove_vars = \"name\")\n][, rbindlist(data) |> cbind(function_name, end = _)]\n\n   function_name end.adj.r.squared end.sigma   end.p.value\n          <char>             <num>     <num>         <num>\n1:      original         0.8182238  3.327682 2.037106e-139\n2:           log         0.8474528  3.048425 5.352738e-154\n3:          sqrt         0.8312704  3.206041 1.304165e-145\n4:           x^2         0.7986663  3.502124 6.372862e-131\n\n\n\nThis question should be answered using the Carseats data set.\n\n\nFit a multiple regression model to predict Sales using Price, Urban, and US.\n\n\nCarseatsModel <-\n  lm(Sales~Price+Urban+US, data = Carseats)\n\nCarseatsModelSummary <-  \n  summary(CarseatsModel)\n\nCarseatsModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price + Urban + US, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9206 -1.6220 -0.0564  1.5786  7.0581 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.043469   0.651012  20.036  < 2e-16 ***\nPrice       -0.054459   0.005242 -10.389  < 2e-16 ***\nUrbanYes    -0.021916   0.271650  -0.081    0.936    \nUSYes        1.200573   0.259042   4.635 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.472 on 396 degrees of freedom\nMultiple R-squared:  0.2393,    Adjusted R-squared:  0.2335 \nF-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16\n\n\n\nProvide an interpretation of each coeﬃcient in the model. Be careful—some of the variables in the model are qualitative!\n\n\nCarseatsInterationModel <-\n  lm(Sales~Price*Urban*US, data = Carseats)\n\nCarseatsInterationModelSummary <-  \n  summary(CarseatsInterationModel)\n\nCarseatsInterationModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price * Urban * US, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7952 -1.6659 -0.0984  1.6119  7.2433 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          13.456350   1.727210   7.791 6.03e-14 ***\nPrice                -0.061657   0.014875  -4.145 4.17e-05 ***\nUrbanYes             -0.651545   2.071401  -0.315    0.753    \nUSYes                 2.049051   2.322591   0.882    0.378    \nPrice:UrbanYes        0.010793   0.017796   0.606    0.545    \nPrice:USYes          -0.001567   0.019972  -0.078    0.937    \nUrbanYes:USYes       -1.122034   2.759662  -0.407    0.685    \nPrice:UrbanYes:USYes  0.001288   0.023619   0.055    0.957    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.473 on 392 degrees of freedom\nMultiple R-squared:  0.2467,    Adjusted R-squared:  0.2333 \nF-statistic: 18.34 on 7 and 392 DF,  p-value: < 2.2e-16\n\n\n\nWrite out the model in equation form, being careful to handle the qualitative variables properly.\n\n\ncoef(CarseatsInterationModel) |>\n  round(3) |>\n  (\\(x) paste0(ifelse(x < 0, \" - \",\" + \"), abs(x),\" \\text{ \", names(x),\"}\"))() |>\n  sub(pattern = \" \\text{ (Intercept)}\",replacement = \"\", fixed = TRUE) |>\n  paste0(collapse = \"\") |>\n  sub(pattern = \"^ \\\\+ \", replacement = \"\") |>\n  sub(pattern = \"^ - \", replacement = \"\") |>\n  paste0(\"hat{Y} = \", FUN = _)\n\n[1] \"hat{Y} = 13.456 - 0.062 \\text{ Price} - 0.652 \\text{ UrbanYes} + 2.049 \\text{ USYes} + 0.011 \\text{ Price:UrbanYes} - 0.002 \\text{ Price:USYes} - 1.122 \\text{ UrbanYes:USYes} + 0.001 \\text{ Price:UrbanYes:USYes}\"\n\n\n\\[\n\\begin{split}\n\\hat{Sales} & = 13.456 - 0.062 \\text{ Price} - 0.652 \\text{ UrbanYes} \\\\\n            & \\quad + 2.049 \\text{ USYes} + 0.011 \\text{ Price:UrbanYes} \\\\\n            & \\quad - 0.002 \\text{ Price:USYes} - 1.122 \\text{ UrbanYes:USYes} \\\\\n            & \\quad + 0.001 \\text{ Price:UrbanYes:USYes}\n\\end{split}\n\\]\n\nFor which of the predictors can you reject the null hypothesis H0 : βj = 0?\n\n\ncoef(CarseatsInterationModelSummary) |>\n  as.data.frame() |>\n  (\\(DF) DF[DF$`Pr(>|t|)` < 0.05,])()\n\n               Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 13.45634952 1.72720976  7.790802 6.030364e-14\nPrice       -0.06165717 0.01487479 -4.145079 4.165536e-05\n\n\n\nOn the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n\n\nCarseatsPriceModel <-\n  lm(Sales~Price, data = Carseats)\n\nCarseatsPriceModelSummary <-\n  summary(CarseatsPriceModel)\n\nCarseatsPriceModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.641915   0.632812  21.558   <2e-16 ***\nPrice       -0.053073   0.005354  -9.912   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: < 2.2e-16\n\n\n\nHow well do the models in (a) and (e) ﬁt the data?\n\nModel a fits better to the data with 0.23 against 0.2 of model e.\n\nUsing the model from (e), obtain 95 % conﬁdence intervals for the coeﬃcient(s).\n\n\nconfint(CarseatsPriceModel, level = 0.95)\n\n                 2.5 %      97.5 %\n(Intercept) 12.3978438 14.88598655\nPrice       -0.0635995 -0.04254653\n\n\n\nIs there evidence of outliers or high leverage observations in the model from (e)?\n\n\npar(mfrow = c(2,2))\nplot(CarseatsPriceModel)\n\n\n\npar(mfrow = c(1,1))\n\nThere is a leverage point.\n\nIn this problem we will investigate the t-statistic for the null hypothesis H0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.\n\n\nset.seed(1)\n\nx <- rnorm(100)\ny <- 2*x+rnorm(100)\n\nSimulatedData <- data.frame(x, y) \n\n\nPerform a simple linear regression of y onto x, without an intercept. Report the coeﬃcient estimate ˆβ, the standard error of this coeﬃcient estimate, and the t-statistic and p-value associated with the null hypothesis H0 : β = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).)\n\nAs we can see below we can reject the null hypothesis and conclude that y increases 1.99 for each unit of x explaining 78% of the variability.\n\nlm(y~ x+0, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9154 -0.6472 -0.1771  0.5056  2.3109 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   1.9939     0.1065   18.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9586 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nNow perform a simple linear regression of x onto y without an intercept, and report the coeﬃcient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis H0 : β = 0. Comment on these results.\n\nAs we can see below we can reject the null hypothesis and conclude that x increases 0.39 for each unit of y explaining 78% of the variability.\n\nlm(x~ y+0, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8699 -0.2368  0.1030  0.2858  0.8938 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \ny  0.39111    0.02089   18.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4246 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nWhat is the relationship between the results obtained in (a) and (b)?\n\ny can explain x as well a x explains y.\n\nIn R, show that when regression is performed with an intercept, the t-statistic for H0 : β1 = 0 is the same for the regression of y onto x as it is for the regression of x onto y.\n\nAs you can see below the t-statistic for \\(\\beta_{1}\\) is t-statistic for both regressions is 18.56.\n\nlm(y~ x, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8768 -0.6138 -0.1395  0.5394  2.3462 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.03769    0.09699  -0.389    0.698    \nx            1.99894    0.10773  18.556   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9628 on 98 degrees of freedom\nMultiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 \nF-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\n\nlm(x~ y, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y, data = SimulatedData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90848 -0.28101  0.06274  0.24570  0.85736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.03880    0.04266    0.91    0.365    \ny            0.38942    0.02099   18.56   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4249 on 98 degrees of freedom\nMultiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 \nF-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\n\nThis problem involves simple linear regression without an intercept.\n\n\nRecall that the coeﬃcient estimate β for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coeﬃcient estimate for the regression of X onto Y the same as the coeﬃcient estimate for the regression of Y onto X?\n\nThe coefficient would be different between y~x and x~y.\n\nGenerate an example in R with n = 100 observations in which the coeﬃcient estimate for the regression of X onto Y is diﬀerent from the coeﬃcient estimate for the regression of Y onto X.\n\n\nset.seed(5)\nSimulatedData2 <- \n  data.frame(x = rnorm(100, mean = 8, sd = 4))\n\nset.seed(8)\nSimulatedData2$y <- \n  10*SimulatedData2$x + rnorm(100, sd = 10) \n\nplot(SimulatedData2$x, SimulatedData2$y)\n\n\n\nlm(y~ x+0, data = SimulatedData2) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.7686  -6.8107  -0.3744   6.5070  24.3187 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   9.9363     0.1207    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.81 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\nlm(x~ y+0, data = SimulatedData2) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2861 -0.5429  0.1264  0.7279  3.0421 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \ny 0.099191   0.001205    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.08 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nGenerate an example in R with n = 100 observations in which the coeﬃcient estimate for the regression of X onto Y is the same as the coeﬃcient estimate for the regression of Y onto X.\n\n\nset.seed(5)\nSimulatedData3 <- \n  data.frame(x = rnorm(100, mean = 8, sd = 4))\n\nset.seed(8)\nSimulatedData3$y <- \n  SimulatedData3$x + rnorm(100, sd = 1) \n\nplot(SimulatedData3$x, SimulatedData3$y)\n\n\n\nlm(y~ x+0, data = SimulatedData3) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.97686 -0.68107 -0.03744  0.65070  2.43187 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx  0.99363    0.01207    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.081 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\nlm(x~ y+0, data = SimulatedData3) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2861 -0.5429  0.1264  0.7279  3.0421 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \ny  0.99191    0.01205    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.08 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nIn this exercise you will create some simulated data and will ﬁt simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.\n\n\nUsing the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0, 1) distribution. This represents a feature, X.\n\n\nset.seed(1)\n\nx <- rnorm(100)\n\n\nUsing the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution—a normal distribution with mean zero and variance 0.25.\n\n\neps <- rnorm(100, sd = sqrt(0.25))\n\n\nUsing x and eps, generate a vector y according to the model.\n\n\\[\nY = -1 + 0.5X + \\epsilon\n\\]\n\ny <- -1 + 0.5*x +eps\n\n\n- **What is the length of the vector y?**\n\nIt has the same length of x.\n\n\nWhat are the values of β0 and β1 in this linear model?\n\n\n\\(\\beta_{0} = -1\\) and \\(\\beta_{1} = 0.5\\).\n\nCreate a scatterplot displaying the relationship between x and y. Comment on what you observe.\n\n\nplot(x,y)\n\n\n\n\n\nFit a least squares linear model to predict y using x. Comment on the model obtained. How do ˆβ0 and ˆ β1 compare to β0 and β1?\n\nAfter rounding the value to one decimal the coefficients are the same.\n\nSimilatedModel <-lm(y~x)\n\nSimilatedModel |>\n  coef() |>\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\n\nDisplay the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a diﬀerent color. Use the legend() command to create an appropriate legend.\n\n\nplot(x,y)\nabline(SimilatedModel, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nNow ﬁt a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model ﬁt? Explain your answer.\n\nThere is no evidence that the polynomial model fits better to the data.\n\nSimilatedPolyModel <-lm(y~x+I(x^2))\n\nanova(SimilatedModel,SimilatedPolyModel)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ x + I(x^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     98 22.709                           \n2     97 22.257  1   0.45163 1.9682 0.1638\n\n\n\nRepeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term ϵ in (b). Describe your results.\n\n\nset.seed(1)\n\nx <- rnorm(100)\neps <- rnorm(100, sd = sqrt(0.10))\ny <- -1 + 0.5*x +eps\n\nplot(x,y)\n\n\n\n\nThe coefficients remind the same.\n\nSimilatedModel2 <-lm(y~x)\n\nSimilatedModel2 |>\n  coef() |>\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\nAnd the Lease Square Line and the Population Line are closer.\n\nplot(x,y)\nabline(SimilatedModel2, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nRepeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term ϵ in (b). Describe your results.\n\n\nset.seed(1)\n\nx <- rnorm(100)\neps <- rnorm(100, sd = sqrt(1.5))\ny <- -1 + 0.5*x +eps\n\nplot(x,y)\n\n\n\n\nThe coefficients remind the same.\n\nSimilatedModel3 <-lm(y~x)\n\nSimilatedModel3 |>\n  coef() |>\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\nDespite, y has a wider range of values are almost the same.\n\nplot(x,y)\nabline(SimilatedModel3, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nWhat are the conﬁdence intervals for β0 and β1 based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.\n\n\nlibrary(ggplot2)\n\n\nadd_source <- function(list.DT, source.name = \"source\"){\n  \n  table_names <- names(list.DT)\n  \n  for(tb_i in seq_along(list.DT)){\n    list.DT[[tb_i]][, (source.name) := names(list.DT)[tb_i] ] \n  }\n  \n  return(list.DT)\n}\n\n\nlist(original = SimilatedModel,\n     less_noisy = SimilatedModel2,\n     noisier = SimilatedModel3) |>\n lapply(\\(model) cbind(center = coef(model), confint(model)) |> \n                 as.data.table(keep.rownames = \"coef\")) |>\n add_source(source.name = \"model\") |>\n rbindlist() |>\n DT(j = model := factor(model, \n                        levels = c(\"less_noisy\", \"original\",\"noisier\"))) |>\n ggplot(aes(model, center, color = model))+\n  geom_hline(yintercept = 0, linetype = 2, size = 1)+\n  geom_point()+\n  geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`), width = 0.5)+\n  scale_color_brewer(palette = \"Blues\")+\n  facet_wrap(~coef, ncol = 2, scales = \"free_y\")+\n  labs(title = \"Coefficient Confident Intervals get wider\",\n       subtitle = \"as the error increase but it isn't enough to change conclusions\")+\n  theme_classic()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nThis problem focuses on the collinearity problem.\n\n\nPerform the following commands in R:\n\n\nset.seed(1)\nx1 <- runif(100)\nx2 <- 0.5*x1+rnorm(100) / 10\ny <- 2+2*x1+0.3*x2+rnorm(100)\n\n\nThe last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coeﬃcients?\n\n\\[\nY = 2 + 2 x_{1} + 0.3 x_{2} + \\epsilon\n\\]\n\nWhat is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.\n\n\nplot(x1,x2, \n     main = paste0(\"x1 and x2 correlation :\",round(cor(x1,x2), 2)))\n\n\n\n\n\nUsing this data, ﬁt a least squares regression to predict y using x1 and x2. Describe the results obtained. What are ˆβ0, ˆ β1, and ˆβ2? How do these relate to the true β0, β1, and β2? Can you reject the null hypothesis H0 : β1 = 0? How about the null hypothesis H0 : β2 = 0?\n\n\nSimulatedModelExc14 <- lm(y~x1+x2)\n\nSimulatedModelExc14Summary <- summary(SimulatedModelExc14)\n\nSimulatedModelExc14Summary\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8311 -0.7273 -0.0537  0.6338  2.3359 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1305     0.2319   9.188 7.61e-15 ***\nx1            1.4396     0.7212   1.996   0.0487 *  \nx2            1.0097     1.1337   0.891   0.3754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.056 on 97 degrees of freedom\nMultiple R-squared:  0.2088,    Adjusted R-squared:  0.1925 \nF-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05\n\n\nThe \\(\\hat{\\beta}_{0} = 2.13\\) which is really close to the true value of \\(\\beta_{0} = 2\\), but \\(\\hat{\\beta}_{1}\\) and \\(\\hat{\\beta}_{2}\\) are very different to their real values. We almost can not reject the null hypothesis for \\(\\beta_{1}\\) and can not reject the null hypothesis for \\(\\beta_{1}\\) where both should be significant to explain \\(\\hat{Y}\\).\n\nNow ﬁt a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?\n\n\nlm(y~x1) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89495 -0.66874 -0.07785  0.59221  2.45560 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1124     0.2307   9.155 8.27e-15 ***\nx1            1.9759     0.3963   4.986 2.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.055 on 98 degrees of freedom\nMultiple R-squared:  0.2024,    Adjusted R-squared:  0.1942 \nF-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06\n\n\nNow \\(\\beta_{1}\\) we can surely reject the null t-value is now 2.5 times higher that it used to be.\n-Now ﬁt a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?\n\nlm(y~x2) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62687 -0.75156 -0.03598  0.72383  2.44890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.3899     0.1949   12.26  < 2e-16 ***\nx2            2.8996     0.6330    4.58 1.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.072 on 98 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.1679 \nF-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05\n\n\nNow \\(\\beta_{2}\\) we can surely reject the null t-value is now 5.14 times higher that it used to be.\n\nDo the results obtained in (c)–(e) contradict each other? Explain your answer.\n\nYes, they do. In c, we couldn’t reject the null hypothesis for x2 but that change in the e question.\n\nNow suppose we obtain one additional observation, which was unfortunately mismeasured.\n\n\nx1_c<-c(x1, 0.1)\nx2_c<-c(x2, 0.8)\ny_c<-c(y, 6)\n\n\nRe-ﬁt the linear models from (c) to (e) using this new data. What eﬀect does this new observation have on the each of the models?\n\nThanks the additional row x2 seems to be significant rather than x1.\n\nModelC <- lm(y_c~x1_c+x2_c) \nsummary(ModelC)\n\n\nCall:\nlm(formula = y_c ~ x1_c + x2_c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.73348 -0.69318 -0.05263  0.66385  2.30619 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2267     0.2314   9.624 7.91e-16 ***\nx1_c          0.5394     0.5922   0.911  0.36458    \nx2_c          2.5146     0.8977   2.801  0.00614 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.075 on 98 degrees of freedom\nMultiple R-squared:  0.2188,    Adjusted R-squared:  0.2029 \nF-statistic: 13.72 on 2 and 98 DF,  p-value: 5.564e-06\n\n\nIn the next model, we can see that the previous model was fitting better to y based on x1. The \\(R^2\\) went down from 0.20 to 0.16.\n\nModelD <- lm(y_c~x1_c) \nsummary(ModelD)\n\n\nCall:\nlm(formula = y_c ~ x1_c)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8897 -0.6556 -0.0909  0.5682  3.5665 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2569     0.2390   9.445 1.78e-15 ***\nx1_c          1.7657     0.4124   4.282 4.29e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.111 on 99 degrees of freedom\nMultiple R-squared:  0.1562,    Adjusted R-squared:  0.1477 \nF-statistic: 18.33 on 1 and 99 DF,  p-value: 4.295e-05\n\n\nIn the next model, we can see that the previous model was fitting worse to y based on x2. The \\(R^2\\) went up from 0.18 to 0.21.\n\nModelE <- lm(y_c~x2_c) \nsummary(ModelE)\n\n\nCall:\nlm(formula = y_c ~ x2_c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.64729 -0.71021 -0.06899  0.72699  2.38074 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.3451     0.1912  12.264  < 2e-16 ***\nx2_c          3.1190     0.6040   5.164 1.25e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.074 on 99 degrees of freedom\nMultiple R-squared:  0.2122,    Adjusted R-squared:  0.2042 \nF-statistic: 26.66 on 1 and 99 DF,  p-value: 1.253e-06\n\n\n\nIn each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.\n\nIn the c model the last observation is a high-leverage point.\n\npar(mfrow = c(2,2))\nplot(ModelC)\n\n\n\n\nIn the d model the last observation is an outlier point as it’s studentized residuals is greater than 3.\n\npar(mfrow = c(2,2))\nplot(ModelD)\n\n\n\n\nIn the e model the last observation is a high-leverage point.\n\npar(mfrow = c(2,2))\nplot(ModelE)\n\n\n\n\n\nThis problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n\n\nFor each predictor, ﬁt a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically signiﬁcant association between the predictor and the response? Create some plots to back up your assertions.\n\nAs we can see below the only predictor that wasn’t found significant was the chas one.\n\nBostonModelSummary <-\n  data.table(predictor = colnames(Boston) |> setdiff(\"crim\")\n  )[, model := lapply(predictor, \\(x) paste0(\"crim~\",x) |> \n                           lm(data = Boston) |>\n                           summary() |>\n                           coef() |>\n                           as.data.table(keep.rownames = \"coef\")) \n  ][, model[[1]], \n    by = \"predictor\"\n  ][predictor == coef, !c(\"coef\")\n  ][, is_significant :=  `Pr(>|t|)` < 0.05\n  ][order(`Pr(>|t|)`)]\n\nBostonModelSummary\n\n    predictor    Estimate  Std. Error   t value     Pr(>|t|) is_significant\n       <char>       <num>       <num>     <num>        <num>         <lgcl>\n 1:       rad  0.61791093 0.034331820 17.998199 2.693844e-56           TRUE\n 2:       tax  0.02974225 0.001847415 16.099388 2.357127e-47           TRUE\n 3:     lstat  0.54880478 0.047760971 11.490654 2.654277e-27           TRUE\n 4:       nox 31.24853120 2.999190381 10.418989 3.751739e-23           TRUE\n 5:     indus  0.50977633 0.051024332  9.990848 1.450349e-21           TRUE\n 6:      medv -0.36315992 0.038390175 -9.459710 1.173987e-19           TRUE\n 7:       dis -1.55090168 0.168330031 -9.213458 8.519949e-19           TRUE\n 8:       age  0.10778623 0.012736436  8.462825 2.854869e-16           TRUE\n 9:   ptratio  1.15198279 0.169373609  6.801430 2.942922e-11           TRUE\n10:        rm -2.68405122 0.532041083 -5.044819 6.346703e-07           TRUE\n11:        zn -0.07393498 0.016094596 -4.593776 5.506472e-06           TRUE\n12:      chas -1.89277655 1.506115484 -1.256727 2.094345e-01          FALSE\n\n\nAny relation seems to be really linear and the chas predictor has been wrongly classify as numeric when it should have been a qualitative variable.\n\nfor(predictor in BostonModelSummary$predictor){\n \n  cor(Boston[[predictor]],Boston$crim) |>\n  round(2) |>\n  paste0(\"Crim vs \", predictor,\"\\nCorrelation :\", correlation = _) |>\n  plot(Boston[[predictor]],Boston$crim, \n       xlab = predictor, ylab = \"crim\", \n       main = _)\n   \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut after changing chas to a factor we keep the same conclusion and the coefficient it’s the same.\n\nlm(crim~as.factor(chas), data = Boston) |>\n  summary()\n\n\nCall:\nlm(formula = crim ~ as.factor(chas), data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.738 -3.661 -3.435  0.018 85.232 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        3.7444     0.3961   9.453   <2e-16 ***\nas.factor(chas)1  -1.8928     1.5061  -1.257    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.597 on 504 degrees of freedom\nMultiple R-squared:  0.003124,  Adjusted R-squared:  0.001146 \nF-statistic: 1.579 on 1 and 504 DF,  p-value: 0.2094\n\n\n\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0\n\nNow we just can reject the null hypothesis for the following predictos:\n\nBostonModel2 <-\n  lm(formula = crim ~ . , data = Boston)\n\nBostonModelSummary2 <-\n  summary(BostonModel2) |>\n  coef() |>\n  as.data.table(keep.rownames = \"predictor\")\n\nBostonModelSummary2[`Pr(>|t|)` < 0.05]\n\n   predictor    Estimate Std. Error   t value     Pr(>|t|)\n      <char>       <num>      <num>     <num>        <num>\n1:        zn  0.04571004 0.01879032  2.432637 1.534403e-02\n2:       dis -1.01224674 0.28246757 -3.583586 3.725942e-04\n3:       rad  0.61246531 0.08753576  6.996744 8.588123e-12\n4:      medv -0.22005636 0.05982396 -3.678399 2.605302e-04\n\n\n\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coeﬃcients from (a) on the x-axis, and the multiple regression coeﬃcients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coeﬃcient in a simple linear regression model is shown on the x-axis, and its coeﬃcient estimate in the multiple linear regression model is shown on the y-axis.\n\n\nmerge(BostonModelSummary, BostonModelSummary2,\n      by = \"predictor\", suffixes = c(\"_uni\",\"_multi\")\n  )[, .(predictor,\n        Estimate_uni = round(Estimate_uni, 2),\n        Estimate_multi = round(Estimate_multi, 2), \n        coef_change = abs(Estimate_uni / Estimate_multi),\n        vif = car::vif(BostonModel2)[predictor],\n        kept_significant = is_significant & `Pr(>|t|)_multi` < 0.05)\n  ][, predictor := reorder(predictor, coef_change)] |>\n  ggplot(aes(coef_change,vif))+\n  geom_point(aes(color = kept_significant))+\n  geom_text(aes(label = predictor), vjust = 1.2)+\n  scale_color_manual(values = c(\"TRUE\" = \"dodgerblue4\", \"FALSE\" = \"gray80\"))+\n  scale_x_log10()+\n  scale_y_log10()+\n  labs(title = \"Significan Predictors Change Less\",\n       subtitle = \"Predictos Coeﬃcient Change Between Simple and Multiple Lineal Models\")+\n  theme_classic()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n\n\n\n\nIs there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, ﬁt a model of the form.\n\n\\[\nY = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^2 + \\beta_{3} X^2 + \\epsilon\n\\]\n\nSimpleVsPolySummary <-\n  data.table(predictor = colnames(Boston) |> setdiff(c(\"crim\",\"chas\"))\n  )[,`:=`(r2_simple = sapply(predictor, \\(x) paste0(\"crim~\",x) |> \n                               lm(data = Boston) |>\n                               summary() |>\n                               (\\(x) x[[\"r.squared\"]])() ),\n          r2_poly = sapply(predictor, \\(x) gsub(\"x\",x,\"crim~x+I(x^2)+I(x^3)\") |> \n                             lm(data = Boston) |>\n                             summary() |>\n                             (\\(x) x[[\"r.squared\"]])() )), \n  ][, change := r2_poly - r2_simple\n  ][order(-change)]\n\n\nSimpleVsPolySummary[, lapply(.SD, \\(x) \n                             if(is.numeric(x)) scales::percent(x, accuracy = 0.01)\n                             else x)]\n\n    predictor r2_simple r2_poly change\n       <char>    <char>  <char> <char>\n 1:      medv    15.08%  42.02% 26.94%\n 2:       dis    14.41%  27.78% 13.37%\n 3:       nox    17.72%  29.70% 11.98%\n 4:     indus    16.53%  25.97%  9.43%\n 5:       age    12.44%  17.42%  4.98%\n 6:   ptratio     8.41%  11.38%  2.97%\n 7:       tax    33.96%  36.89%  2.93%\n 8:        rm     4.81%   6.78%  1.97%\n 9:        zn     4.02%   5.82%  1.81%\n10:     lstat    20.76%  21.79%  1.03%\n11:       rad    39.13%  40.00%  0.88%\n\nfor(predictor in SimpleVsPolySummary[change >= 0.1 ,predictor]){\n \n  cor(Boston[[predictor]],Boston$crim) |>\n  round(2) |>\n  paste0(\"Crim vs \", predictor,\"\\nCorrelation :\", correlation = _) |>\n  plot(Boston[[predictor]],Boston$crim, \n       xlab = predictor, ylab = \"crim\", \n       main = _)\n   \n}"
  },
  {
    "objectID": "04-execises.html",
    "href": "04-execises.html",
    "title": "04 - Classification",
    "section": "",
    "text": "Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\n\\[\n\\begin{split}\np(X) &= \\frac{e^{\\beta_{0}+\\beta_{1}X}}\n            {1+e^{\\beta_{0}+\\beta_{1}X}} \\\\\np(X) (1+e^{\\beta_{0}+\\beta_{1}X}) & = e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X)+p(X) e^{\\beta_{0}+\\beta_{1}X} & = e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X) & = e^{\\beta_{0}+\\beta_{1}X} - p(X) e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X) & = (1 - p(X))e^{\\beta_{0}+\\beta_{1}X} \\\\\n\\frac{p(X)}{1 - p(X)} & = e^{\\beta_{0}+\\beta_{1}X}\n\\end{split}\n\\]\n\n\n\n\nIt was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a N(µk,σ2) distribution, the Bayes classiﬁer assigns an observation to the class for which the discriminant function is maximized.\n\nTo prove that both functions would provide the same class we need to check that both functions change from one \\(Y\\) class to other in the same \\(x\\) value.\nIf \\(K = 2\\) and \\(\\pi_1 = \\pi_2 = \\pi_c\\) we can show that:\n\\[\n\\begin{split}\np_{1}(x) & = p_{2}(x) \\\\\n\\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{l})^2}}\n& =\n\\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{l})^2}} \\\\\ne^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}\n& =\ne^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2} \\\\\n\\frac{e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}}{e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2}}\n& = 1 \\\\\n(x - \\mu_{2})^2 - (x - \\mu_{1})^2\n& = 0 \\\\\nx^2 - 2x\\mu_{2} + \\mu_{2}^2 - (x^2 - 2x\\mu_{1} + \\mu_{1}^2)\n& = 0 \\\\\n2x (\\mu_{1}- \\mu_{2}) & = \\mu_{1}^2 - \\mu_{2}^2  \\\\\nx & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2 (\\mu_{1}- \\mu_{2})} \\\\\nx & = \\frac{\\mu_1 + \\mu_2}{2}\n\\end{split}\n\\]\nAnd also:\n\\[\n\\begin{split}\n\\delta_{1}(x) & = \\delta_{2}(x) \\\\\n\\log{(\\pi_{c})}\n- \\frac{\\mu_{1}^2}{2\\sigma^2}\n+ x \\cdot \\frac{\\mu_{1}}{\\sigma^2}\n& =\n\\log{(\\pi_{c})}\n- \\frac{\\mu_{2}^2}{2\\sigma^2}\n+ x \\cdot \\frac{\\mu_{2}}{\\sigma^2} \\\\\nx (\\mu_{1} - \\mu_{2}) & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2} \\\\\nx & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2(\\mu_{1} - \\mu_{2})} \\\\\nx & = \\frac{\\mu_1 + \\mu_2}{2}\n\\end{split}\n\\]\nLet’s see an example visually by setting as example the next values for each \\(Y\\) class:\n\n\n\n\\(k\\)\n\\(\\sigma\\)\n\\(\\pi\\)\n\\(\\mu\\)\n\n\n\n\n1\n0.5\n0.5\n2\n\n\n2\n0.5\n0.5\n4\n\n\n\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nldm_k_prop <- function(x,\n                       k,\n                       sigma = c(0.5,0.5),\n                       pi_k = c(0.5,0.5),\n                       mu = c(2, 4),\n                       logit = FALSE){\n  \n  if(logit){\n    \n    return(x * mu[k]/sigma[k]^2 - mu[k]^2/(2*sigma[k]^2) + log(pi_k[k]))\n    \n  }\n  \n  denominator <-\n    sapply(x, \\(y) sum(pi_k * (1/(sqrt(2*pi)*sigma)) * exp(-1/(2*sigma^2) * (y - mu)^2) ) )\n  \n  k_numerador <-\n   (pi_k[k]* (sqrt(2*pi)*sigma[k])^-1 * exp(- (2*sigma[k]^2)^-1 * (x - mu[k])^2))\n  \n  return(k_numerador / denominator)\n  \n}\n\nBasePlot <-\n  data.frame(x = 1:5) |>\n  ggplot(aes(x))+\n  scale_x_continuous(breaks = scales::breaks_width(1))+\n  theme_light()+\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n  \np1 <-\n  BasePlot +\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 1), color = \"blue\")+\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 2), color = \"red\")\n\np2 <-\n  BasePlot +\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 1, logit = TRUE), color = \"blue\")+\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 2, logit = TRUE), color = \"red\")\n\n\n(p1 / p2) +\n  plot_annotation(title = 'Comparing Proportion Function vs Logit Function of LDA',\n                  theme = theme(plot.title = element_text(face = \"bold\")) )\n\n\n\n\n\n\n\n\nThis problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, X ∼ N(µk, σ2k). Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\n\n\\[\n\\begin{split}\np_k(x) & =\n\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi} \\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n& =\n\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi} \\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\frac{1}{\\sqrt{2\\pi}}\n     \\sum_{l=1}^{K}\\pi_l \\frac{1}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n& =\n\\frac{\\frac{\\pi_k}{\\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K} \\frac{\\pi_l}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n\\end{split}\n\\]\nAs the denominator is a constant for any \\(k\\), we can define: \\(g(x) = \\frac{1}{\\sum_{l=1}^{K} \\frac{\\pi_l}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}}\\)\n\\[\n\\begin{split}\np_k(x) & = g(x) \\frac{\\pi_k}{\\sigma_k}\n           e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2}\\\\\n\\log{(p_k(x))}& =\n\\log{\\left( g(x) \\frac{\\pi_k}{\\sigma_k}\n            e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2} \\right)}\\\\\n& =\n\\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} -\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2 \\\\\n& =\n\\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} -\\frac{\\mu_k^2 - 2\\mu_kx + x^2}{2\\sigma_k^2} \\\\\n& =\n\\left( \\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} - \\frac{\\mu_k^2}{2\\sigma_k^2} \\right)\n+ \\frac{\\mu_k}{\\sigma_k^2} \\cdot x - \\frac{1}{2\\sigma_k^2} \\cdot x^2 \\\\\n\\end{split}\n\\]\n\n\n\n\nWhen the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.\n\n(A) Suppose that we have a set of observations, each with measurements on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0, 1]. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55, 0.65]. On average, what fraction of the available observations will we use to make the prediction?\n\nlibrary(data.table)\n\nset.seed(123)\n\nUnifDistVar1 <-\n  data.table(sample = 1:1000\n  )[, .(x1 = runif(1000)),\n    by = \"sample\"\n  ][, .(prop = mean(x1 %between% c(0.6 - 0.1/2, 0.6 + 0.1/2))),\n    by = \"sample\"]\n  \nmean(UnifDistVar1$prop)\n\n[1] 0.100297\n\n\n(B) Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. We assume that (X1, X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of X2 closest to that test observation. For instance, in order to predict the response for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3, 0.4] for X2. On average, what fraction of the available observations will we use to make the prediction?\n\nset.seed(123)\n\nUnifDistVar2 <-\n  UnifDistVar1[, .(x1 = runif(1000),\n                   x2 = runif(1000)),\n               by = \"sample\"\n  ][ , .(prop = mean(x1 %between% c(0.6 - 0.1/2, 0.6 + 0.1/2) &\n                     x2 %between% c(0.35 - 0.1/2, 0.35 + 0.1/2))),\n     by = \"sample\"]\n\nmean(UnifDistVar2$prop)\n\n[1] 0.009943\n\n\n(C) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n\n0.1^100\n\n[1] 1e-100\n\n\n(D) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.\nAs p gets lager every point has more specifications to meet and the number of point which meet them decrease.\n(E) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For p = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer.\n\n\n\n\nWe now examine the differences between LDA and QDA.\n\n(A) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\n\nType of set to test\nPerform better\n\n\n\n\ntraining set\nQDA, as it will model some noise\n\n\ntest set\nLDA, as decision boundary is lineal\n\n\n\n(B) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\n\nType of set to test\nPerform better\n\n\n\n\ntraining set\nQDA, as it will model some noise\n\n\ntest set\nQDA, as decision boundary is non-linear\n\n\n\n(C) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\nIf the Bayes decision boundary is non-linear, QDA could improve its test prediction accuracy as the sample size n increases. When the model has more examples it has less changes to overfit the data.\n(D) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\nFALSE, as QDA would model some noise that could affect the prediction accuracy.\n\n\n\n\nSuppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0 = −6\\), \\(\\hat{\\beta}_1 = 0.05\\) and \\(\\hat{\\beta}_2 = 1\\).\n\n(A) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.\n\nLogisticCOef <- -6 + 0.05 * 40 + 1* 3.5\n\nexp(LogisticCOef)/(1+exp(LogisticCOef))\n\n[1] 0.3775407\n\n\n(B) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\n\\[\n\\begin{split}\n\\log{ \\left( \\frac{p(X)}{1 - p(X)} \\right)} & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\n\\log{ \\left( \\frac{0.5}{1 - 0.5} \\right)} & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\n0 & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\nX_2 & = \\frac{-\\beta_0-\\beta_{2}X_2}{\\beta_{1}} \\\\\nX_2 & = \\frac{-(-6)-(1 \\times 3.5)}{0.05} \\\\\nX_2 & = 50 \\text{ horas}\n\\end{split}\n\\]\n\n\n\n\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was \\(\\overline{X}_y = 10\\), while the mean for those that didn’t was \\(\\overline{X}_n = 0\\). In addition, the variance of X for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, \\(80\\text{%}\\) of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(x = 4\\) last year.\n\n\\[\n\\begin{split}\np_{k}(x) & = \\frac{\\pi_{k} f_{k}(x)}\n                  {\\sum_{l=1}^{K} \\pi_{l} f_{l}(x)} \\\\\np_{y}(4) & =\n\\frac{0.8 \\times\n      \\left(\n      \\frac{e^{\\frac{-(4-10)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n      \\right)}\n{0.2 \\times\n\\left(\n\\frac{e^{\\frac{-(4-0)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n\\right)\n+\n0.8 \\times\n\\left(\n\\frac{e^{\\frac{-(4-10)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n\\right)\n} \\\\\np_{k}(4) & = 0.75\n\\end{split}\n\\]\n\nldm_k_prop(4,\n           k = 1,\n           sigma = c(6, 6),\n           pi_k = c(0.8, 0.2),\n           mu = c(10, 0)) |>\n  scales::percent(accuracy = 0.01)\n\n[1] \"75.19%\"\n\n\n\n\n\n\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures.\n\n\n\n\nModel\nTraining Error Rate\nTest Error Rate\n\n\n\n\nLogistic regression\n20%\n30%\n\n\n1-nearestbors\n0%\n36%\n\n\n\nBased on these results, which method should we prefer to use for classification of new observations? Why?\nWe should use the Logistic Regression as it has a lower test error rate than the logistic regression.\n\n\n\n\nThis problem has to do with odds.\n\n(A) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\\[\n\\begin{split}\n\\frac{p(X)}{1 - p(X)} & = Y \\\\\np(X) & = Y - Yp(X) \\\\\np(X) + Yp(X) & = Y \\\\\np(X) & = \\frac{Y}{1+Y} = \\frac{0.37}{1.37} = 0.27\n\\end{split}\n\\]\n(B) Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?\n\nscales::percent(0.16/(1-0.16), accuracy = 0.01) \n\n[1] \"19.05%\"\n\n\n\n\n\n\nEquation 4.32 derived an expression for \\(\\log{ \\left( \\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=K|X=x)} \\right)}\\) in the setting where \\(p > 1\\), so that the mean for the \\(k\\)th class, \\(\\mu_k\\), is p-dimensional vector, and the shared covariance \\(\\Sigma\\) is a \\(p \\times p\\) matrix. However, in the setting with \\(p=1\\), (4.32) takes a simpler form, since the means \\(\\mu_1,\\dots,\\mu_K\\) and the variance \\(\\sigma^2\\) are scalars In this simpler setting, repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(\\pi_k\\), \\(\\pi_k\\), \\(\\mu_k\\), \\(\\mu_K\\), and \\(\\sigma^2\\).\n\n\\[\n\\begin{split}\n\\log{ \\left( \\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=K|X=x)} \\right)}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                & =\n\\end{split}\n\\]\n\n\n\n\nWork out the detailed forms of \\(a_k\\), \\(b_{kj}\\) , and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(\\pi_k\\), \\(\\pi_k\\), \\(\\mu_k\\), \\(\\mu_K\\), \\(\\Sigma_k\\), and \\(\\Sigma_K\\).\n\n\n\n\n\nSuppose that you wish to classify an observation X ∈ R into apples and oranges. You fit a logistic regression model and find that\n\n\\[\n\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x) =\n\\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1x)}\n{1 + \\exp(\\hat{\\beta}_0+\\hat{\\beta}_1x)}\n\\]\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\\[\n\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x) =\n\\frac{\\exp(\\hat{\\alpha}_{\\text{orange}0}+\n           \\hat{\\alpha}_{\\text{orange}1}x)}\n{\\exp(\\hat{\\alpha}_{\\text{orange}0}+\n      \\hat{\\alpha}_{\\text{orange}1}x)+\n\\exp(\\hat{\\alpha}_{\\text{apple}0}+\n      \\hat{\\alpha}_{\\text{apple}1}x)}\n\\]\n(A) What is the log odds of orange versus apple in your model?\n\\[\n\\log{\n  \\left(\n    \\frac{\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x)}\n         {1 - \\widehat{\\text{Pr}}(Y = \\text{orange}|X=x)}\n  \\right)} =\n\\hat{\\beta}_0+\\hat{\\beta}_1x\n\\]\n(B) What is the log odds of orange versus apple in your friend’s model?\n\\[\n\\log{\n  \\left(\n    \\frac{\\exp(\\hat{\\alpha}_{\\text{orange}0} + \\hat{\\alpha}_{\\text{orange}1}x)}\n         {\\exp(\\hat{\\alpha}_{\\text{apple}0} + \\hat{\\alpha}_{\\text{apple}1}x)}\n  \\right)} =\n(\\hat{\\alpha}_{\\text{orange}0} - \\hat{\\alpha}_{\\text{apple}0}) +\n(\\hat{\\alpha}_{\\text{orange}1} - \\hat{\\alpha}_{\\text{apple}1}) x\n\\]\n(C) Suppose that in your model, \\(\\hat{\\beta}_0 = 2\\) and \\(\\hat{\\beta}_1 = -1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\\(\\hat{\\beta}_0 = 2\\) and \\(\\hat{\\beta}_1 = -1\\)\n(D) Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat{\\alpha}_{\\text{orange}0} = 1.2\\), \\(\\hat{\\alpha}_{\\text{orange}1} = −2\\), \\(\\hat{\\alpha}_{\\text{apple}0} = 3\\), \\(\\hat{\\alpha}_{\\text{apple}1} = 0.6\\). What are the coefficient estimates in your model?\n\\(\\hat{\\beta}_0 = 1.8\\) and \\(\\hat{\\beta}_1 = -2.6\\)\n(E) Finally, suppose you apply both models from (D) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\nBoth models will predict the same class for every test observation."
  },
  {
    "objectID": "04-execises.html#applied",
    "href": "04-execises.html#applied",
    "title": "04 - Classification",
    "section": "Applied",
    "text": "Applied\n\n13\n\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n\n(A) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n\nWeeklyDT <- \n  as.data.table(ISLR2::Weekly\n  )[,`:=`(id = .I,\n          # We want to predict the prob of going Up\n          Direction = factor(Direction, levels = c(\"Up\",\"Down\")))]\n\n\nggplot(WeeklyDT,aes(Year))+\n  geom_bar()+\n  scale_y_continuous(breaks = c(10*1:4,52))+\n  labs(title = \"1990 doesn't have all the weeks\")\n\n\n\n\n\nggplot(WeeklyDT,aes(id, Today, group = \"1\"))+\n  geom_point(aes(color = Direction))+\n  geom_smooth()+\n  ggtitle(\"The average return doesn't change with time\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nWeeklyDT[, .(sd = sd(Today),\n             Today = median(Today) >= 0),\n         by = \"Year\"] |>\n  ggplot(aes(Year, sd))+\n  geom_line(group = \"1\")+\n  geom_point(aes(color = Today))+\n  labs(title = \"The standar desviation changes over time\",\n       color = \"The median is over 0\")+\n  theme(legend.position = \"top\",\n        plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n(B) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\n\nlibrary(parsnip)\n\nWeeklyTrain <- WeeklyDT[,!c(\"id\", \"Year\",\"Today\")]\n\nLogisticModel <-\n  logistic_reg() |>\n  fit(Direction ~ ., data = WeeklyTrain)\n\nWeeklyTrainPred <-\n  WeeklyTrain[, cbind(.SD,\n                      predict(LogisticModel, \n                              new_data = .SD,  type = \"class\"))]\n\nsummary(LogisticModel$fit)\n\n\nCall:\nstats::glm(formula = Direction ~ ., family = stats::binomial, \n    data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4579  -1.0849  -0.9913   1.2565   1.6949  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept) -0.26686    0.08593  -3.106   0.0019 **\nLag1         0.04127    0.02641   1.563   0.1181   \nLag2        -0.05844    0.02686  -2.175   0.0296 * \nLag3         0.01606    0.02666   0.602   0.5469   \nLag4         0.02779    0.02646   1.050   0.2937   \nLag5         0.01447    0.02638   0.549   0.5833   \nVolume       0.02274    0.03690   0.616   0.5377   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1496.2  on 1088  degrees of freedom\nResidual deviance: 1486.4  on 1082  degrees of freedom\nAIC: 1500.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nLag2 it’s significant.\n\n(C) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n\n\nlibrary(yardstick)\n\nFor binary classification, the first factor level is assumed to be the event.\nUse the argument `event_level = \"second\"` to alter this as needed.\n\nget_class_metrics <- metric_set(sens, spec)\n\nconf_mat(WeeklyTrainPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction  Up Down\n      Up   557  430\n      Down  48   54\n\nget_class_metrics(WeeklyTrainPred,\n                  truth = Direction,\n                  estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.921\n2 spec    binary         0.112\n\n\nBy checking the specificity rate we can see that the model is making a bad job identifying when the number is going to get Down.\n\n(D) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n\n\nLogisticModelSplitYear <-\n  logistic_reg() |>\n  fit(Direction ~ Lag2, data =  WeeklyDT[Year <= 2008])\n\nWeeklyTestPred <-\n  WeeklyDT[Year > 2008, \n           cbind(.SD,\n                 predict(LogisticModelSplitYear, \n                         new_data = .SD,  type = \"class\"))]\n\nconf_mat(WeeklyTestPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction Up Down\n      Up   56   34\n      Down  5    9\n\nLogisticMetrics <-\n  get_class_metrics(WeeklyTestPred,\n                    truth = Direction,\n                    estimate = .pred_class)\n\nLogisticMetrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.918\n2 spec    binary         0.209\n\n\n\n(E) Repeat (d) using LDA.\n\n\nlibrary(discrim)\n\nLdaModelSplitYear <-\n  discrim_linear() |>\n  fit(Direction ~ Lag2, data =  WeeklyDT[Year <= 2008])\n\nLdaWeeklyTestPred <-\n  WeeklyDT[Year > 2008, \n           cbind(.SD,\n                 predict(LdaModelSplitYear, \n                         new_data = .SD,  type = \"class\"))]\n\nconf_mat(LdaWeeklyTestPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction Up Down\n      Up   56   34\n      Down  5    9\n\nLdaMetrics <-\n  get_class_metrics(LdaWeeklyTestPred,\n                    truth = Direction,\n                    estimate = .pred_class)\n\nLdaMetrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.918\n2 spec    binary         0.209\n\n\n\n(F) Repeat (d) using QDA.\n\n\nQdaModelSplitYear <-\n  discrim_quad() |>\n  fit(Direction ~ Lag2, data =  WeeklyDT[Year <= 2008])\n\nQdaWeeklyTestPred <-\n  WeeklyDT[Year > 2008, \n           cbind(.SD,\n                 predict(QdaModelSplitYear, \n                         new_data = .SD,  type = \"class\"))]\n\nconf_mat(QdaWeeklyTestPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction Up Down\n      Up   61   43\n      Down  0    0\n\nQdaMetrics <-\n  get_class_metrics(QdaWeeklyTestPred,\n                    truth = Direction,\n                    estimate = .pred_class)\n\nQdaMetrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary             1\n2 spec    binary             0\n\n\n\n(G) Repeat (d) using KNN with K = 1.\n\n\nlibrary(kknn)\n\nKnnModelSplitYear <-\n  nearest_neighbor(neighbors = 1) |>\n  set_mode(\"classification\") |>\n  set_engine(\"kknn\") |>\n  fit(Direction ~ Lag2, data =  WeeklyDT[Year <= 2008])\n\nKnnWeeklyTestPred <-\n  WeeklyDT[Year > 2008, \n           cbind(.SD,\n                 predict(KnnModelSplitYear, \n                         new_data = .SD,  type = \"class\"))]\n\nconf_mat(KnnWeeklyTestPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction Up Down\n      Up   30   21\n      Down 31   22\n\nKnnMetrics <-\n  get_class_metrics(KnnWeeklyTestPred,\n                    truth = Direction,\n                    estimate = .pred_class)\n\nKnnMetrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.492\n2 spec    binary         0.512\n\n\n\n(H) Repeat (d) using naive Bayes.\n\n\nlibrary(klaR)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\nNbModelSplitYear <-\n  naive_Bayes() |>\n  set_mode(\"classification\") |> \n  set_engine(\"klaR\") |>\n  set_args(usekernel = FALSE) |>\n  fit(Direction ~ Lag2, data =  WeeklyDT[Year <= 2008])\n\nNbWeeklyTestPred <-\n  WeeklyDT[Year > 2008, \n           cbind(.SD,\n                 predict(NbModelSplitYear, \n                         new_data = .SD,  type = \"class\"))]\n\nconf_mat(NbWeeklyTestPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction Up Down\n      Up   61   43\n      Down  0    0\n\nNbMetrics <-\n  get_class_metrics(NbWeeklyTestPred,\n                    truth = Direction,\n                    estimate = .pred_class)\n\nNbMetrics\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary             1\n2 spec    binary             0\n\n\n\n(I) Which of these methods appears to provide the best results on this data?\n\n\nlist(cbind(LogisticMetrics, model = \"logistic\"),\n     cbind(LdaMetrics, model = \"lda\"),\n     cbind(QdaMetrics, model = \"qda\"),\n     cbind(NbMetrics, model = \"nb\")) |>\n  rbindlist() |>\n  (\\(DT) DT[.metric == \"spec\"][order(-.estimate)])()\n\n   .metric .estimator .estimate    model\n    <char>     <char>     <num>   <char>\n1:    spec     binary 0.2093023 logistic\n2:    spec     binary 0.2093023      lda\n3:    spec     binary 0.0000000      qda\n4:    spec     binary 0.0000000       nb\n\n\nThe best models are Logistic Regression and the Linear Discriminant Analysis.\n\n(J) Experiment with diﬀerent combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classiﬁer."
  }
]