[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Statistical Learning (Non Official Solution Book)",
    "section": "",
    "text": "The purpose of creating this book is share the lessons learnt by reading and completing the exercises in the book.\nIt also could work as summary of the main point of the book."
  },
  {
    "objectID": "model-performance.html",
    "href": "model-performance.html",
    "title": "Understanding model performance",
    "section": "",
    "text": "The goal when we are analyzing data is to find a function that based on some Predictors and some random noise could explain the Response variable.\n\\[\nY = f(X) + \\epsilon\n\\]\n\\(\\epsilon\\) represent the random error and correspond to the irreducible error as it cannot be predicted using the Predictors in regression models. It would have a mean of 0 unless are missing some relevant Predictors.\nIn classification models, the irreducible error is represented by the Bayes Error Rate.\n\\[\n1 -  E\\left(\n     \\underset{j}{max}Pr(Y = j|X)\n     \\right)\n\\]\nAn error is reducible if we can improve the accuracy of \\(\\hat{f}\\) by using a most appropriate statistical learning technique to estimate \\(f\\).\nThe challenge to achieve that goal it’s that we don’t at the beginning how much of the error correspond to each type.\n\\[\n\\begin{split}\nE(Y-\\hat{Y})^2 & = E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\\n               & = \\underbrace{[f(X)- \\hat{f}(X)]^2}_\\text{Reducible} +\n                   \\underbrace{Var(\\epsilon)}_\\text{Irredicible}\n\\end{split}\n\\]\nThe reducible error can be also spitted in two parts:\n\nVariance refers to the amount by which \\(\\hat{f}\\) would change if we estimate it using a different training data set. If a method has high variance then small changes in the training data can result in large changes of \\(\\hat{f}\\).\nSquared bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model as for example a linear model.\n\n\\[\nE(y_{0} - \\hat{f}(x_{0}))^2 =\nVar(\\hat{f}(x_{0})) +\n[Bias(\\hat{f}(x_{0}))]^2 +\nVar(\\epsilon)\n\\]\n\n\n\n\n\n\nOur challenge lies in ﬁnding a method for which both the variance and the squared bias are low."
  },
  {
    "objectID": "model-performance.html#types-of-models",
    "href": "model-performance.html#types-of-models",
    "title": "Understanding model performance",
    "section": "Types of models",
    "text": "Types of models\n\nParametric methods\n\nMake an assumption about the functional form. For example, assuming linearity.\nEstimate a small number parameters based on training data.\nAre easy to interpret.\nTend to outperform non-parametric approaches when there is a small number of observations per predictor.\n\nNon-parametric methods\n\nDon’t make an assumption about the functional form, to accurately ﬁt a wider range of possible shapes for \\(f\\).\nNeed a large number of observations in order to obtain an accurate estimate for \\(f\\).\nThe data analyst must select a level of smoothness (degrees of freedom)."
  },
  {
    "objectID": "model-performance.html#evaluating-model-performance",
    "href": "model-performance.html#evaluating-model-performance",
    "title": "Understanding model performance",
    "section": "Evaluating model performance",
    "text": "Evaluating model performance\nTo evaluate how good works a models we need to split the available data in two parts.\n\nTraining data: Used to fit the model.\nTest data: Used to confirm how well the model works with new data.\n\nSome measurements to evaluate our test data are:\n\nTest mean squared error (MSE)\n\n\\[\nAve(y_{0}-\\hat{f}(x_{0}))^2\n\\]\n\n\n\n\n\n\nTest error rate\n\n\\[\nI(y_{0} \\neq \\hat{y}_{0}) =\n\\begin{cases}\n    1 & \\text{If } y_{0} \\neq \\hat{y}_{0} \\\\\n    0 & \\text{If } y_{0} = \\hat{y}_{0}\n\\end{cases}\n\\]\n\\[\nAve(I(y_{0} \\neq \\hat{y}_{0}))\n\\]\n\nConfusion Matrix\n\nIt contracts the model’s outputs with real values and makes easier to calculate more metrics to validate the model.\n\n\n\n\n\n\nSome metrics related with the confussion matrix are:\n\nSensitivity: Represents the percentage of positive values that have been correctly identiﬁed \\(\\text{TP}/\\text{P}\\).\nSpecificity: Represents the percentage of negative values that have been correctly identiﬁed \\(\\text{TN}/\\text{N}\\).\n\n\nYou can more metrics in the next table.\n\n\n\n\n\nThe ROC (receiver operating characteristics) Curve displays the two types of errors for all possible thresholds. The area under the curve (AUC) the represent overall performance of a classiﬁer, summarized over all possible thresholds and as larger the AUC the better the classiﬁer."
  },
  {
    "objectID": "generalized-linear-models.html",
    "href": "generalized-linear-models.html",
    "title": "Generalized linear models (GLM)",
    "section": "",
    "text": "As the population regression line is unobserved the least squares line of a sample is a good estimation. To get it we need to follow the next steps:\n\nDefine the function to fit.\n\n\\[\n\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x\n\\]\n\nDefine how to calculate residuals.\n\n\\[\ne_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nDefine the residual sum of squares (RSS).\n\n\\[\nRSS = e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\n\\]\n\nUse calculus or make estimation with a computer to find the coefficients that minimize the RSS.\n\n\\[\n\\hat{\\beta}_{1} = \\frac{\\Sigma_{i=1}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})}\n                       {\\Sigma_{i=1}^{n}(x_{i}-\\overline{x})}\n, \\quad\n\\hat{\\beta}_{0} = \\overline{y} - \\hat{\\beta}_{1}\\overline{x}\n\\]\n\nTo estimate the population regression line we can calculate conﬁdence intervals for sample coefficients, to define a range where we can find the population values with a defined confidence level.\n\nIf we want to use 95% of confidence we need to know that after taking many samples only 95% of the intervals produced with this confident level would have the true value (parameter).\n\nTo generate confident intervals we would need to calculate the variance of the random error.\n\\[\n\\sigma^2 = Var(\\epsilon)\n\\]\nBut as we can not calculate that variance an alternative can be to estimate it based on residuals if they meet the next conditions:\n\nEach residual have common variance \\(\\sigma^2\\), so the variances of the error terms shouldn’t have any relation with the value of the response.\nResiduals are uncorrelated. For example, if \\(\\epsilon_{i}\\) is positive, that provides little or no information about the sign of \\(\\epsilon_{i+1}\\).\n\nIf not, we would end underestimating the true standard errors, reducing the probability a given confident level to contain the true value of the parameter and underrating the p-values associated with the model.\n\\[\n\\sigma \\approx RSE = \\sqrt{\\frac{RSS}{(n-p-1)}}\n\\]\nNow we can calculate the standard error of each coefficient and calculate the confident intervals.\n\\[\nSE(\\hat{\\beta_{0}})^2 = \\sigma^2\n                       \\left[\\frac{1}{n}+\n                             \\frac{\\overline{x}^2}\n                                  {\\Sigma_{i=1}^{n} (x_{i}-\\overline{x})^2}\n                       \\right]\n\\]\n\\[\nSE(\\hat{\\beta_{1}})^2 = \\frac{\\sigma^2}\n                             {\\Sigma_{i=1}^{n} (x_{i} - \\overline{x})^2}\n\\]\n\\[  \n\\hat{\\beta_{1}} \\pm 2 \\cdot SE(\\hat{\\beta_{1}}), \\quad \\hat{\\beta_{0}} \\pm 2 \\cdot SE(\\hat{\\beta_{0}})\n\\]\n\n\nUse the regression overall P-value (based on the F-statistic) to confirm that at least one predictor is related with the Response and avoid interpretative problems associated with the number of observations (n) or predictors (p).\n\\[\nH_{0}: \\beta_{1} = \\beta_{2} = \\dots = \\beta_{p} = 0\n\\]\n\\[\nH_{a}: \\text{at least one } \\beta_{j} \\text{ is non-zero}\n\\]\n\nIf we want to know how well the model fits to the data we have two options:\n\nResidual standard error (RSE): Even if the model were correct, the actual values of \\(\\hat{y}\\) would differ from the true regression line by approximately this units, on average. To get the percentage error we can calculate \\(RSE/\\overline{x}\\)\nThe \\(R^2\\) statistic: The proportion of variance explained by taking as a reference the total sum of squares (TSS).\n\n\\[\nTSS = \\Sigma(y_{i} - \\overline{y})^2\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS}\n\\]\n\\[\nR^2 =\n\\begin{cases}\n    Cor(X, Y)^2  & \\text{Simple Lineal Regresion} \\\\\n    Cor(Y,\\hat{Y})^2 & \\text{Multipline Lineal Regresion}\n\\end{cases}\n\\]\n\nTo answer that we can test if a particular subset of q of the coefficients are zero.\n\\[\nH_{0}: \\beta_{p-q+1} = \\beta_{p-q+2} = \\dots = \\beta_{p} = 0\n\\]\nIn this case, F-statistic reports the partial eﬀect of adding a extra variable to the model (the order matters) to apply a variable selection technique. The classical approach is to:\n\nFit a model for each variable combination \\(2^p\\).\nSelect the best model based on Mallow’s Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted \\(R^2\\) or plot various model outputs, such as the residuals, in order to search for patterns.\n\nBut just think that if we have \\(p = 30\\) we will have \\(2^{30} = =1,073,741,824\\ models\\) to fit, that it’s too much. Some alternative approaches for this task:\n\nForward selection\nBackward selection (cannot be used if p >n)\nMixed selection\n\nTo check that we need to see the \\(\\hat{\\beta}_{j}\\) confident intervals as the real \\(\\beta_{j}\\) is in that range.\n\nIf we want to predict the average response \\(f(X)\\) we can use the confident intervals, but if we want to predict an individual response \\(Y = f(X) + \\epsilon\\) we need to use prediction intervals as they account for the uncertainty associated with \\(\\epsilon\\), the irreducible error.\n\n\nThe additivity assumption means that the association between a predictor \\(X_{j}\\) and the response \\(Y\\) does not depend on the values of the other predictors, as it happens when there is a interaction (synergy) effect\nThe linearity assumption states that the change in the response Y associated with a one-unit change in \\(X_{j}\\) is constant, regardless of the value of \\(X_{j}\\).\n\n\nThis approach relax the additivity assumption that models usually have.\n\n2 quantitative variables\n\nIt consist in adding an extra coefficient which multiplies two or more variables.\n\\[\n\\begin{split}\nY & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\beta_{3} X_{1} X_{2} + \\epsilon \\\\\n  & = \\beta_{0} + (\\beta_{1} + \\beta_{3} X_{2}) X_{1} + \\beta_{2} X_{2} + \\epsilon \\\\\n  & = \\beta_{0} + \\tilde{\\beta}_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon\n\\end{split}\n\\]\nAfter adding the interaction term we could interpret the change as making one of the original coefficient a function of the another variable. Now we could say that \\(\\beta_{3}\\) represent the change of \\(X_{1}\\) effectiveness associated with a one-unit increase in \\(X_{2}\\).\nIt very important that we keep hierarchical principle, which states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant as it would alter the meaning of the interaction.\n\n1 quantitative and 1 qualitative variable\n\nIf \\(X_{1}\\) is quantitative and \\(X_{2}\\) is qualitative:\n\\[\n\\hat{Y} =\n\\begin{cases}\n    (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3})X_{1} & \\text{if }X_{2} \\text{ is TRUE}\\\\\n    \\beta_{0} + \\beta_{1}X_{1}                             & \\text{if }X_{2} \\text{ is FALSE}\n\\end{cases}\n\\]\nAdding the \\(\\beta_{3}\\) interaction allow the line to change the line slope based on \\(X_{2}\\) and not just a different intercept.\n\n\n\n\n\nThis approach relax the linearity assumption that models usually have. It consist in including transformed versions of the predictors.\n\\[\n\\begin{split}\nY & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} \\\\\n  & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{1}^2\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nPlot the residuals versus predicted values \\(\\hat{y}_{i}\\). Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\nA simple approach is to use non-linear transformations of the predictors, such as \\(\\log{X}\\), \\(\\sqrt{X}\\), and \\(X^2\\), in the regression model\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n1. Plot the residuals from our model as a function of time or execution order. If the errors are uncorrelated, then there should be no discernible pattern.   2. Check if some observation have been exposed to the same environmental factors\nGood experimental design is crucial in order to mitigate these problems\n\n\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nPlot the residual plot en check if you can see a funnel shape\nWe can transform the response using a concave function such as \\(\\log{Y}\\) or \\(\\sqrt{Y}\\)\n\n\n\n\n\n\n\n\nAn outlier is a point for which \\(y_{i}\\) is far from the value predicted by the model. Sometimes, they have little effect on the least squares line, but over estimate the RSE making bigger p-values of the model and under estimate the \\(R^2\\).\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nPlot the studentized residuals, computed by dividing each residual \\(e_{i}\\) by its estimated standard error. Then search for points which absolute value is greater than 3\nThey can be removed if it has occurred due to an error in data collection. Otherwise, they may indicate a deficiency with the model, such as a missing predictor.\n\n\n\n\n\n\n\nObservations with high leverage have an unusual value for \\(x_{i}\\). High leverage observations tend to have a sizable impact on the estimated regression line and any problems with these points may invalidate the entire fit.\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nCompute the leverage statistic. Find an observation with higher value than mean, represented by \\((p + 1)/n\\). Leverage values are always between \\(1/n\\) and \\(1\\)\n\nMake sure that the value is correct and not a data collection problem\n\n\n\\[\nh_{i} = \\frac{1}{n} +\n        \\frac{(x_{i} - \\overline{x})^2}\n              {\\Sigma_{i'=1}^n(x_{i'} - \\overline{x})^2}\n\\]\nIn a multiple linear regression, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors.\n\n\n\n\n\nCollinearity refers to the situation in which two or more predictor variables are closely related (highly correlated) to one another. It reduces the accuracy of the estimates of the regression coeﬃcients and causes the standard error for \\(\\hat{\\beta}_{j}\\) to grow. That reduce the power of the hypothesis test,that is, the probability of correctly detecting a non-zero coeﬃcient.\nLooking at the correlation matrix of the predictors could be usefull, but it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation (multicollinearity).\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nThe best way to assess multicollinearity is to compute the variance inﬂation factor (VIF), which is the ratio of the variance of \\(\\hat{\\beta}_{j}\\) when ﬁtting the full model divided by the variance of \\(\\hat{\\beta}_{j}\\) if ﬁt on its own with 1 as its lowest value and 5 or 10 as problematic values of collinearity\n1. Drop one of the problematic variables from the regression.   2. Combine the collinear variables together into a single predictor\n\n\n\\[\n\\text{VIF}(\\hat{\\beta}_{j}) = \\frac{1}\n                                   {1 - R_{X_{j}|X_{-j}}^2}\n\\]\nWhere \\(R_{X_{j}|X_{-j}}^2\\) is the \\(R^2\\) from a regression of \\(X_{j}\\) onto all of the other predictors.\n\nThere are better model to achieve that kind of situation. For example, he linear discriminant analysis (LDA) procedure the same response of a linear regression for a binary problem. Other reasons are:\n\nA regression method cannot accommodate a qualitative response with more than two classes.\nA regression method will not provide meaningful estimates of \\(Pr(Y|X)\\) as some of our estimates might be outside the [0, 1] interval.\n\n\nPrediction Accuracy: If n is not much larger than p, then there can be a lot of variability in the least squares ﬁt, resulting in overﬁtting and consequently poor predictions on future observations not used in model training. And if p >n, then there is no longer a unique least squares coeﬃcient estimate: the variance is inﬁnite so the method cannot be used at all. As an alternative, We could reduce the variance by increasing in bias (constraining and shrinking).\nModel Interpretability:There are some methods that can exclude irrelevant variables from a multiple regression model (feature selection or variable selection).\n\nTo perform Linear Regression we just need to create the model specification by using lm engine.\n\nlibrary(ISLR2)\nlibrary(tidymodels)\n\nset.seed(123)\nBikeshareSpit <- initial_split(Bikeshare)\n\nlm_spec <- linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\nlm_rec_spec <- \n  recipe(bikers ~ mnth + hr + workingday + temp + weathersit,\n         data = Bikeshare ) %>% \n  step_dummy(all_nominal_predictors())\n\nworkflow() %>%\n  add_model(lm_spec) %>%\n  add_recipe(lm_rec_spec) %>%\n  last_fit(split = BikeshareSpit) %>%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      76.1   Preprocessor1_Model1\n2 rsq     standard       0.671 Preprocessor1_Model1"
  },
  {
    "objectID": "generalized-linear-models.html#subset-selection",
    "href": "generalized-linear-models.html#subset-selection",
    "title": "Generalized linear models (GLM)",
    "section": "Subset Selection",
    "text": "Subset Selection\nThis approach involves identifying a subset of the p predictors that we believe to be related to the response. We then ﬁt a model using least squares on the reduced set of variables.\nBest Subset Selection\nIt ﬁts all p models that contain exactly one predictor, all \\(\\left( \\begin{array}{c} p \\\\ 2 \\end{array} \\right) = p (p-1)/2\\) models that contain exactly two predictors, and so forth. Then it selects the best model based on smallest RSS or the largest \\(R^2\\).\n\n\nAlgorithm 6.1\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which represent the sample mean for each observation.\nFor \\(k = 1, 2, \\dots, p\\):\n\n\nFit all \\(\\left( \\begin{array}{c} p \\\\ k \\end{array} \\right)\\) models that contain exactly k predictors.\nPick the best among these \\(\\left( \\begin{array}{c} p \\\\ k \\end{array} \\right)\\) models using the smallest RSS or the deviance for classification (negative two times the maximized log-likelihood), and call it \\(\\mathcal{M}_k\\)\n\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\dots, \\mathcal{M}_p\\) using cross- validated prediction error, \\(C_p\\) (AIC), BIC or adjusted \\(R^2\\).\n\n\n\nThis method is really computational expensive as it needs to fit \\(2^p\\) models. Just think that if your data has 20 predicts, then there are over one million possibilities. Thus an enormous search space can also lead to overﬁtting and high variance of the coeﬃcient estimates.\nStepwise Selection: Forward Stepwise Selection\nIt begins with a model containing no predictors, and then adds the predictors who gives the greatest additional improvement to the ﬁt, one-at-a-time, until all of the predictors are in the model, as result we will need to fit \\(1+p(p+1)/2\\) models.\n\n\nAlgorithm 6.2\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which represent the sample mean for each observation.\nFor \\(k = 0, \\dots, p-1\\):\n\n\nConsider all \\(p-k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\nChoose the best among these \\(p-k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is deﬁned as having smallest RSS.\n\n\nSelect a single best model from among \\(\\mathcal{M}_k, \\dots, \\mathcal{M}_p\\) using cross- validated prediction error, \\(C_p\\) (AIC), BIC or adjusted \\(R^2\\).\n\n\n\nThough it tends to do well in practice, it is not guaranteed to ﬁnd the best possible model out of all \\(2^p\\) models containing subsets of the p predictors, but can be used even if \\(n < p\\).\nStepwise Selection: Backward Stepwise Selection\nIt begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time. As result we will need to fit \\(1+p(p+1)/2\\) models\n\n\nAlgorithm 6.3\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains all p predictors.\nFor \\(k = p, p -1, \\dots , 1\\):\n\n\nConsider all k models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k-1\\) predictors.\nChoose the best among these k models, and call it \\(\\mathcal{M}_{k-1}\\). Here best is deﬁned as having smallest RSS.\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\dots, \\mathcal{M}_p\\) using cross- validated prediction error, \\(C_p\\) (AIC), BIC or adjusted \\(R^2\\).\n\n\n\nThis method don’t guarantee to yield the best model containing a subset of the p predictors and we need to cofirm that \\(n \\geq p\\).\nStepwise Selection: Hybrid Approaches\nIn this method variables adds variables sequentially, but after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model ﬁt. Such an approach attempts to more closely mimic Best Subset Selection while retaining the computational advantages of forward and backward stepwise selection.\nChoosing the Optimal Model\nAs the model which contains all of the predictors will always have the smallest RSS, we need to estimate the test error rate by:\n\nMaking an adjustment to the training error to account for the bias due to overﬁtting.\nUsing either a validation set approach or a cross-validation approach.\n\nLet’s see the methods that relay on correcting the training error:\n\n\n\n\n\n\n\nMethod\nFormula\nInterpretation\n\n\n\n\\(C_p\\)\n\\(C_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2)\\)\n\n\\(2d\\hat{\\sigma}^2\\) represent the penalty of adding new predictors. This method is a good estimation of test MSE if the \\(\\hat{\\sigma}^2\\) is an unbiased estimate of the \\(\\sigma^2\\)\n\n\n\nAkaike information criterion\n\\(\\text{AIC} = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2)\\)\nThe book omits irrelevant constants to show that \\(C_p\\) and \\(\\text{AIC}\\) are proportional to each other\n\n\nBayesian information criterion\n\\(\\text{BIC} = \\frac{1}{n}(\\text{RSS} +\\log(n) d \\hat{\\sigma}^2)\\)\nAfter omitting irrelevant constants, we can see that \\(\\log n > 2\\) for any \\(n > 7\\), as consequence, the metric trends to add a heavier penalty on models with many variables than the \\(C_p\\) and tent to select models with fewer predictors\n\n\nAdjusted \\(R^2\\)\n\n\\(\\text{Adjusted} \\; R^2 = 1 - \\frac{\\text{RSS}/(n - d -1)}{\\text{TSS}/(n-1)}\\)\nA large value of adjusted \\(R^2\\) indicates a model with a small test error, even though the metric doesn’t rely on rigorous theoretical justiﬁcations.\n\n\n\nWhere:\n\n\n\\(n\\): Number of observations\n\n\\(d\\): Number of predictors\n\n\\(\\hat{\\sigma}^2\\): Estimate of the variance of the error \\(\\epsilon\\) associated with each response"
  },
  {
    "objectID": "generalized-linear-models.html#shrinkage",
    "href": "generalized-linear-models.html#shrinkage",
    "title": "Generalized linear models (GLM)",
    "section": "Shrinkage",
    "text": "Shrinkage\nThis approach involves ﬁtting a model involving all p predictors and shrinks the estimated coeﬃcients towards zero. Depending on what type of shrinkage is performed, some of the coeﬃcients may be estimated exactly at zero, performing some variable selection.\nRidge Regression\nThe method rather than using RSS as the metric to minimize with the regression coefficient \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\), it is modified by adding a shrinkage penalty that the effect of estimating \\(\\beta_j\\) towards zero when the coefficient is close to 0:\n\\[\nRSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nWhere:\n\n\n\\(\\lambda\\): It’s a tuning parameter \\(\\geq 0\\) that can be calculated using cross-validation.\n\n\\(\\text{RSS}\\): Present the residual standard error.\n\nAs result, the ridge regression will produce a diﬀerent set of coeﬃcient estimates for each \\(\\lambda\\) value, \\(\\hat{\\beta}_\\lambda^R\\). By plotting the new coefficients against the penalty used we can see how the coefficients move towards zero without reaching the absolute 0, but may also be useful to plot the \\(\\ell_2 \\; \\mathcal{norm}\\) (\\(\\| \\beta \\|_2 = \\sqrt{\\sum_{j=1}^p \\beta^2}\\)) proportion of the ridge vs least squares coefficients, to compute the proportion in which the ridge regression coeﬃcient estimate have been shrunken towards zero.\n\n\n\n\nAs the \\(\\lambda\\) to select is sensible to the predictors scaling, it’s a good practice scaling the predictors using the next formula, to make all the predictors to have an standard deviation of 1:\n\\[\n\\tilde{x}_{ij} =\n\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}} =\n\\frac{x_{ij}}{\\sigma_j}\n\\]\nWhen the number of variables p is almost as large as the number of observations n the least squares estimates will be extremely variable and this method increase the bias by reducing the flexibility through \\(\\lambda\\) to reduce the variance and find the lower error rate, without making many computations as happens the best subset selection method.\n\n\n\n\nLasso Regression\nRidge Regression don’t set the coefficient exactly to zero (unless \\(\\lambda = \\infty\\)). That doesn’t affect the model accuracy but doesn’t provide any help when we have to interpret a model with many predicts. To over come that problem, the Lasso Regression performs variable selection based on minimization of the next function:\n\\[\nRSS + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\]\nWhere:\n\n\n\\(\\lambda\\): It’s a tuning parameter \\(\\geq 0\\) that can be calculated using cross-validation.\n\n\\(\\text{RSS}\\): Present the residual standard error.\n\nAs consequence, the method uses the \\(\\ell_1\\) penalty (\\(\\| \\beta \\|_1 = \\sum_{j=1}^p |\\beta|\\)), instead of the \\(\\ell_2\\).\n\n\n\n\nCoding example\nTo perform a Ridge or Lasso regression we need to use the function linear_reg and define the mixture argument depending on the regression we want to perform.\n\n\nModel\nMixture\n\n\n\nRidge Regression\nmixture = 0\n\n\n\nLasso Regression\nmixture = 1\n\n\n\n\nAs both regression depend on the penalty parameter we need to use cross-validation to estimate the best one. But, if you want to explore the parameter by yourself you has the next options.\n\nFit a model and explore different penalties using tidy, augment, predict or autoplot functions.\n\n\nHitters <- \n  as_tibble(Hitters) %>%\n  filter(!is.na(Salary))\n\nridge_fit <- \n  linear_reg(mixture = 0, penalty = 0) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"glmnet\") %>%\n  fit(Salary ~ ., data = Hitters)\n\ntidy(ridge_fit, penalty = 11498)\n\n# A tibble: 20 × 3\n   term         estimate penalty\n   <chr>           <dbl>   <dbl>\n 1 (Intercept) 407.        11498\n 2 AtBat         0.0370    11498\n 3 Hits          0.138     11498\n 4 HmRun         0.525     11498\n 5 Runs          0.231     11498\n 6 RBI           0.240     11498\n 7 Walks         0.290     11498\n 8 Years         1.11      11498\n 9 CAtBat        0.00314   11498\n10 CHits         0.0117    11498\n11 CHmRun        0.0876    11498\n12 CRuns         0.0234    11498\n13 CRBI          0.0242    11498\n14 CWalks        0.0250    11498\n15 LeagueN       0.0866    11498\n16 DivisionW    -6.23      11498\n17 PutOuts       0.0165    11498\n18 Assists       0.00262   11498\n19 Errors       -0.0206    11498\n20 NewLeagueN    0.303     11498\n\npredict(ridge_fit, new_data = Hitters, penalty = 11498)\n\n# A tibble: 263 × 1\n   .pred\n   <dbl>\n 1  533.\n 2  553.\n 3  620.\n 4  487.\n 5  559.\n 6  440.\n 7  446.\n 8  453.\n 9  620.\n10  615.\n# ℹ 253 more rows\n\nautoplot(ridge_fit)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()\n\n\n\n\nLet’s have a example using the tidymodels approach. By following the next steps:\n\nSplitting the data in testing and training data.\n\n\nset.seed(18)\nHitters_split <- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train <- training(Hitters_split)\nHitters_test <- testing(Hitters_split)\n\n\nDefine the workflow to use.\n\n\nridge_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())\n\nridge_spec <- \n  linear_reg(penalty = tune(), mixture = 0) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\")\n\nridge_workflow <- \n  workflow() %>% \n  add_recipe(ridge_recipe) %>% \n  add_model(ridge_spec)\n\n\nUse cross-validation to estimate the testing error and tune the penalty.\n\n\nset.seed(40)\nHitters_fold <- vfold_cv(Hitters_train, v = 10)\n\n\nDefine the penalties to check. Where the range indicates the limit of \\(x\\) in the function \\(10^x\\) and the level the number of step to complete the range.\n\n\npenalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\n\nLet’s fit our models.\n\n\ntune_res <- \n  tune_grid(ridge_workflow,\n            resamples = Hitters_fold, \n            grid = penalty_grid)\n\n\nCheck the test error change in a plot or just export a table.\n\n\nautoplot(tune_res)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()\n\n\n\ncollect_metrics(tune_res) %>%\n  filter(.metric == \"rsq\") %>%\n  arrange(desc(mean))\n\n# A tibble: 50 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1    910. rsq     standard   0.454    10  0.0647 Preprocessor1_Model40\n 2    569. rsq     standard   0.454    10  0.0646 Preprocessor1_Model39\n 3   1456. rsq     standard   0.453    10  0.0646 Preprocessor1_Model41\n 4    356. rsq     standard   0.453    10  0.0644 Preprocessor1_Model38\n 5   2330. rsq     standard   0.452    10  0.0645 Preprocessor1_Model42\n 6    222. rsq     standard   0.450    10  0.0640 Preprocessor1_Model37\n 7   3728. rsq     standard   0.450    10  0.0643 Preprocessor1_Model43\n 8   5964. rsq     standard   0.449    10  0.0642 Preprocessor1_Model44\n 9   9541. rsq     standard   0.448    10  0.0641 Preprocessor1_Model45\n10    139. rsq     standard   0.448    10  0.0634 Preprocessor1_Model36\n# ℹ 40 more rows\n\n\n\nSelect the best model configuration and update the workflow to use that parameter.\n\n\nbest_penalty <- select_best(tune_res, metric = \"rsq\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1    910. Preprocessor1_Model40\n\nridge_final <- finalize_workflow(ridge_workflow, best_penalty)\nridge_final\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 910.298177991523\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\nFit the final model and validate the performance.\n\n\nridge_final_fit <- fit(ridge_final, data = Hitters_train)\n\naugment(ridge_final_fit, new_data = Hitters_test) %>%\n  rsq(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.514\n\n\nTo perform a Lasso regression we don’t need to repeat all the code.\n\nlasso_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\")\n\nlasso_workflow <- \n  workflow() %>% \n  add_recipe(ridge_recipe) %>% \n  add_model(lasso_spec)\n\nlasso_penalty_grid <- grid_regular(penalty(range = c(-3, 2)), levels = 50)\n\nlasso_tune_res <- \n  tune_grid(lasso_workflow,\n            resamples = Hitters_fold, \n            grid = lasso_penalty_grid)\n\nautoplot(lasso_tune_res)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()"
  },
  {
    "objectID": "generalized-linear-models.html#dimension-reduction",
    "href": "generalized-linear-models.html#dimension-reduction",
    "title": "Generalized linear models (GLM)",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\nThis method projects the \\(p\\) predictors into an \\(M\\)-dimensional subspace. If \\(Z_1, Z_2, \\dots, Z_M\\) represent \\(M < p\\) lineal combinations \\(Z_m = \\sum_{j=1}^p \\phi_{jm}X_j\\) of ALL our original predictors based on some constants \\(\\phi_{1m}, \\phi_{2m}, \\dots, \\phi_{pm}\\), then we can use the new variables to fit a linear regression model by least squares.\n\\[\ny_i = \\theta_0 + \\sum_{m=1}^M \\theta_m z_{im} + \\epsilon_i,\n\\qquad i=1, \\dots, n.\n\\]\nThis is not a feature selection method as each of the M principal components used in the regression is a linear combination of all p of the original features.In this sense, PCR is more closely related to ridge regression than to the lasso.\nTo select the \\(\\phi_{jm}\\)’s we will discuss two different ways:\nPrincipal Components Regression (PCR)\nThe PCR assumes that the directions in which \\(X_1, \\dots, X_M\\) show the most variation are the directions that are associated with \\(Y\\). If it’s that is true then fitting the model to \\(Z_1, \\dots, Z_m\\) will lead better results than using the original variables \\(X_1, \\dots, X_p\\)\nTo perform a principal components analysis (PCA):\n\nIt’s recommended to standardize each predictor\n\n\\[\n\\tilde{x}_{ij} =\n\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}} =\n\\frac{x_{ij}}{\\sigma_j}\n\\]\n\nSelect as the ﬁrst principal component the variable where the data vary the most.\n\n\n\n\n\n\nProject the observations on the first component, to get the largest possible variance\n\n\n\n\n\n\nThen maximize the \\(\\text{Var}(\\phi_{11} \\times (x_1-\\overline{x_1}) + \\phi_{21} \\times (x_2-\\overline{x_2}))\\) where \\(\\phi_{11}^2 + \\phi_{21}^2 = 1\\) to the get the principal component loadings. As result \\(Z_1\\) it’s a weighted average of the to variables.\nRepeat the process until having p distinct principal components perpendicular to the previews one.\n\nIn general, this method performs better when we just need to use few principal components.\n\n\n\n\n\n\n\n\nThe number of principal components, M, is typically chosen by cross-validation.\nPartial Least Squares (PLS)\nThe PLS approach attempts to ﬁnd directions that help explain both the response and the predictors by placing the highest weight on the variables that are most strongly related to the response.\n\nStandardize the predictors and response.\n\n\\[\n\\tilde{x}_{ij} =\n\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}} =\n\\frac{x_{ij}}{\\sigma_j}\n\\]\n\nCompute the ﬁrst direction \\(Z_1\\) by setting each \\(\\phi_{j1}\\) equal to the coeﬃcient from the simple linear regression of \\(Y \\sim X_j\\), which it’s also proportional to the correlation.\nAdjust each of the variables for \\(Z_1\\), by regressing each variable on \\(Z_1\\) and taking residuals.\nCompute \\(Z_2\\) using this orthogonalized data (residuals) in exactly the same fashion as \\(Z_1\\) was computed based on the original data and repete the process \\(M\\) times.\nUse least squares to ﬁt a linear model to predict \\(Y\\) using \\(Z_1, \\dots, Z_M\\)\n\nTo select the number \\(M\\) we can use cross-validation.\n\n\n\n\n\n\nPerformance Note\n\n\n\nIn practice, it often performs no better than ridge regression or PCR. While the supervised dimension reduction of PLS can reduce bias, it also has the potential to increase variance, so that the overall beneﬁt of PLS relative to PCR is a wash.\n\n\nCoding example\nTo run a PCR or a PLS we just need to set a lineal model.\n\nlm_spec <- \n  linear_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"lm\")\n\nAnd tune our recipe to determinate the threshold and the number of components to use num_comp.\n\npca_recipe <- \n  recipe(formula = Salary ~ ., data = Hitters_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), \n           threshold = tune(),\n           num_comp = tune())\n\npca_workflow <- \n  workflow() %>% \n  add_recipe(pca_recipe) %>% \n  add_model(lm_spec)\n\nthreshold_grid <- \n  grid_regular(threshold(), \n               num_comp(c(1, 20)),\n               levels = 5)\n\npca_tune_res <- \n  tune_grid(pca_workflow,\n            resamples = Hitters_fold, \n            grid = threshold_grid)\n\nAs we can see below the number of component don’t have any effect over the test error.\n\nautoplot(pca_tune_res)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()\n\n\n\nbest_threshold <- select_best(pca_tune_res, metric = \"rmse\")\nbest_threshold\n\n# A tibble: 1 × 3\n  num_comp threshold .config              \n     <int>     <dbl> <chr>                \n1        1      0.75 Preprocessor04_Model1\n\nfinalize_workflow(pca_workflow, best_threshold) |>\n  fit(data = Hitters_train) |>\n  augment(new_data = Hitters_test) |>\n  rmse(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        402."
  },
  {
    "objectID": "generalized-linear-models.html#poisson-regression",
    "href": "generalized-linear-models.html#poisson-regression",
    "title": "Generalized linear models (GLM)",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nIf \\(Y \\in \\{ 0, 1, 2, 3, \\dots \\}\\) that could be the result after counting a particular event the linear regression might not meet our needs as it could bring negative numbers. The Poisson Distribution follow the next function:\n\\[\nPr(Y = k) = \\frac{e^{-\\lambda} \\lambda^{k}}\n                 {k!}\n\\]\nWhere: - \\(\\lambda\\) must be greater than 0. It represents the expected number of events \\(E(Y)\\) and variance related \\(Var(Y)\\) - \\(k\\) represent the number of events that we want to evaluate base of \\(\\lambda\\). Its numbers should be greater or equal to 0.\nSo, it makes sense that the value that we want to predict with our regression would be \\(\\lambda\\), by using next structure:\n\\[\n\\log{ \\left( \\lambda(X_1, X_2, \\dots , X_p)  \\right)} = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\nIf select this model we need to be aware how to interpret the coefficients. For example, if \\(\\beta_1 = -0.08\\) for a categorical variable, we can conclude by calculating \\(e^{-0.08}\\) that 92.31% of events of the base line related to \\(\\beta_0\\) would happen.\nCoding example\nTo perform Poisson Regression we just need to create the model specification by loading the poissonreg package and using glm engine.\n\nlibrary(poissonreg)\n\npois_spec <- poisson_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glm\")\n\npois_rec_spec <- lm_rec_spec\n\nworkflow() %>%\n  add_model(pois_spec) %>%\n  add_recipe(pois_rec_spec) %>%\n  last_fit(split = BikeshareSpit) %>%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      69.7   Preprocessor1_Model1\n2 rsq     standard       0.725 Preprocessor1_Model1"
  },
  {
    "objectID": "generalized-linear-models.html#logistic-regression",
    "href": "generalized-linear-models.html#logistic-regression",
    "title": "Generalized linear models (GLM)",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIt models the probability (\\(p(X) = Pr(Y=1|X)\\)) that Y belongs to a particular category given some predictors by assuming that \\(Y\\) follows a Bernoulli Distribution. This model calculates the probability using the logistic function which produce a S form between 0 and 1:\n\\[\np(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X}}\n            {1+e^{\\beta_{0}+\\beta_{1}X}}\n\\]\n\n\n\n\nAs the functions returns probabilities is responsibility of the analyst to define a threshold to make classifications.\nEstimating coefficients\nTo estimate \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) the method used is called as maximum likelihood which consists in maximizing the likelihood function. It is important to clarify that the least squares approach is in fact a special case of maximum likelihood.\n\\[\n\\ell(\\beta_{0}, \\beta_{1}) = \\prod_{i:y_{i} = 1}p(x_{i})\\prod_{i':y_{i'} = 0}p(1-x_{i'})\n\\]\nMultiple regression\nWe also can generalize the logistic function as you can see bellow.\n\\[\np(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}\n            {1+e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}\n\\]\nInterpreting the model\nTo understand how each variable influence the probability \\(p(X)\\), we need to manipulate the logistic function until having a lineal combination on the right site.\n\\[\n\\underbrace{ \\log{ \\left( \\overbrace{\\frac{p(X)}{1 - p(X)}}^\\text{odds ratio} \\right)} }_\\text{log odds or logit} = \\beta_{0}+\\beta_{1}X\n\\]\nAs we can see, the result of the linear combination is the \\(\\log\\) of the odds ratio, known as log odd or logit.\nAn odds ratio of an event presents the likelihood that the event will occur as a proportion of the likelihood that the event won’t occur. It can take any value between \\(0\\) and \\(\\infty\\), where low probabilities are close to \\(0\\), higher to \\(\\infty\\) and equivalents ones are equals to 1. For example, if we have an \\(\\text{odds ratio} = 2\\), we can say that it’s 2 times more likely that the event happens rather than not.\nApplying \\(\\log{(\\text{odds ratio})}\\) makes easier to compare the effect of variables as values below 1 become negative numbers of the scale of possible numbers and 1 becomes 0 for non-significant ones. To have an idea, an odds ratio of 2 has the same effect as 0.5, which it’s hard to see at first hand, but if we apply the \\(\\log\\) to each value we can see that \\(\\log{(2)} = 0.69\\) and \\(\\log{(0.5)} = -0.69\\).\nAt end, \\(p(X)\\) will increase as \\(X\\) increases if \\(\\beta_{1}\\) is positive despite the relationship between each other isn’t a linear one.\nUnderstanding a confounding paradox\n\n\n\n\n\n\nSimple Regression\nMultiple Regression\n\n\n\n\n\n\n\nThe positive coeﬃcient for student indicates that for over all values of balance and income, a student is more likely to default than a non-student.\nThe negative coeﬃcient for student indicates that for a ﬁxed value of balance and income, a student is less likely to default than a non-student.\n\n\n\n\n\n\n\nThe problem relays on the fact that student and balance are correlated. In consequence, a student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!\n\n\n\n\nMultinomial Logistic Regression\nWe also can generalize the logistic function to support more than 2 categories (\\(K > 2\\)) by defining by convention the last category \\(K\\) as a baseline.\nFor \\(k = 1, \\dotsc,K-1\\) we use function.\n\\[\nPr(Y = k|X= x) = \\frac{e^{\\beta_{k0}+\\beta_{k1}x_{1}+\\dots+\\beta_{kp}x_{p}}}\n                      {1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_{l1}x_{1}+\\dots+\\beta_{lp}x_{p}}}\n\\]\nFor \\(k=K\\), we use the function.\n\\[\nPr(Y = K|X= x) = \\frac{1}\n                      {1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_{l1}x_{1}+\\dots+\\beta_{lp}x_{p}}}\n\\] And after some manipulations we can show that \\(\\log\\) of the probability of getting \\(k\\) divided by the probability of the baseline is equivalent to a linear combinations of the functions parameters.\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|X= x)}{Pr(Y = K|X= x)} \\right)} = \\beta_{k0}+\\beta_{k1}x_{1}+\\dots+\\beta_{kp}x_{p}\n\\]\nIn consequence, each coefficient represent a measure of how much change the probability from the baseline probability.\nModel limitatios\nThere are models that could make better classifications when:\n\nThere is a substantial separation between the \\(Y\\) classes.\nThe predictors \\(X\\) are approximately normal in each class and the sample size is small.\nWhen the decision boundary is not lineal.\nCoding example\nTo perform Logistic Regression we just need to create the model specification by loading the discrim package and using MASS engine.\n\nSmarket_train <- \n  Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- \n  Smarket %>%\n  filter(Year == 2005)\n\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\nSmarketLrPredictions <-\n  lr_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |>\n  augment(new_data = Smarket_test) \n\n\nconf_mat(SmarketLrPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n\naccuracy(SmarketLrPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.560"
  },
  {
    "objectID": "generative-classification-models.html",
    "href": "generative-classification-models.html",
    "title": "Generative Models for Classiﬁcation",
    "section": "",
    "text": "These models instead of trying to predict the posterior probability ( \\(Pr(Y=k|X=x)\\) ) directly, they try to estimate the distribution of the predictors \\(X\\) separately in each of the response classes \\(Y\\) ( \\(f_{k}(X) = Pr(X|Y=k)\\) ). Then, they use the Bayes’ Theorem and the overall or prior probability \\(\\pi_{k}\\) (probability of a randomly chosen observation comes from the \\(k\\)th class) to flip these around into estimates for \\(Pr(Y=k|X=x)\\) by approximating the Bayes Classifier, which has the lowest total error rate.\n\\[\np_{k}(x) = Pr(Y = k | X = x) = \\frac{\\pi_{k} f_{k}(x)} {\\sum_{l=1}^{K} \\pi_{l} f_{l}(x)}\n\\]\nEstimating the prior probability can be as easy calculate \\(\\hat{\\pi}_{k} = n_{k}/ n\\) for each \\(Y\\) class by assuming that the trainning data its representative of the population, but estimating the density function of \\(X\\) for each class \\(f_{k}\\) it’s more challenging, so models need to make more simplifying assumptions to estimate it."
  },
  {
    "objectID": "generative-classification-models.html#linear-discriminant-analysis-lda",
    "href": "generative-classification-models.html#linear-discriminant-analysis-lda",
    "title": "Generative Models for Classiﬁcation",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\nThis model assumes that:\n\nThe density function of \\(X\\) for each \\(Y\\) class \\(f_{k}\\) follows a Normal (Gaussian) distribution within each class. Even though, it is often remarkably robust to model violations like Boolean variables.\n\n\\(X\\) has a different mean across all \\(Y\\) classes \\(\\mu_{1}^2 \\neq \\dots \\neq \\mu_{k}^2\\).\n\n\\(X\\) has a common variance across all \\(Y\\) classes \\(\\sigma_{1}^2 = \\dots = \\sigma_{k}^2\\).\n\nTo understand how the model calculates its parameters, let’s see the discriminant function when the number of predictors is \\(p=1\\) and the number of \\(Y\\) classes is \\(K=2\\).\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{ \\left( p_{x}(x) \\right)} \\\\\n              & = \\log{(\\pi_{k})}\n                - \\frac{\\mu_{k}^2}{2\\sigma^2}\n                + x \\cdot \\frac{\\mu_{k}}{\\sigma^2}\n\\end{split}\n\\]\nIn this function, it’s clear that a class \\(k\\) has more possibilities to be selected as mean of \\(x\\) for that particular class increases and its variance decreases. It is also important to take in consideration the effect of \\(\\log{(\\pi_{k})}\\), in consequence the proportion of classes also influence the results.\nIf we want to extend the model to work with \\(p \\geq 1\\) we also need to consider that:\n\nEach individual predictor follows a one-dimensional normal distribution\nThere is some correlation between each pair of predictors\n\nAs result, the discriminant function is:\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{\\pi_{k}}  - \\frac{1}{2} \\mu_{k}^T \\Sigma^{-1} \\mu_{k} \\\\\n                & \\quad + x^T \\Sigma^{-1} \\mu_{k}\n\\end{split}                      \n\\]\n\n\nWhere:\n\n\n\\(x\\) refers to a vector the current value of each \\(p\\) element.\n\n\\(\\mu\\) refers to a vector with the mean of each predictor.\n\n\\(\\Sigma\\) refers to the covariance matrix \\(p \\times p\\) of \\(\\text{Cov}(X)\\).\n\n\n\nThe model also can be extended to handle \\(K > 2\\) after defining the \\(K\\) class as the baseline, we can extend the discriminant function to have the next form:\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{ \\left(\n                        \\frac{Pr(Y = k|K=x)}\n                             {Pr(Y=K|X=x)}\n                      \\right)} \\\\\n              & = \\log{ \\left( \\frac{\\pi_{k}}{\\pi_{K}} \\right)}\n                  - \\frac{1}{2} (\\mu_{k} + \\mu_{K})^T \\Sigma^{-1} (\\mu_{k} - \\mu_{K}) \\\\\n              & \\quad + x^{T} \\Sigma^{-1} (\\mu_{k} - \\mu_{K})\n\\end{split}\n\\] ### Coding example\nTo perform LDA we just need to create the model specification by loading the discrim package and using MASS engine.\n\nlibrary(tidymodels)\nlibrary(ISLR) \nlibrary(discrim)\n\nSmarket_train <- \n  Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- \n  Smarket %>%\n  filter(Year == 2005)\n\nlda_spec <- discrim_linear() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nSmarketLdaPredictions <-\n  lda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |>\n  augment(new_data = Smarket_test) \n\n\nconf_mat(SmarketLdaPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n\naccuracy(SmarketLdaPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.560"
  },
  {
    "objectID": "generative-classification-models.html#quadratic-discriminant-analysis-qda",
    "href": "generative-classification-models.html#quadratic-discriminant-analysis-qda",
    "title": "Generative Models for Classiﬁcation",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nLike LDA, the QDA classiﬁer plugs estimates for the parameters into Bayes’ theorem in order to perform prediction results and assumes that:\n\nThe observations from each class are drawn from a Gaussian distribution\nEach class has its own covariance matrix, \\(X \\sim N(\\mu_{k}, \\Sigma_{k})\\)\n\n\nUnder this assumption, the Bayes classiﬁer assigns an observation \\(X = x\\) to the class for which \\(\\delta_{k}(x)\\) is largest.\n\\[\n\\begin{split}\n\\delta_{k}(x) = & \\quad \\log{\\pi_{k}}\n                - \\frac{1}{2} \\log{|\\Sigma_{k}|}\n                - \\frac{1}{2} \\mu_{k}^T \\Sigma_{k}^{-1}\\mu_{k} \\\\\n              & + x^T \\Sigma_{k}^{-1} \\mu_{k} \\\\\n              & - \\frac{1}{2} x^T \\Sigma_{k}^{-1} x\n\\end{split}                  \n\\]\nIn consequence, QDA is more flexible than LDA and has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes or when we need non-linear decision boundaries.\nThe model also can be extended to handle \\(K > 2\\) after defining the \\(K\\) class as the baseline, we can extend the discriminant function to have the next form:\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|K=x)}{Pr(Y=K|X=x)} \\right)} =\na_k + \\sum_{j=1}^{p}b_{kj}x_{j} +\n      \\sum_{j=1}^{p} \\sum_{l=1}^{p} c_{kjl} x_{j}x_{l}\n\\]\nWhere \\(a_k\\), \\(b_{kj}\\) and \\(c_{kjl}\\) are functions of \\(\\pi_{k}\\), \\(\\pi_{K}\\), \\(\\mu_{k}\\), \\(\\mu_{K}\\), \\(\\Sigma_{k}\\) and \\(\\Sigma_{K}\\)\nCoding example\nTo perform QDA we just need to create the model specification by loading the discrim package and using MASS engine.\n\nqda_spec <- discrim_quad() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nSmarketQdaPredictions <-\n  qda_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |>\n  augment(new_data = Smarket_test) \n\n\nconf_mat(SmarketQdaPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   30  20\n      Up     81 121\n\naccuracy(SmarketQdaPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.599"
  },
  {
    "objectID": "generative-classification-models.html#naive-bayes",
    "href": "generative-classification-models.html#naive-bayes",
    "title": "Generative Models for Classiﬁcation",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nTo estimate \\(f_{k}(X)\\) this model assumes that Within the kth class, the p predictors are independent (correlation = 0) and as consequence:\n\\[\nf_{k}(x) = f_{k1}(x_{1}) \\times f_{k2}(x_{2}) \\times \\dots \\times f_{kp}(x_{p})\n\\]\nEven thought the assumption might not be true, the model often leads to pretty decent results, especially in settings where n is not large enough relative to p for us to eﬀectively estimate the joint distribution of the predictors within each class. It has been used to classify text data, for example, to predict whether an email is spam or not.\nTo estimate the one-dimensional density function \\(f_{kj}\\) using training data we have the following options:\n\nWe can assume that \\(X_{j}|Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2)\\)\n\nWe can estimate the distribution by defining bins and creating a histogram\nWe can estimate the distribution by use a kernel density estimator\nIf \\(X_{j}\\) is qualitative, we can count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\n\nThe model also can be extended to handle \\(K > 2\\) after defining the \\(K\\) class as the baseline, we can extend the function to have the next form:\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|K=x)}{Pr(Y=K|X=x)} \\right)} =\n\\log{ \\left(\n        \\frac{\\pi_{k}}\n             {\\pi_{K}}\n      \\right)}\n+\n\\log{ \\left(\n        \\frac{\\prod_{j=1}^{p} f_{kj}(x_{j}) }\n             {\\prod_{j=1}^{p} f_{Kj}(x_{j}) }\n      \\right)}\n\\]\nThe infrequent problem\nThe method has the problem that if you don’t an example for a particular event in your training set it would estimate the probability of that event as 0.\n\nThe solution to this problem involves adding a small number, usually ‘1’, to each event and outcome combination to eliminate this veto power. This is called the Laplace correction or Laplace estimator. After adding this correction, each Venn diagram now has at least a small bit of overlap; there is no longer any joint probability of zero.\n\nPre-processing\nThis method works better with categories, so if your data has numeric data try to bin it in categories by:\n\nTurning an Age variable in the ‘child’ or ‘adult’ categories\nTurning geographic coordinates into geographic regions like ‘West’ or ‘East’\nTurning test scores into four groups by percentile\nTurning hour into ‘morning’, ‘afternoon’ and ‘evening’\nTurning temperature into ‘cold’, ‘warm’ and ‘hot’\n\nAs this method works really well when we have few examples and many predictors we can transform text documents into a Document Term Matrix (DTM) using a bag-of-words model with package like tidytext or tm.\nCoding example\nTo perform Naive Bayes we just need to create the model specification by loading the discrim package and using klaR engine. We can apply Laplace correction by setting Laplace = 1 in the parsnip::naive_Bayes function.\n\nnb_spec <- naive_Bayes() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"klaR\") %>% \n  set_args(usekernel = FALSE) \n\nSmarketNbPredictions <-\n  nb_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |>\n  augment(new_data = Smarket_test) \n\nconf_mat(SmarketNbPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   28  20\n      Up     83 121\n\naccuracy(SmarketNbPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.591"
  },
  {
    "objectID": "flexible-regression.html",
    "href": "flexible-regression.html",
    "title": "Flexible Regression Models",
    "section": "",
    "text": "In this book the main goal is to provide more tools to explore the relation of our response and a single predictor, that could be useful when we are are performing an EDA and the extend that power using generalized additive models."
  },
  {
    "objectID": "flexible-regression.html#based-on-linear-regression",
    "href": "flexible-regression.html#based-on-linear-regression",
    "title": "Flexible Regression Models",
    "section": "Based on linear regression",
    "text": "Based on linear regression\n\nPolynomial regression\nIt extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power.\nAs result, if the response is a numeric variable we can fit our model to follow the next form:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\n\n\n\n\n\nOn the other hand, we can use the logistic regression and apply the same structure to predict the probability of particular class:\n\\[\n\\Pr(y_i > 250|x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d)}\n                          {1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d)}\n\\]\n\n\n\n\n\n\n\nPiecewise constant regression\nThey cut the range of a variable into K distinct regions (known as bins) in order to produce a qualitative variable. This has the eﬀect of ﬁtting a piecewise constant function.\nIf we define the cutpoints as \\(c_1, c_2, \\dots, c_K\\) in the range of X, we can create dummy variables to represent each range. For example, if \\(c_1 \\leq x_i < c_2\\) is TRUE then \\(C_1(x_i) = 1\\) and then we need to repeat that process for each value of \\(X\\) and range. As result we can fit a lineal regression based on the new variables.\n\\[\ny_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i)  \\dots + \\beta_K C_K(x_i) + \\epsilon_i\n\\]\n\n\n\n\n\nOn the other hand, we can use the logistic regression and apply the same structure to predict the probability of particular class:\n\\[\n\\Pr(y_i > 250|x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i)  \\dots + \\beta_K C_K(x_i)}\n                          {1 + \\exp(\\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i)  \\dots + \\beta_K C_K(x_i))}\n\\]\n\n\n\n\n\n\n\nPiecewise polynomials regression (Natural Spine)\nIt consist in ﬁtting separate low-degree polynomials over diﬀerent regions of X. For example, a piecewise cubic polynomial with a single knot at a point c takes the form.\n\\[\ny_i =\n  \\begin{cases}\n    \\beta_{01} + \\beta_{11} x_i + \\beta_{21} x_i^2 + \\beta_{31} x_i^3 + \\epsilon_i &  \\text{if } x_i<c\\\\\n    \\beta_{02} + \\beta_{12} x_i + \\beta_{22} x_i^2 + \\beta_{32} x_i^3 + \\epsilon_i &  \\text{if } x_i \\geq c\n  \\end{cases}\n\\]\nAs each polynomial has four parameters, we are using a total of 8 degrees of freedom in ﬁtting that model. By using that model with Wagedata, we can see a problem as the model used was too flexible and to solve it we need to constrain it to be continuous at age = 50.\n But as you could see after applying the continuity constraint the plot still present an unnatural V-shape that can solve by apply the continuity constraint to the first and second derivative of the function, the end with 5 degrees of freedom model.\n\n\n\n\n\nIn this context, a natural spline refers to a regression spline with the additional constraints of maintaining linearity at the boundaries."
  },
  {
    "objectID": "flexible-regression.html#smoothing-splines",
    "href": "flexible-regression.html#smoothing-splines",
    "title": "Flexible Regression Models",
    "section": "Smoothing splines",
    "text": "Smoothing splines\nThey arise as a result of minimizing a residual sum of squares criterion subject to a smoothness penalty.\nIn this method rather than trying to minimize \\(\\text{RSS} = \\sum_{i=1}^n (y_i - g(x_i))^2\\), we try to find a function \\(g(x)\\), known as smoothing spline, which could minimize the following expression based on the \\(\\lambda\\) nonnegative tuning parameter.\n\\[\n\\underbrace{\\sum_{i=1}^n (y_i - g(x_i))^2 }_{\\text{loss function (data fitting)}} +\n\\underbrace{\\lambda \\int g''(t)^2dt}_{\\text{penalty term (g varibility)}}\n\\]\nThe second derivative of \\(g(t)\\) measure how wiggly is the function near \\(t\\), where its value is \\(0\\) when the function is a straight line as a line is perfectly smooth. The we can use integral to get total change in the function \\(g'(t)\\), over its entire range. As consequence, the larger the value of \\(\\mathbf{\\lambda}\\) , the smoother \\(\\mathbf{g}\\) will be.\n\n\n\n\n\n\nThe function \\(g(x)\\) is a natural cubic spline with knots at \\(x_1, \\dots ,x_n\\). As results the effective degrees of freedom (\\(df_{\\lambda} = \\sum_{i=1}^n \\{ \\mathbf{S}_{\\lambda} \\}_{ii}\\)) are between \\(n\\) and \\(2\\) depending on the value of \\(\\mathbf{\\lambda}\\).\n\n\n\nWe can use cross-validation to find the best value which can minimize the RSS. It turns out that the leave one-out cross-validation error (LOOCV) can be computed very eﬃciently for smoothing splines, with essentially the same cost as computing a single ﬁt, using the following formula:\n\\[\n\\text{RSS}_{cv} (\\lambda) = \\sum_{i = 1}^n (y_i - \\hat{g}_\\lambda^{(-i)} (x_i))^2 =\n                            \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{g}_{\\lambda} (x_i)}\n                                                       {1 - \\{ \\mathbf{S_{\\lambda}} \\}_{ii}}\n                                           \\right]^2\n\\]\nWhere:\n\n\\(\\hat{g}_\\lambda^{(-i)}\\): Refers to the function fitted without the ith observation \\((x_i, y_i)\\).\n\\(\\hat{g}_\\lambda\\): Refers the smoothing spline function ﬁtted to all of the training observations."
  },
  {
    "objectID": "flexible-regression.html#local-regression",
    "href": "flexible-regression.html#local-regression",
    "title": "Flexible Regression Models",
    "section": "Local regression",
    "text": "Local regression\nComputes the fit at target point \\(x_0\\) using only the nearly training observations by:\n\nGather the \\(s=k/n\\) closest (known as span) fraction of points. This step is very important as it controls the flexibility level can be selected using cross-validation*.\nAssign a weight \\(K_{i0} = K(x_i, x_0)\\) for each selected point based on the distance to \\(x_0\\). As lower is the distance as higher needs to be the weight.\nFind the coefficients which minimize the weighted least squares regression for the current \\(x_0\\) value.\n\n\\[\n\\sum_{i=1}^n = K_{i0}(y_i - \\beta_0 - \\beta_1x_i)^2\n\\]\n\nCalculate the fitted value of \\(x_0\\) using \\(\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1x_0\\).\n\nIn the next illustration we can see how the model works with some simulated data.\n\n\n\n\n\n\n\n\n\n\n\nIt performs poorly when we have more than 3 or 4 predictors in our model."
  },
  {
    "objectID": "flexible-regression.html#generalized-additive-models-gam",
    "href": "flexible-regression.html#generalized-additive-models-gam",
    "title": "Flexible Regression Models",
    "section": "Generalized Additive Models (GAM)",
    "text": "Generalized Additive Models (GAM)\nThey provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.\nTo transform each predictor we have the next options:\n\nA constant for each categorical level (step function)\nPolynomial regression\nNatural spines (optimized with least squares)\nSmoothing splines (optimized with backﬁtting)\nLocal regression\n\n\n\n\n\n\n\nBackﬁtting ﬁts a model involving multiple predictors by repeatedly updating the ﬁt for each predictor in turn, holding the others ﬁxed.\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\n1. It finds relationships that a lineal model would miss without applying many transformations as it can fit a non-linear \\(f_j\\) to each \\(X_j\\)   2. We can examine the eﬀect of each predictor on \\(Y\\) individually as the model is additive.   3. The smoothness of each function \\(f_j\\) can be summarized via degrees of freedom.\n1. Important interactions can be missed, but they can be added manually\n\n\n\n\nGAM Regression\nTo predict a numeric variable this method creates a function with the next form:\n\\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\dots + f_p(x_{ip}) + \\epsilon_i\n\\]\nBy taking the other predictor as constant we can plot effect of each function for each predictor in the Wage example:\n\n\n\n\n\n\n\nGAM Classification\nTo predict a categorical variable this method creates a function with the next form:\n\\[\n\\log \\left( \\frac{p(X)}{1-p(X)} \\right)= \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\dots + f_p(x_{ip})\n\\]\nBy taking the other predictor as constant we can plot effect of each function for each predictor in the Wage example:"
  },
  {
    "objectID": "non-parametric-models.html",
    "href": "non-parametric-models.html",
    "title": "Non-parametric Methods",
    "section": "",
    "text": "It uses the principle of nearest neighbors to classify unlabeled examples by using the Euclidean Distance to calculate distance between the point we want to predict and \\(k\\) closest neighbors on the training data.\n\\[\nd\\left( a,b\\right)   = \\sqrt {\\sum _{i=1}^{p}  \\left( a_{i}-b_{i}\\right)^2 }\n\\]\nKNN unlike parametric models does not tell us which predictors are important, making it hard to make inferences using this model.\nThis method performs worst than a parametric as we starting adding noise predictors. In fact, we will get in the situation where for a given observation has no nearby neighbors, known as curse of dimensionality and leading to a very poor prediction of \\(f(x_{0})\\).\n\nThe next function estimates the conditional probability for class \\(j\\) as the fraction of points in \\(N_{0}\\) whose response values equal \\(j\\).\n\\[\n\\text{Pr}(Y = j|X = x_{0}) = \\frac{1}{K}\n                      \\displaystyle\\sum_{i \\in N_{0}} I(y_{i} = j)\n\\]\n\nWhere\n\n\n\\(j\\) response value to test\n\n\\(x_{0}\\) is the test observation\n\n\\(K\\) the number of points in the training data that are closest to \\(x_{0}\\) and reduce the model flexibility\n\n\\(N_{0}\\) points in the training data that are closest to \\(x_{0}\\)\n\n\n\n\nThen KNN classiﬁes the test observation \\(x_{0}\\) to the class with the largest probability.\n\n\n\n\n\nKNN regression estimates \\(f(x_{0})\\) using the average of all the training responses in \\(N_{0}\\).\n\\[\n\\hat{f}(x_{0}) = \\frac{1}{K}\n                      \\displaystyle\\sum_{i \\in N_{0}} y_{i}\n\\]\n\nWhere\n\n\n\\(x_{0}\\) is the test observation\n\n\\(K\\) the number of points in the training data that are closest to \\(x_{0}\\) and reduce the model flexibility\n\n\\(N_{0}\\) points in the training data that are closest to \\(x_{0}\\)\n\n\n\n\nTo use this method we need to make sure that all our variables are numeric. If one our variables is a factor we need to perform a dummy transformation of that variable with the recipes::step_dummy function.\nOn the other hand, as this model uses distances to make predicts it’s important to check that each feature of the input data is measured with the same range of values with the recipes::step_range function which normalize from 0 to 1 as happens with the dummy function.\n\\[\nx' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\nAnother normalization alternative is centering the predictors in \\(\\overline{x} = 0\\) with \\(S = 0\\) with the function recipes::step_normalize or the function scale() which apply the z-score normalization.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\nTo perform K-Nearest Neighbors we just need to create the model specification by using kknn engine.\n\nlibrary(tidymodels)\nlibrary(ISLR2)\n\nSmarket_train <- \n  Smarket %>%\n  filter(Year != 2005)\n\nSmarket_test <- \n  Smarket %>%\n  filter(Year == 2005)\n\nknn_spec <- nearest_neighbor(neighbors = 3) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\nSmarketKnnPredictions <-\n  knn_spec %>%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |>\n  augment(new_data = Smarket_test) \n\nconf_mat(SmarketKnnPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down Up\n      Down   43 58\n      Up     68 83\n\naccuracy(SmarketKnnPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary           0.5"
  },
  {
    "objectID": "resampling-methods.html",
    "href": "resampling-methods.html",
    "title": "Resampling methods",
    "section": "",
    "text": "The Resampling Methods are indispensable to obtain additional information about a fitted model.\nIn this chapter, we will explore the methods:\n\n\nCross-validation\n\nUsed to estimate the test error in order to evaluate a model’s performance (model assessment).\nThe location of the minimum point in the test error curve to select between several models or find the best of level of ﬂexibility to one model (model selection).\n\n\n\nBootstrap\n\nEstimates the uncertainty associated with a given value or statistical learning method. For example, it can estimate the standard errors of the coeﬃcients from a linear regression ﬁt"
  },
  {
    "objectID": "resampling-methods.html#cross-validation",
    "href": "resampling-methods.html#cross-validation",
    "title": "Resampling methods",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nTraining error limitations\nThe training error trends to underestimate the real model’s error.\n\n\n\n\nTo solve that problem we can hold out a subset of the training observations from the ﬁtting process.\nValidation Set Approach\nSplits randomly the available set of observations into two parts, a training set and a validation set.\n\n\n\n\nAs we split the data randomly we will have a different estimation of the error rate based on the seed set in R.\n\n\n\n\n\n\n\n\n\n\nMain Characteristics\nLevel\n\n\n\nAccuracy in estimating the testing error\nLow\n\n\nTime efficiency\nHigh\n\n\nProportion of data used to train the models (bias mitigation)\nLow\n\n\nEstimation variance\n-\n\n\n\nCoding example\nYou can learn more about tidymodels and this book in ISLR tidymodels labs by Emil Hvitfeldt.\n\n# Loading functions and data\nlibrary(tidymodels)\nlibrary(ISLR)\nlibrary(data.table)\n\n\n# Defining the model type to train\nLinealRegression <- linear_reg() |>\n  set_mode(\"regression\") |>\n  set_engine(\"lm\")\n\n\n# Creating the rplit object\nset.seed(1)\nAutoValidationSplit <- initial_split(Auto, strata = mpg, prop = 0.5)\n\nAutoValidationTraining <- training(AutoValidationSplit)\nAutoValidationTesting <- testing(AutoValidationSplit)\n\n\nlapply(1:10, function(degree){\n  \n  recipe_to_apply <- \n    recipe(mpg ~ horsepower, data = AutoValidationTraining) |>\n    step_poly(horsepower, degree = degree)\n  \n  workflow() |>\n    add_model(LinealRegression) |>\n    add_recipe(recipe_to_apply) |>\n    fit(data = AutoValidationTraining) |>\n    augment(new_data = AutoValidationTesting) |>\n    rmse(truth = mpg, estimate = .pred) |>\n    transmute(degree = degree,\n              .metric, \n              .estimator, \n              .estimate) }) |>\n  rbindlist()\n\n    degree .metric .estimator .estimate\n 1:      1    rmse   standard  5.058317\n 2:      2    rmse   standard  4.368494\n 3:      3    rmse   standard  4.359262\n 4:      4    rmse   standard  4.351128\n 5:      5    rmse   standard  4.299916\n 6:      6    rmse   standard  4.319319\n 7:      7    rmse   standard  4.274027\n 8:      8    rmse   standard  4.330195\n 9:      9    rmse   standard  4.325196\n10:     10    rmse   standard  4.579561\n\n\nLeave-One-Out Cross-Validation (LOOCV)\nThe statistical learning method is ﬁt on the \\(n-1\\) training observations, and a prediction \\(\\hat{y}_1\\) is made for the excluded observation to calculate \\(\\text{MSE}_1 = (y_1-\\hat{y}_1)^2\\). Then it repeats the process \\(n\\) times and estimate the test error rate.\n\n\n\n\nBased on the average of \\(n\\) test estimates it reports the test error rate.\n\\[\n\\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n\\text{MSE}_i\n\\]\n\n\n\n\n\n\n\n\n\n\nMain Characteristics\nLevel\n\n\n\nAccuracy in estimating the testing error\nHigh\n\n\nTime efficiency\nLow\n\n\nProportion of data used to train the models (bias mitigation)\nHigh\n\n\nEstimation variance\nHigh\n\n\n\nAmazing shortcuts\nThere some cases where we can perform this technique just fitting one model with all the observations. there are listed in the next table.\n\n\n\n\n\n\n\nModel\nFormula\nDescription\n\n\n\nLineal or polynomial regression\n\\(\\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_i}{1-h_i} \\right)^2\\)\n- \\(\\hat{y}_i\\): Refers to the ith ﬁtted value from the original least squares ﬁt.  - \\(h_i\\) Refers to the leverage of \\(x_i\\) as measure of the rarity of each value.\n\n\nSmoothing splines\n\\(\\text{RSS}_{cv} (\\lambda) = \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{g}_{\\lambda} (x_i)} {1 - \\{ \\mathbf{S_{\\lambda}} \\}_{ii}} \\right]^2\\)\n- \\(\\hat{g}_\\lambda\\): Refers the smoothing spline function ﬁtted to all of the training observations.\n\n\nCoding example\n\ncollect_loo_testing_error <- function(formula,\n                                      loo_split,\n                                      metric_function = rmse,\n                                      ...){\n  # Validations\n  stopifnot(\"There is no espace between y and ~\" = formula %like% \"[A-Za-z]+ \")\n  stopifnot(\"loo_split must be a data.table object\" = is.data.table(loo_split))\n  \n  predictor <- sub(pattern = \" .+\", replacement = \"\", formula)\n  formula_to_fit <- as.formula(formula)\n  \n  Results <-\n    loo_split[, training(splits[[1L]]), by = \"id\"\n    ][, .(model = .(lm(formula_to_fit, data = .SD))),\n      by = \"id\"\n    ][loo_split[, testing(splits[[1L]]), by = \"id\"],\n      on = \"id\"\n    ][, .pred := predict(model[[1L]], newdata = .SD),\n      by = \"id\"\n    ][,  metric_function(.SD, truth = !!predictor, estimate = .pred, ...) ]\n \n  setDT(Results)\n  \n  \n  if(formula %like% \"degree\"){\n    \n    degree <- gsub(pattern = \"[ A-Za-z,=\\\\~()]\", replacement = \"\", formula)\n    \n    Results <- \n      Results[,.(degree = degree, \n                .metric, \n                .estimator, \n                .estimate)]\n    \n  }\n  \n  return(Results)\n    \n}\n\n\n# Creating the rplit object\nAutoLooSplit <- loo_cv(Auto)\n\n# Transforming to data.table\nsetDT(AutoLooSplit)\n\npaste0(\"mpg ~ poly(horsepower, degree=\", 1:10, \")\") |>\n  lapply(collect_loo_testing_error,\n         loo_split = AutoLooSplit) |>\n  rbindlist()\n\n    degree .metric .estimator .estimate\n 1:      1    rmse   standard  4.922552\n 2:      2    rmse   standard  4.387279\n 3:      3    rmse   standard  4.397156\n 4:      4    rmse   standard  4.407316\n 5:      5    rmse   standard  4.362707\n 6:      6    rmse   standard  4.356449\n 7:      7    rmse   standard  4.339706\n 8:      8    rmse   standard  4.354440\n 9:      9    rmse   standard  4.366764\n10:     10    rmse   standard  4.414854\n\n\nk-Fold Cross-Validation\nInvolves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The ﬁrst fold is treated as a validation set, and the method is ﬁt on the remaining \\(k-1\\) folds.\n\n\n\n\nBased on the average of \\(k\\) test estimates it reports the test error rate.\n\\[\n\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{i=1}^k\\text{MSE}_i\n\\]\n\n\n\n\n\n\n\n\n\n\nMain Characteristics\nLevel\n\n\n\nAccuracy in estimating the testing error\nHigh\n\n\nTime efficiency\nRegular\n\n\nProportion of data used to train the models (bias mitigation)\nRegular\n\n\nEstimation variance\nRegular\n\n\n\nNote: The book recommends using \\(k = 5\\) or \\(k = 10\\).\nCoding example\n\nAutoKFoldRecipe <- \n  recipe(mpg ~ horsepower, data = Auto) |>\n  step_poly(horsepower, degree = tune())\n\nAutoTuneReponse <-\n  workflow() |>\n  add_recipe(AutoKFoldRecipe) |>\n  add_model(LinealRegression) |>\n  tune_grid(resamples = vfold_cv(Auto, v = 10), \n            grid = tibble(degree = seq(1, 10)))\n\nshow_best(AutoTuneReponse, metric = \"rmse\")\n\n# A tibble: 5 × 7\n  degree .metric .estimator  mean     n std_err .config              \n   <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1      7 rmse    standard    4.33    10   0.144 Preprocessor07_Model1\n2      6 rmse    standard    4.34    10   0.143 Preprocessor06_Model1\n3      8 rmse    standard    4.35    10   0.145 Preprocessor08_Model1\n4      5 rmse    standard    4.35    10   0.146 Preprocessor05_Model1\n5      9 rmse    standard    4.36    10   0.149 Preprocessor09_Model1\n\nautoplot(AutoTuneReponse)\n\n\n\n\nLOOCV vs 10-Fold CV accuracy\n\n\nTrue test MSE: Blue solid line\n\nLOOCV: Black dashed line\n\n10-fold CV: Orange solid line"
  },
  {
    "objectID": "resampling-methods.html#bootstrap",
    "href": "resampling-methods.html#bootstrap",
    "title": "Resampling methods",
    "section": "Bootstrap",
    "text": "Bootstrap\nBy taking many samples from a population we can obtain the Sampling Distribution of a value, but in many cases we just can get a single sample from the population. In theses cases, we can resample the data with replacement to generate many samples from one sample, creating a Bootstrap Distribution of a value.\n\n\n\n\nAs you can see bellow the center of the Bootstrap Distribution must of the time differs from the center of the Sampling Distribution, but its very accurate at estimating the dispersion of the value.\n\n\n\n\nCoding example\n\nAutoBootstraps <- bootstraps(Auto, times = 500)\n\nboot.fn <- function(split) {\n  LinealRegression |> \n    fit(mpg ~ horsepower, data = analysis(split)) |>\n    tidy()\n}\n\nAutoBootstraps |>\n  mutate(models = map(splits, boot.fn)) |>\n  unnest(cols = c(models)) |>\n  group_by(term) |>\n  summarise(low = quantile(estimate, 0.025),\n            mean = mean(estimate),\n            high = quantile(estimate, 0.975),\n            sd = sd(estimate))\n\n# A tibble: 2 × 5\n  term           low   mean   high      sd\n  <chr>        <dbl>  <dbl>  <dbl>   <dbl>\n1 (Intercept) 38.5   40.0   41.7   0.851  \n2 horsepower  -0.173 -0.158 -0.145 0.00734"
  },
  {
    "objectID": "02-execises.html",
    "href": "02-execises.html",
    "title": "02 - Statistical Learning",
    "section": "",
    "text": "For each of parts (a) through (d), indicate whether we would generally expect the performance of a ﬂexible statistical learning method to be better or worse than an inﬂexible method. Justify your answer.\n\n\nThe sample size n is extremely large, and the number of predictors p is small.\n\nBetter, flexible models reduce bias and quit the variance low when having a large n.\n\nThe number of predictors p is extremely large, and the number of observations n is small.\n\nWorse, a flexible model could increase its variance very high when having small n.\n\nThe relationship between the predictors and response is highly non-linear.\n\nBetter, a lineal model would have a very high bias in this case.\n\nThe variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high.\n\nWorse, a flexible model would over-fit trying to follow the irreducible error.\n\nExplain whether each scenario is a classiﬁcation or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\n\n\nWe collect a set of data on the top 500 ﬁrms in the US. For each ﬁrm we record proﬁt, number of employees, industry and the CEO salary. We are interested in understanding which factors aﬀect CEO salary.\n\nRegression, inference, 500, 4\n\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n\nClassification, prediction, 20, 14\n\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n\nRegression, prediction, 52, 4\n\nWe now revisit the bias-variance decomposition.\n\n\nProvide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less ﬂexible statistical learning methods towards more ﬂexible approaches. The x-axis should represent the amount of ﬂexibility in the method, and the y-axis should represent the values for each curve. There should be ﬁve curves. Make sure to label each one.\n\n\n\nExplain why each of the ﬁve curves has the shape displayed in part (a).\n\nIn the example f isn’t lineal, so the the test error lower as we add flexibility until the point the models starts to overfit. The training error always goes down as we increase the flexibility. As we make the model more flexible variance always increase as the model is more likely to change as we change the training data and the bias always goes down as a more flexible model has fewer assumptions. The Bayes error is the irreducible error we can not change it.\n\nYou will now think of some real-life applications for statistical learning.\n\n\nDescribe three real-life applications in which classiﬁcation might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n\n\n\n\n\n\nGoal\nResponse\nPredictors\n\n\n\nInference\nCustomer Tower Churn (0 or 1)\nAnnual Rent, Tower Lat, Tower Log, Tower Type, Number of sites around 10 km, Population around 10 km, Average Annual Salary in the city, contract Rent increases, customer technology\n\n\nInference\nEmployee Churn (0 or 1)\nMonths in company, Salary, Number of positions, Major, Sex, Total Salary Change, Bono, Wellness Expend, Number of depends, Home location\n\n\nInference\nAbsent (0 or 1)\nSalary, Rain?, Holiday?, Number of uniforms, distance from home to work place, Months in company, Neighborhood median Salary, number of depends, number of marriage, Work start, Work End, Free day\n\n\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n\n\n\n\n\n\nGoal\nResponse\nPredictors\n\n\n\nInference\nNumber of likes\nWords, Has a video?, Has a picture?, Post time, hashtag used\n\n\nInference\nVelocidad de picheo\nEdad, Altura, Peso, Horas de corrida, Cantidad de sentadillas, cantidad de practicas por semana, Años practicando el deporte\n\n\nInference\nFood Satisfaction level (0 to 10)\nCountry, City, Height, Weight, Salary (US $), Salt, Spacy Level, Sugar (gr), Meat Type, Cheese (gr), Cheese Type\n\n\n\n\nDescribe three real-life applications in which cluster analysis might be useful.\n\n\n\n\n\n\n\nGoal\nPredictors\n\n\n\nClassify costumer to improve advertising\nWords searched, products clicked, Explored image, Seconds spent on each product, start time, end time, customer location\n\n\nClassify company towers to see patterns in customers\nTower Lat, Tower Log, Tower Type, Number of sites around 10 km, Population around 10 km, Average Annual Salary in the city, BTS?, start date, Height\n\n\nClassify football players check which players have similar results\nNumber of passes on each game, Number of meters run on each game, Position Played, Number of goals, Number of stolen balls, total time played\n\n\n\n\nWhat are the advantages and disadvantages of a very ﬂexible (versus a less ﬂexible) approach for regression or classiﬁcation? Under what circumstances might a more ﬂexible approach be preferred to a less ﬂexible approach? When might a less ﬂexible approach be preferred?\n\n\nFlexible model advantages\n\nThey have the potential to accurately ﬁt a wider range of possible shapes for f\n\n\nFlexible model disadvantages\n\nThey do not reduce the problem of estimating f to a small number of parameters.\nA very large number of observations is required in order to obtain an accurate estimate for f.\nThey are harder to interpret\n\n\n\nIt is preferred when we have a lot of data to train the model and the goal is to get accurate predictions rather than good interpretations.\nA less flexible approach is preferred when we don’t have a lot data to train the model or when the main goal is to make inferences to understand business rules.\n\nDescribe the diﬀerences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classiﬁcation (as opposed to a nonparametric approach)? What are its disadvantages?\n\n\n\n\n\n\n\nParametric\nNon-parametric\n\n\n\nMake an assumption about the functional form\nDon’t make an assumption about the functional form, to accurately ﬁt a wider range of possible shapes for \\(f\\)\n\n\n\nEstimates a small number parameters based on training data\nEstimates a large number parameters based on training data\n\n\nCan be trained with few examples\nNeeds many examples to be trained\n\n\nSmoothness level is fixed\nData analyst needs define a level of smoothness\n\n\n\n\nParametric model advantages\n\nReduce the problem of estimating f to a small number of parameters.\nCan be trained with few examples.\nThey are easy to interpret.\n\n\nParametric model disadvantages\n\nIn many times \\(f\\) doesn’t have the assumed shape adding a lot of bias to the model.\n\n\n\n\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable.\n\n\nDF_07 <-\n  data.frame(X1 = c(0,2,0,0,-1,1),\n             X2 = c(3,0,1,1,0,0),\n             X3 = c(0,0,3,2,1,1),\n             Y = c(\"Red\",\"Red\",\"Red\",\"Green\",\"Green\",\"Red\"))\n\nDF_07\n\n  X1 X2 X3     Y\n1  0  3  0   Red\n2  2  0  0   Red\n3  0  1  3   Red\n4  0  1  2 Green\n5 -1  0  1 Green\n6  1  0  1   Red\n\n\nSuppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.\n\nCompute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.\n\n\n# As (C - 0)^2 = C^2\n\nDF_07 <- transform(DF_07, dist = sqrt(X1^2+X2^2+X3^2))\n\n\nWhat is our prediction with K = 1? Why?\n\n\nDF_07[order(DF_07$dist),]\n\n  X1 X2 X3     Y     dist\n5 -1  0  1 Green 1.414214\n6  1  0  1   Red 1.414214\n2  2  0  0   Red 2.000000\n4  0  1  2 Green 2.236068\n1  0  3  0   Red 3.000000\n3  0  1  3   Red 3.162278\n\n\nIn the is case, the point would be in the Bayes decision boundary as there are two points of different colors at the same distance.\n\nWhat is our prediction with K = 3? Why?\n\nIn this case, the point would be a Red one as 2 of 3 of them are from that color.\n\nIf the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?\n\nAs flexibility decrease as K gets bigger, for highly nonlinear Bayes decision boundary the best K value should be a small one."
  },
  {
    "objectID": "02-execises.html#applied",
    "href": "02-execises.html#applied",
    "title": "02 - Statistical Learning",
    "section": "Applied",
    "text": "Applied\n\nThis exercise relates to the College data set, which can be found in the ﬁle College.csv on the book website. It contains a number of variables for 777 diﬀerent universities and colleges in the US\n\nBefore reading the data into R, it can be viewed in Excel or a text editor.\n\nUse the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. You should notice that the ﬁrst column is just the name of each university. We don’t really want R to treat this as data.\n\n\ncollege <- \n  here::here(\"data/College.csv\") |>\n  read.csv(row.names = 1, stringsAsFactors = TRUE)\n\n\nUse the summary() function to produce a numerical summary of the variables in the data set.\n\n\nsummary(college)\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\nUse the pairs() function to produce a scatterplot matrix of the ﬁrst ten columns or variables of the data. \n\n\npairs(college[,1:10])\n\n\n\n\n\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private.\n\n\nplot(college$Private, college$Outstate)\n\n\n\n\n\nCreate a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10 % of their high school classes exceeds 50 %.\n\n\ncollege$Elite <- ifelse(college$Top10perc > 50, \"Yes\", \"No\") |> as.factor()\n\n\nThen Use the summary() function to see how many elite universities there are.\n\n\nsummary(college$Elite)\n\n No Yes \n699  78 \n\n\n\nNow use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\n\nplot(college$Elite, college$Outstate)\n\n\n\n\n\nUse the hist() function to produce some histograms with diﬀering numbers of bins for a few of the quantitative variables. You may ﬁnd the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\n\n\npar(mfrow = c(3, 1))\n\nhist(college$Apps)\nhist(college$Accept)\nhist(college$Enroll)\n\n\n\n\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\npar(mfrow = c(2, 2))\n\nplot(college$S.F.Ratio, college$Expend)\nplot(college$S.F.Ratio, college$Outstate)\nplot(college$Top10perc, college$Terminal)\nplot(college$Top10perc, college$Room.Board)\n\n\n\npar(mfrow = c(1, 1))\n\nAs students have more resources like teaching, supervision, curriculum development, and pastoral support institutions tend to expend less on each student and quest less money from out state students.\nWe also can see that students from top 10 % of high school class tend to go to universities where most the professors have the highest academic level available for each field or the highest room and board costs\n\nThis exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n\n\nWhich of the predictors are quantitative, and which are qualitative?\n\n\nlibrary(ISLR2)\n\nquantitative_vars <-\n  sapply(Auto, is.numeric) |>\n  (\\(x) names(x)[x])()\n\nqualitative_vars <- setdiff(names(Auto), quantitative_vars)\n\n\nWhat is the range of each quantitative predictor? You can answer this using the range() function.\n\n\nsapply(quantitative_vars, \\(var) range(Auto[[var]]))\n\n      mpg cylinders displacement horsepower weight acceleration year origin\n[1,]  9.0         3           68         46   1613          8.0   70      1\n[2,] 46.6         8          455        230   5140         24.8   82      3\n\n\n\nWhat is the mean and standard deviation of each quantitative predictor?\n\n\nsapply(quantitative_vars, \\(var) c(mean(Auto[[var]]), sd(Auto[[var]])))\n\n           mpg cylinders displacement horsepower    weight acceleration\n[1,] 23.445918  5.471939      194.412  104.46939 2977.5842    15.541327\n[2,]  7.805007  1.705783      104.644   38.49116  849.4026     2.758864\n          year    origin\n[1,] 75.979592 1.5765306\n[2,]  3.683737 0.8055182\n\n\n\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\n\nAutoFiltered <-Auto[-c(10:85),]\n\nsapply(quantitative_vars, \\(var) c(mean(AutoFiltered[[var]]), sd(AutoFiltered[[var]])))\n\n           mpg cylinders displacement horsepower    weight acceleration\n[1,] 24.404430  5.373418    187.24051  100.72152 2935.9715    15.726899\n[2,]  7.867283  1.654179     99.67837   35.70885  811.3002     2.693721\n          year   origin\n[1,] 77.145570 1.601266\n[2,]  3.106217 0.819910\n\n\n\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your ﬁndings.\n\nCars with 4 or 5 cylinders are more efficient than others.\n\nplot(factor(Auto$cylinders) ,Auto$mpg,\n     xlab = \"cylinders\", ylab = \"mpg\", col = 2)\n\n\n\n\nCars have improved their efficiency each year.\n\nplot(factor(Auto$year) ,Auto$mpg,\n     xlab = \"year\", ylab = \"mpg\", col = \"blue\")\n\n\n\n\n\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\npairs(Auto[, quantitative_vars])\n\n\n\n\ncylinders, horsepower and year are good candidates as they have correlations with the mpg variable.\n\nThis exercise involves the Boston housing data set.\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\nA data frame with 506 rows and 13 variables. Suburbs are represented as rows and columns represent different indicators.\n\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your ﬁndings.\n\nAs all the data is numeric, let’s get the highest correlations.\n\nBostonRelations <-\n  cor(Boston) |>\n  (\\(m) data.frame(row = rownames(m)[row(m)[upper.tri(m)]], \n                   col = colnames(m)[col(m)[upper.tri(m)]], \n                   cor = m[upper.tri(m)]))() |>\n  (\\(DF) DF[order(-abs(DF$cor)),])() \n\nhead(BostonRelations,3)\n\n     row col        cor\n45   rad tax  0.9102282\n26   nox dis -0.7692301\n9  indus nox  0.7636514\n\n\nAs we can see, if a suburb has high accessibility to radial highways the house value also increase.\n\nwith(Boston, plot(rad, tax, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(dis, nox, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(indus, nox, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\nwith(BostonRelations, BostonRelations[row == \"crim\", ])\n\n    row     col         cor\n29 crim     rad  0.62550515\n37 crim     tax  0.58276431\n56 crim   lstat  0.45562148\n7  crim     nox  0.42097171\n2  crim   indus  0.40658341\n67 crim    medv -0.38830461\n22 crim     dis -0.37967009\n16 crim     age  0.35273425\n46 crim ptratio  0.28994558\n11 crim      rm -0.21924670\n1  crim      zn -0.20046922\n4  crim    chas -0.05589158\n\n\n\nwith(Boston, plot(rad, crim, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(tax, crim, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\n\npar(mfrow=c(3,1))\n\nboxplot(Boston$crim, horizontal = TRUE)\nhist(Boston$tax)\nhist(Boston$ptratio)\n\n\n\npar(mfrow=c(1,1))\n\n\nHow many of the census tracts in this data set bound the Charles river?\n\n\nsum(Boston$chas)\n\n[1] 35\n\n\n\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\n\nmedian(Boston$ptratio)\n\n[1] 19.05\n\n\n\nWhich census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your ﬁndings.\n\n\nwhich.min(Boston$age)\n\n[1] 42\n\nMinOwnerOccupiedHomes <- Boston[which.min(Boston$age),]\n\nMinOwnerOccupiedHomes\n\n      crim zn indus chas   nox   rm age    dis rad tax ptratio lstat medv\n42 0.12744  0  6.91    0 0.448 6.77 2.9 5.7209   3 233    17.9  4.84 26.6\n\nVarsToPlot <-\n  names(Boston) |>\n  setdiff(\"crim\")\n\nfor(variable in VarsToPlot){\n  \n  hist(Boston[[variable]],\n       main = paste(\"Histogram of\" , variable), xlab = variable)\n  abline(v=MinOwnerOccupiedHomes[[variable]],col=\"blue\",lwd=2)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n\nsum(Boston$rm > 7)\n\n[1] 64\n\nsum(Boston$rm > 8)\n\n[1] 13\n\nBostonRelations[BostonRelations$row == \"rm\" | BostonRelations$col == \"rm\",]\n\n     row     col         cor\n72    rm    medv  0.69535995\n61    rm   lstat -0.61380827\n13 indus      rm -0.39167585\n51    rm ptratio -0.35550149\n12    zn      rm  0.31199059\n15   nox      rm -0.30218819\n42    rm     tax -0.29204783\n21    rm     age -0.24026493\n11  crim      rm -0.21924670\n34    rm     rad -0.20984667\n27    rm     dis  0.20524621\n14  chas      rm  0.09125123\n\npar(mfrow=c(1,2))\nwith(Boston, plot(rm, medv, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\nabline(v=8,col=\"red\",lwd=2)\n \n\nwith(Boston, plot(rm, lstat, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\nabline(v=8,col=\"red\",lwd=2)\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "03-execises.html",
    "href": "03-execises.html",
    "title": "03 - Linear Regression",
    "section": "",
    "text": "1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\n\nNull hypotheses for each predictor each coefficient is 0. We can see in the table that we can reject the null hypotheses for TV and radio but there isn’t enough evidence to reject the null hypotheses for newspaper.\n\n2. Carefully explain the differences between the KNN classifier and KNN regression methods.\n\nThe classifier assigns classes based on the most often class of the closest \\(K\\) elements, on the other hand the regression estimate each value taking the mean of the closest \\(K\\) elements.\n\n3. Suppose we have a data set with five predictors to predict the starting salary after graduation (in thousands of dollars) and after using least squares we fitted the next model:\n\n\nVariable\nCoefficient\n\n\n\nLevel (High School)\n\\(\\hat{\\beta}_{0} = 50\\)\n\n\n\n\\(X_{1}\\) = GPA\n\\(\\hat{\\beta}_{1} = 20\\)\n\n\n\n\\(X_{2}\\) = IQ\n\\(\\hat{\\beta}_{2} = 0.07\\)\n\n\n\n\\(X_{3}\\) = Level (College)\n\\(\\hat{\\beta}_{3} = 35\\)\n\n\n\n\\(X_{4}\\) = Interaction between GPA and IQ\n\\(\\hat{\\beta}_{4} = 0.01\\)\n\n\n\n\\(X_{5}\\) = Interaction between GPA and Level\n\\(\\hat{\\beta}_{5} = −10\\)\n\n\n\nWhich answer is correct, and why?\n\nBased on this information we can say that:\n\n\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduate.\n\n\nAs High School students earn on average \\(\\hat{\\beta}_{0} = 50\\) College students earn |\\(\\hat{\\beta}_{0} + \\hat{\\beta}_{3} = 85\\)\n\n\n(A) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n\\[\n\\begin{split}\n\\hat{Y} & = 35 + 20 (4) + 0.07 (110) + 35 + 0.01(4)(110) - 10 (4) \\\\\n        & = 122.1\n\\end{split}\n\\]\n\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n\nFALSE, we can not make conclusions about the significance of any tern about checking the the standard error of each term. The coefficient might small because the IQ has very high values if we contrast the GPA ones.\n\nI collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^2 + \\beta_{3}x^3 + \\epsilon\\).\n\n\nSuppose that the true relationship between X and Y is linear, i.e. \\(Y = \\beta_{0} + \\beta_{1}x + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\nAs the training RSS always gets lower as we increase the flexibility the cubic regression would have a lower RSS.\n\nAnswer (a) using test rather than training RSS.\n\nThe linear regression would have a lower test RSS, as it reduces de scare bias of the model.\n\nSuppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\nAs the training RSS always gets lower as we increase the flexibility the cubic regression would have a lower RSS.\n\nAnswer (c) using test rather than training RSS.\n\nThe cubic regression would have a lower test RSS, as it reduces de scare bias of the model.\n\nConsider the fitted values that result from performing linear regression without an intercept. In this setting, the \\(i\\)th fitted value takes the form.\n\n\\[\n\\hat{y}_{i} = x_{i}\\hat{\\beta}\n\\]\nWhere\n\\[\n\\hat{\\beta}= \\left( \\sum_{i=1}^{n}{x_{i}y_{i}}  \\right) /\n             \\left( \\sum_{i'=1}^{n}{x_{i'}^2}  \\right)\n\\]\n\nShow that we can write\n\n\\[\n\\hat{y}_{i} = \\sum_{i'=1}^{n}{a_{i'}y_{i'}}\n\\] I am not sure about this execise as I don’t understand the difference between \\(i\\) and \\(i'\\).\n\\[\n\\begin{split}\n\\sum_{i'=1}^{n}{a_{i'}y_{i'}} & = x_{i}\\hat{\\beta} \\\\\n\\sum_{i'=1}^{n}{a_{i'}y_{i'}} & = x_{i}\\frac{\\sum_{i=1}^{n}{x_{i}y_{i}}}\n                                            {\\sum_{i'=1}^{n}{x_{i'}^2} } \\\\\n\\sum_{i'=1}^{n}{a_{i'}} \\sum_{i'=1}^{n}{y_{i'}} & = \\frac{x_{i}\\sum_{i=1}^{n}{x_{i}}}\n                                                         {\\sum_{i'=1}^{n}{x_{i'}^2} }\n                                                    \\sum_{i=1}^{n} {y_{i}} \\\\\n\\sum_{i'=1}^{n}{a_{i'}} & = \\frac{x_{i}\\sum_{i=1}^{n}{x_{i}}}\n                                                         {\\sum_{i'=1}^{n}{x_{i'}^2} }\n\\end{split}\n\\]\n\nUsing (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \\((\\overline{x},\\overline{x})\\).\n\nAs you can see bellow the intercept it’s the responsible for that property.\n\\[\n\\begin{split}\n\\hat{y} & = \\left( \\hat{\\beta}_{0} \\right) + \\hat{\\beta}_{1} \\overline{x} \\\\\n\\hat{y} & = \\overline{y} - \\hat{\\beta}_{1}\\overline{x} + \\hat{\\beta}_{1} \\overline{x} \\\\\n\\hat{y} & = \\overline{y}\n\\end{split}\n\\]"
  },
  {
    "objectID": "03-execises.html#applied",
    "href": "03-execises.html#applied",
    "title": "03 - Linear Regression",
    "section": "Applied",
    "text": "Applied\n\nThis question involves the use of simple linear regression on the Auto data set.\n\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.\n\n\nlibrary(ISLR2)\n\nAutoSimpleModel <- lm(mpg ~ horsepower, data = Auto)\n\nsummary(AutoSimpleModel)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.935861   0.717499   55.66   <2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\nAs we see the regression p-value is much lower than 0.05 and we can reject the null hypotheses to conclude that there is a strong relationship between the response en the predictor. The coefficient of horsepower is negative, so we know that as the predictor increase the response decrease.\n\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals.\n\n\npredict(AutoSimpleModel, newdata = data.frame(horsepower = 98), interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\npredict(AutoSimpleModel, newdata = data.frame(horsepower = 98), interval = \"prediction\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\n\n\nPlot the response and the predictor. Use the abline() function to display the least squares regression line.\n\n\nplot(Auto$horsepower,Auto$mpg)\nabline(AutoSimpleModel)\n\n\n\n\n\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\n\n\npar(mfrow = c(2, 2))\nplot(AutoSimpleModel)\n\n\n\n\nThe Residuals vs Fitted shows that the relation is not linear and variance isn’t constant.\n\nThis question involves the use of multiple linear regression on the Auto data set.\n\n\nProduce a scatterplot matrix which includes all of the variables in the data set.\n\n\npairs(Auto)\n\n\n\n\n\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.\n\n\nAuto |>\n  subset(select = -name)|>\n  cor()\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\n\n\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance\n\n\nAutoModelNoInteraction <- \n  lm(mpg ~ . -name, data = Auto)\n\nAutoModelNoInteractionummary <- \n  summary(AutoModelNoInteraction)\n\nAutoModelNoInteractionummary\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  < 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  < 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: < 2.2e-16\n\n\n\nIs there a relationship between the predictors and the response?\n\nAs the regression p-value is bellow 0.05 we can reject the null hypothesis and conclude that at least one of the predictors have a relation with the response.\n\nWhich predictors appear to have a statistically significant relationship to the response?\n\n\nAutoModelNoInteractionummary |>\n  coefficients() |>\n  as.data.frame() |>\n  subset(`Pr(>|t|)` < 0.05)\n\n                  Estimate   Std. Error   t value     Pr(>|t|)\n(Intercept)  -17.218434622 4.6442941494 -3.707438 2.401841e-04\ndisplacement   0.019895644 0.0075150792  2.647430 8.444649e-03\nweight        -0.006474043 0.0006520478 -9.928787 7.874953e-21\nyear           0.750772678 0.0509731223 14.728795 3.055983e-39\norigin         1.426140495 0.2781360924  5.127492 4.665681e-07\n\n\n\nWhat does the coefficient for the year variable suggest?\n\nIt suggests that cars in average cars can drive 0.75 more miles per gallon every year.\n\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n\nNon-linearity of the response-predictor relationships\nNon-constant variance\nHigh-leverage points\n\n\n\n\npar(mfrow = c(2, 2))\nplot(AutoModelNoInteraction)\n\n\n\n\n\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\n\nremove_rownames <- function(DF){\n  \n  DF <- cbind(name = row.names(DF), DF)\n  rownames(DF) <- NULL\n  return(DF)\n  \n}\n\n\nnames(Auto) |>\n  setdiff(c(\"mpg\",\"name\")) |>\n  (\\(x) c(x,\n          combn(x, m = 2, \n                FUN = \\(y) paste0(y,collapse =\":\"))))() |>\n  paste0(collapse = \" + \") |>\n  paste0(\"mpg ~ \", predictors = _) |>\n  lm(data = Auto) |>\n  summary() |>\n  coef() |>\n  as.data.frame() |>\n  remove_rownames() |>\n  subset(`Pr(>|t|)` < 0.05 | name == \"year\")\n\n                  name      Estimate  Std. Error   t value    Pr(>|t|)\n3         displacement  -0.478538689 0.189353429 -2.527225 0.011920695\n6         acceleration  -5.859173212 2.173621188 -2.695582 0.007353578\n7                 year   0.697430284 0.609670317  1.143947 0.253399572\n8               origin -20.895570401 7.097090511 -2.944245 0.003445892\n18   displacement:year   0.005933802 0.002390716  2.482019 0.013515633\n27   acceleration:year   0.055621508 0.025581747  2.174265 0.030330641\n28 acceleration:origin   0.458316099 0.156659694  2.925552 0.003654670\n\n\n\nTry a few different transformations of the variables, such as \\(\\log{x}\\), \\(\\sqrt{x}\\), \\(x^2\\). Comment on your findings.\n\nAs we can see bellow we can explain 3% more of the variability by applying log to some variables.\n\nlibrary(data.table)\n\napply_fun_lm <- function(FUN,DF, trans_vars, remove_vars){\n  \n    as.data.table(DF\n    )[, (trans_vars) := lapply(.SD, FUN), .SDcols = trans_vars\n    ][, !remove_vars, with = FALSE\n    ][, lm(mpg ~ . , data = .SD)] |>\n    summary() |>\n    (\\(x) data.table(adj.r.squared = x$adj.r.squared,\n                     sigma = x$sigma,\n                     p.value = pf(x$fstatistic[\"value\"], \n                                  x$fstatistic[\"numdf\"], \n                                  x$fstatistic[\"dendf\"], \n                                  lower.tail = FALSE)))()\n  \n}\n\n\ndata.table(function_name = c(\"original\",\"log\", \"sqrt\",\"x^2\"),\n           function_list = list(\\(x) x,log, sqrt, \\(x) x^2)\n)[, data :=  \n    lapply(function_list, \n           FUN = apply_fun_lm,\n           DF = Auto, \n           trans_vars = c(\"displacement\", \"horsepower\", \n                          \"weight\", \"acceleration\"),\n           remove_vars = \"name\")\n][, rbindlist(data) |> cbind(function_name, end = _)]\n\n   function_name end.adj.r.squared end.sigma   end.p.value\n1:      original         0.8182238  3.327682 2.037106e-139\n2:           log         0.8474528  3.048425 5.352738e-154\n3:          sqrt         0.8312704  3.206041 1.304165e-145\n4:           x^2         0.7986663  3.502124 6.372862e-131\n\n\n\nThis question should be answered using the Carseats data set.\n\n\nFit a multiple regression model to predict Sales using Price, Urban, and US.\n\n\nCarseatsModel <-\n  lm(Sales~Price+Urban+US, data = Carseats)\n\nCarseatsModelSummary <-  \n  summary(CarseatsModel)\n\nCarseatsModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price + Urban + US, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9206 -1.6220 -0.0564  1.5786  7.0581 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.043469   0.651012  20.036  < 2e-16 ***\nPrice       -0.054459   0.005242 -10.389  < 2e-16 ***\nUrbanYes    -0.021916   0.271650  -0.081    0.936    \nUSYes        1.200573   0.259042   4.635 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.472 on 396 degrees of freedom\nMultiple R-squared:  0.2393,    Adjusted R-squared:  0.2335 \nF-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16\n\n\n\nProvide an interpretation of each coeﬃcient in the model. Be careful—some of the variables in the model are qualitative!\n\n\nCarseatsInterationModel <-\n  lm(Sales~Price*Urban*US, data = Carseats)\n\nCarseatsInterationModelSummary <-  \n  summary(CarseatsInterationModel)\n\nCarseatsInterationModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price * Urban * US, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7952 -1.6659 -0.0984  1.6119  7.2433 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          13.456350   1.727210   7.791 6.03e-14 ***\nPrice                -0.061657   0.014875  -4.145 4.17e-05 ***\nUrbanYes             -0.651545   2.071401  -0.315    0.753    \nUSYes                 2.049051   2.322591   0.882    0.378    \nPrice:UrbanYes        0.010793   0.017796   0.606    0.545    \nPrice:USYes          -0.001567   0.019972  -0.078    0.937    \nUrbanYes:USYes       -1.122034   2.759662  -0.407    0.685    \nPrice:UrbanYes:USYes  0.001288   0.023619   0.055    0.957    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.473 on 392 degrees of freedom\nMultiple R-squared:  0.2467,    Adjusted R-squared:  0.2333 \nF-statistic: 18.34 on 7 and 392 DF,  p-value: < 2.2e-16\n\n\n\nWrite out the model in equation form, being careful to handle the qualitative variables properly.\n\n\ncoef(CarseatsInterationModel) |>\n  round(3) |>\n  (\\(x) paste0(ifelse(x < 0, \" - \",\" + \"), abs(x),\" \\text{ \", names(x),\"}\"))() |>\n  sub(pattern = \" \\text{ (Intercept)}\",replacement = \"\", fixed = TRUE) |>\n  paste0(collapse = \"\") |>\n  sub(pattern = \"^ \\\\+ \", replacement = \"\") |>\n  sub(pattern = \"^ - \", replacement = \"\") |>\n  paste0(\"hat{Y} = \", FUN = _)\n\n[1] \"hat{Y} = 13.456 - 0.062 \\text{ Price} - 0.652 \\text{ UrbanYes} + 2.049 \\text{ USYes} + 0.011 \\text{ Price:UrbanYes} - 0.002 \\text{ Price:USYes} - 1.122 \\text{ UrbanYes:USYes} + 0.001 \\text{ Price:UrbanYes:USYes}\"\n\n\n\\[\n\\begin{split}\n\\hat{Sales} & = 13.456 - 0.062 \\text{ Price} - 0.652 \\text{ UrbanYes} \\\\\n            & \\quad + 2.049 \\text{ USYes} + 0.011 \\text{ Price:UrbanYes} \\\\\n            & \\quad - 0.002 \\text{ Price:USYes} - 1.122 \\text{ UrbanYes:USYes} \\\\\n            & \\quad + 0.001 \\text{ Price:UrbanYes:USYes}\n\\end{split}\n\\]\n\nFor which of the predictors can you reject the null hypothesis H0 : βj = 0?\n\n\ncoef(CarseatsInterationModelSummary) |>\n  as.data.frame() |>\n  (\\(DF) DF[DF$`Pr(>|t|)` < 0.05,])()\n\n               Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 13.45634952 1.72720976  7.790802 6.030364e-14\nPrice       -0.06165717 0.01487479 -4.145079 4.165536e-05\n\n\n\nOn the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n\n\nCarseatsPriceModel <-\n  lm(Sales~Price, data = Carseats)\n\nCarseatsPriceModelSummary <-\n  summary(CarseatsPriceModel)\n\nCarseatsPriceModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.641915   0.632812  21.558   <2e-16 ***\nPrice       -0.053073   0.005354  -9.912   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: < 2.2e-16\n\n\n\nHow well do the models in (a) and (e) ﬁt the data?\n\nModel a fits better to the data with 0.23 against 0.2 of model e.\n\nUsing the model from (e), obtain 95 % conﬁdence intervals for the coeﬃcient(s).\n\n\nconfint(CarseatsPriceModel, level = 0.95)\n\n                 2.5 %      97.5 %\n(Intercept) 12.3978438 14.88598655\nPrice       -0.0635995 -0.04254653\n\n\n\nIs there evidence of outliers or high leverage observations in the model from (e)?\n\n\npar(mfrow = c(2,2))\nplot(CarseatsPriceModel)\n\n\n\npar(mfrow = c(1,1))\n\nThere is a leverage point.\n\nIn this problem we will investigate the t-statistic for the null hypothesis H0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.\n\n\nset.seed(1)\n\nx <- rnorm(100)\ny <- 2*x+rnorm(100)\n\nSimulatedData <- data.frame(x, y) \n\n\nPerform a simple linear regression of y onto x, without an intercept. Report the coeﬃcient estimate ˆβ, the standard error of this coeﬃcient estimate, and the t-statistic and p-value associated with the null hypothesis H0 : β = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).)\n\nAs we can see below we can reject the null hypothesis and conclude that y increases 1.99 for each unit of x explaining 78% of the variability.\n\nlm(y~ x+0, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9154 -0.6472 -0.1771  0.5056  2.3109 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   1.9939     0.1065   18.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9586 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nNow perform a simple linear regression of x onto y without an intercept, and report the coeﬃcient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis H0 : β = 0. Comment on these results.\n\nAs we can see below we can reject the null hypothesis and conclude that x increases 0.39 for each unit of y explaining 78% of the variability.\n\nlm(x~ y+0, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8699 -0.2368  0.1030  0.2858  0.8938 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \ny  0.39111    0.02089   18.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4246 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nWhat is the relationship between the results obtained in (a) and (b)?\n\ny can explain x as well a x explains y.\n\nIn R, show that when regression is performed with an intercept, the t-statistic for H0 : β1 = 0 is the same for the regression of y onto x as it is for the regression of x onto y.\n\nAs you can see below the t-statistic for \\(\\beta_{1}\\) is t-statistic for both regressions is 18.56.\n\nlm(y~ x, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8768 -0.6138 -0.1395  0.5394  2.3462 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.03769    0.09699  -0.389    0.698    \nx            1.99894    0.10773  18.556   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9628 on 98 degrees of freedom\nMultiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 \nF-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\n\nlm(x~ y, data = SimulatedData) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y, data = SimulatedData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90848 -0.28101  0.06274  0.24570  0.85736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.03880    0.04266    0.91    0.365    \ny            0.38942    0.02099   18.56   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4249 on 98 degrees of freedom\nMultiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 \nF-statistic: 344.3 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\n\nThis problem involves simple linear regression without an intercept.\n\n\nRecall that the coeﬃcient estimate β for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coeﬃcient estimate for the regression of X onto Y the same as the coeﬃcient estimate for the regression of Y onto X?\n\nThe coefficient would be different between y~x and x~y.\n\nGenerate an example in R with n = 100 observations in which the coeﬃcient estimate for the regression of X onto Y is diﬀerent from the coeﬃcient estimate for the regression of Y onto X.\n\n\nset.seed(5)\nSimulatedData2 <- \n  data.frame(x = rnorm(100, mean = 8, sd = 4))\n\nset.seed(8)\nSimulatedData2$y <- \n  10*SimulatedData2$x + rnorm(100, sd = 10) \n\nplot(SimulatedData2$x, SimulatedData2$y)\n\n\n\nlm(y~ x+0, data = SimulatedData2) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.7686  -6.8107  -0.3744   6.5070  24.3187 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   9.9363     0.1207    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.81 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\nlm(x~ y+0, data = SimulatedData2) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2861 -0.5429  0.1264  0.7279  3.0421 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \ny 0.099191   0.001205    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.08 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nGenerate an example in R with n = 100 observations in which the coeﬃcient estimate for the regression of X onto Y is the same as the coeﬃcient estimate for the regression of Y onto X.\n\n\nset.seed(5)\nSimulatedData3 <- \n  data.frame(x = rnorm(100, mean = 8, sd = 4))\n\nset.seed(8)\nSimulatedData3$y <- \n  SimulatedData3$x + rnorm(100, sd = 1) \n\nplot(SimulatedData3$x, SimulatedData3$y)\n\n\n\nlm(y~ x+0, data = SimulatedData3) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.97686 -0.68107 -0.03744  0.65070  2.43187 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx  0.99363    0.01207    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.081 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\nlm(x~ y+0, data = SimulatedData3) |>\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2861 -0.5429  0.1264  0.7279  3.0421 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \ny  0.99191    0.01205    82.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.08 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: < 2.2e-16\n\n\n\nIn this exercise you will create some simulated data and will ﬁt simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.\n\n\nUsing the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0, 1) distribution. This represents a feature, X.\n\n\nset.seed(1)\n\nx <- rnorm(100)\n\n\nUsing the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution—a normal distribution with mean zero and variance 0.25.\n\n\neps <- rnorm(100, sd = sqrt(0.25))\n\n\nUsing x and eps, generate a vector y according to the model.\n\n\\[\nY = -1 + 0.5X + \\epsilon\n\\]\n\ny <- -1 + 0.5*x +eps\n\n\n- **What is the length of the vector y?**\n\nIt has the same length of x.\n\n\nWhat are the values of β0 and β1 in this linear model?\n\n\n\\(\\beta_{0} = -1\\) and \\(\\beta_{1} = 0.5\\).\n\nCreate a scatterplot displaying the relationship between x and y. Comment on what you observe.\n\n\nplot(x,y)\n\n\n\n\n\nFit a least squares linear model to predict y using x. Comment on the model obtained. How do ˆβ0 and ˆ β1 compare to β0 and β1?\n\nAfter rounding the value to one decimal the coefficients are the same.\n\nSimilatedModel <-lm(y~x)\n\nSimilatedModel |>\n  coef() |>\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\n\nDisplay the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a diﬀerent color. Use the legend() command to create an appropriate legend.\n\n\nplot(x,y)\nabline(SimilatedModel, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nNow ﬁt a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model ﬁt? Explain your answer.\n\nThere is no evidence that the polynomial model fits better to the data.\n\nSimilatedPolyModel <-lm(y~x+I(x^2))\n\nanova(SimilatedModel,SimilatedPolyModel)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ x + I(x^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     98 22.709                           \n2     97 22.257  1   0.45163 1.9682 0.1638\n\n\n\nRepeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term ϵ in (b). Describe your results.\n\n\nset.seed(1)\n\nx <- rnorm(100)\neps <- rnorm(100, sd = sqrt(0.10))\ny <- -1 + 0.5*x +eps\n\nplot(x,y)\n\n\n\n\nThe coefficients remind the same.\n\nSimilatedModel2 <-lm(y~x)\n\nSimilatedModel2 |>\n  coef() |>\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\nAnd the Lease Square Line and the Population Line are closer.\n\nplot(x,y)\nabline(SimilatedModel2, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nRepeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term ϵ in (b). Describe your results.\n\n\nset.seed(1)\n\nx <- rnorm(100)\neps <- rnorm(100, sd = sqrt(1.5))\ny <- -1 + 0.5*x +eps\n\nplot(x,y)\n\n\n\n\nThe coefficients remind the same.\n\nSimilatedModel3 <-lm(y~x)\n\nSimilatedModel3 |>\n  coef() |>\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\nDespite, y has a wider range of values are almost the same.\n\nplot(x,y)\nabline(SimilatedModel3, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nWhat are the conﬁdence intervals for β0 and β1 based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.\n\n\nlibrary(ggplot2)\n\n\nadd_source <- function(list.DT, source.name = \"source\"){\n  \n  table_names <- names(list.DT)\n  \n  for(tb_i in seq_along(list.DT)){\n    list.DT[[tb_i]][, (source.name) := names(list.DT)[tb_i] ] \n  }\n  \n  return(list.DT)\n}\n\n\nlist(original = SimilatedModel,\n     less_noisy = SimilatedModel2,\n     noisier = SimilatedModel3) |>\n lapply(\\(model) cbind(center = coef(model), confint(model)) |> \n                 as.data.table(keep.rownames = \"coef\")) |>\n add_source(source.name = \"model\") |>\n rbindlist() |>\n (\\(DT) DT[, model := factor(model, \n                        levels = c(\"less_noisy\", \"original\",\"noisier\"))] )() |>\n ggplot(aes(model, center, color = model))+\n  geom_hline(yintercept = 0, linetype = 2, size = 1)+\n  geom_point()+\n  geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`), width = 0.5)+\n  scale_color_brewer(palette = \"Blues\")+\n  facet_wrap(~coef, ncol = 2, scales = \"free_y\")+\n  labs(title = \"Coefficient Confident Intervals get wider\",\n       subtitle = \"as the error increase but it isn't enough to change conclusions\")+\n  theme_classic()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n\n\n\n\nThis problem focuses on the collinearity problem.\n\n\nPerform the following commands in R:\n\n\nset.seed(1)\nx1 <- runif(100)\nx2 <- 0.5*x1+rnorm(100) / 10\ny <- 2+2*x1+0.3*x2+rnorm(100)\n\n\nThe last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coeﬃcients?\n\n\\[\nY = 2 + 2 x_{1} + 0.3 x_{2} + \\epsilon\n\\]\n\nWhat is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.\n\n\nplot(x1,x2, \n     main = paste0(\"x1 and x2 correlation :\",round(cor(x1,x2), 2)))\n\n\n\n\n\nUsing this data, ﬁt a least squares regression to predict y using x1 and x2. Describe the results obtained. What are ˆβ0, ˆ β1, and ˆβ2? How do these relate to the true β0, β1, and β2? Can you reject the null hypothesis H0 : β1 = 0? How about the null hypothesis H0 : β2 = 0?\n\n\nSimulatedModelExc14 <- lm(y~x1+x2)\n\nSimulatedModelExc14Summary <- summary(SimulatedModelExc14)\n\nSimulatedModelExc14Summary\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8311 -0.7273 -0.0537  0.6338  2.3359 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1305     0.2319   9.188 7.61e-15 ***\nx1            1.4396     0.7212   1.996   0.0487 *  \nx2            1.0097     1.1337   0.891   0.3754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.056 on 97 degrees of freedom\nMultiple R-squared:  0.2088,    Adjusted R-squared:  0.1925 \nF-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05\n\n\nThe \\(\\hat{\\beta}_{0} = 2.13\\) which is really close to the true value of \\(\\beta_{0} = 2\\), but \\(\\hat{\\beta}_{1}\\) and \\(\\hat{\\beta}_{2}\\) are very different to their real values. We almost can not reject the null hypothesis for \\(\\beta_{1}\\) and can not reject the null hypothesis for \\(\\beta_{1}\\) where both should be significant to explain \\(\\hat{Y}\\).\n\nNow ﬁt a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?\n\n\nlm(y~x1) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89495 -0.66874 -0.07785  0.59221  2.45560 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1124     0.2307   9.155 8.27e-15 ***\nx1            1.9759     0.3963   4.986 2.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.055 on 98 degrees of freedom\nMultiple R-squared:  0.2024,    Adjusted R-squared:  0.1942 \nF-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06\n\n\nNow \\(\\beta_{1}\\) we can surely reject the null t-value is now 2.5 times higher that it used to be.\n-Now ﬁt a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?\n\nlm(y~x2) |>\n  summary()\n\n\nCall:\nlm(formula = y ~ x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62687 -0.75156 -0.03598  0.72383  2.44890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.3899     0.1949   12.26  < 2e-16 ***\nx2            2.8996     0.6330    4.58 1.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.072 on 98 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.1679 \nF-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05\n\n\nNow \\(\\beta_{2}\\) we can surely reject the null t-value is now 5.14 times higher that it used to be.\n\nDo the results obtained in (c)–(e) contradict each other? Explain your answer.\n\nYes, they do. In c, we couldn’t reject the null hypothesis for x2 but that change in the e question.\n\nNow suppose we obtain one additional observation, which was unfortunately mismeasured.\n\n\nx1_c<-c(x1, 0.1)\nx2_c<-c(x2, 0.8)\ny_c<-c(y, 6)\n\n\nRe-ﬁt the linear models from (c) to (e) using this new data. What eﬀect does this new observation have on the each of the models?\n\nThanks the additional row x2 seems to be significant rather than x1.\n\nModelC <- lm(y_c~x1_c+x2_c) \nsummary(ModelC)\n\n\nCall:\nlm(formula = y_c ~ x1_c + x2_c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.73348 -0.69318 -0.05263  0.66385  2.30619 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2267     0.2314   9.624 7.91e-16 ***\nx1_c          0.5394     0.5922   0.911  0.36458    \nx2_c          2.5146     0.8977   2.801  0.00614 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.075 on 98 degrees of freedom\nMultiple R-squared:  0.2188,    Adjusted R-squared:  0.2029 \nF-statistic: 13.72 on 2 and 98 DF,  p-value: 5.564e-06\n\n\nIn the next model, we can see that the previous model was fitting better to y based on x1. The \\(R^2\\) went down from 0.20 to 0.16.\n\nModelD <- lm(y_c~x1_c) \nsummary(ModelD)\n\n\nCall:\nlm(formula = y_c ~ x1_c)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8897 -0.6556 -0.0909  0.5682  3.5665 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2569     0.2390   9.445 1.78e-15 ***\nx1_c          1.7657     0.4124   4.282 4.29e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.111 on 99 degrees of freedom\nMultiple R-squared:  0.1562,    Adjusted R-squared:  0.1477 \nF-statistic: 18.33 on 1 and 99 DF,  p-value: 4.295e-05\n\n\nIn the next model, we can see that the previous model was fitting worse to y based on x2. The \\(R^2\\) went up from 0.18 to 0.21.\n\nModelE <- lm(y_c~x2_c) \nsummary(ModelE)\n\n\nCall:\nlm(formula = y_c ~ x2_c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.64729 -0.71021 -0.06899  0.72699  2.38074 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.3451     0.1912  12.264  < 2e-16 ***\nx2_c          3.1190     0.6040   5.164 1.25e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.074 on 99 degrees of freedom\nMultiple R-squared:  0.2122,    Adjusted R-squared:  0.2042 \nF-statistic: 26.66 on 1 and 99 DF,  p-value: 1.253e-06\n\n\n\nIn each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.\n\nIn the c model the last observation is a high-leverage point.\n\npar(mfrow = c(2,2))\nplot(ModelC)\n\n\n\n\nIn the d model the last observation is an outlier point as it’s studentized residuals is greater than 3.\n\npar(mfrow = c(2,2))\nplot(ModelD)\n\n\n\n\nIn the e model the last observation is a high-leverage point.\n\npar(mfrow = c(2,2))\nplot(ModelE)\n\n\n\n\n\nThis problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n\n\nFor each predictor, ﬁt a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically signiﬁcant association between the predictor and the response? Create some plots to back up your assertions.\n\nAs we can see below the only predictor that wasn’t found significant was the chas one.\n\nBostonModelSummary <-\n  data.table(predictor = colnames(Boston) |> setdiff(\"crim\")\n  )[, model := lapply(predictor, \\(x) paste0(\"crim~\",x) |> \n                           lm(data = Boston) |>\n                           summary() |>\n                           coef() |>\n                           as.data.table(keep.rownames = \"coef\")) \n  ][, model[[1]], \n    by = \"predictor\"\n  ][predictor == coef, !c(\"coef\")\n  ][, is_significant :=  `Pr(>|t|)` < 0.05\n  ][order(`Pr(>|t|)`)]\n\nBostonModelSummary\n\n    predictor    Estimate  Std. Error   t value     Pr(>|t|) is_significant\n 1:       rad  0.61791093 0.034331820 17.998199 2.693844e-56           TRUE\n 2:       tax  0.02974225 0.001847415 16.099388 2.357127e-47           TRUE\n 3:     lstat  0.54880478 0.047760971 11.490654 2.654277e-27           TRUE\n 4:       nox 31.24853120 2.999190381 10.418989 3.751739e-23           TRUE\n 5:     indus  0.50977633 0.051024332  9.990848 1.450349e-21           TRUE\n 6:      medv -0.36315992 0.038390175 -9.459710 1.173987e-19           TRUE\n 7:       dis -1.55090168 0.168330031 -9.213458 8.519949e-19           TRUE\n 8:       age  0.10778623 0.012736436  8.462825 2.854869e-16           TRUE\n 9:   ptratio  1.15198279 0.169373609  6.801430 2.942922e-11           TRUE\n10:        rm -2.68405122 0.532041083 -5.044819 6.346703e-07           TRUE\n11:        zn -0.07393498 0.016094596 -4.593776 5.506472e-06           TRUE\n12:      chas -1.89277655 1.506115484 -1.256727 2.094345e-01          FALSE\n\n\nAny relation seems to be really linear and the chas predictor has been wrongly classify as numeric when it should have been a qualitative variable.\n\nfor(predictor in BostonModelSummary$predictor){\n \n  cor(Boston[[predictor]],Boston$crim) |>\n  round(2) |>\n  paste0(\"Crim vs \", predictor,\"\\nCorrelation :\", correlation = _) |>\n  plot(Boston[[predictor]],Boston$crim, \n       xlab = predictor, ylab = \"crim\", \n       main = _)\n   \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut after changing chas to a factor we keep the same conclusion and the coefficient it’s the same.\n\nlm(crim~as.factor(chas), data = Boston) |>\n  summary()\n\n\nCall:\nlm(formula = crim ~ as.factor(chas), data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.738 -3.661 -3.435  0.018 85.232 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        3.7444     0.3961   9.453   <2e-16 ***\nas.factor(chas)1  -1.8928     1.5061  -1.257    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.597 on 504 degrees of freedom\nMultiple R-squared:  0.003124,  Adjusted R-squared:  0.001146 \nF-statistic: 1.579 on 1 and 504 DF,  p-value: 0.2094\n\n\n\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0\n\nNow we just can reject the null hypothesis for the following predictos:\n\nBostonModel2 <-\n  lm(formula = crim ~ . , data = Boston)\n\nBostonModelSummary2 <-\n  summary(BostonModel2) |>\n  coef() |>\n  as.data.table(keep.rownames = \"predictor\")\n\nBostonModelSummary2[`Pr(>|t|)` < 0.05]\n\n   predictor    Estimate Std. Error   t value     Pr(>|t|)\n1:        zn  0.04571004 0.01879032  2.432637 1.534403e-02\n2:       dis -1.01224674 0.28246757 -3.583586 3.725942e-04\n3:       rad  0.61246531 0.08753576  6.996744 8.588123e-12\n4:      medv -0.22005636 0.05982396 -3.678399 2.605302e-04\n\n\n\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coeﬃcients from (a) on the x-axis, and the multiple regression coeﬃcients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coeﬃcient in a simple linear regression model is shown on the x-axis, and its coeﬃcient estimate in the multiple linear regression model is shown on the y-axis.\n\n\nmerge(BostonModelSummary, BostonModelSummary2,\n      by = \"predictor\", suffixes = c(\"_uni\",\"_multi\")\n  )[, .(predictor,\n        Estimate_uni = round(Estimate_uni, 2),\n        Estimate_multi = round(Estimate_multi, 2), \n        coef_change = abs(Estimate_uni / Estimate_multi),\n        vif = car::vif(BostonModel2)[predictor],\n        kept_significant = is_significant & `Pr(>|t|)_multi` < 0.05)\n  ][, predictor := reorder(predictor, coef_change)] |>\n  ggplot(aes(coef_change,vif))+\n  geom_point(aes(color = kept_significant))+\n  geom_text(aes(label = predictor), vjust = 1.2)+\n  scale_color_manual(values = c(\"TRUE\" = \"dodgerblue4\", \"FALSE\" = \"gray80\"))+\n  scale_x_log10()+\n  scale_y_log10()+\n  labs(title = \"Significan Predictors Change Less\",\n       subtitle = \"Predictos Coeﬃcient Change Between Simple and Multiple Lineal Models\")+\n  theme_classic()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n\n\n\n\nIs there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, ﬁt a model of the form.\n\n\\[\nY = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^2 + \\beta_{3} X^2 + \\epsilon\n\\]\n\nSimpleVsPolySummary <-\n  data.table(predictor = colnames(Boston) |> setdiff(c(\"crim\",\"chas\"))\n  )[,`:=`(r2_simple = sapply(predictor, \\(x) paste0(\"crim~\",x) |> \n                               lm(data = Boston) |>\n                               summary() |>\n                               (\\(x) x[[\"r.squared\"]])() ),\n          r2_poly = sapply(predictor, \\(x) gsub(\"x\",x,\"crim~x+I(x^2)+I(x^3)\") |> \n                             lm(data = Boston) |>\n                             summary() |>\n                             (\\(x) x[[\"r.squared\"]])() )), \n  ][, change := r2_poly - r2_simple\n  ][order(-change)]\n\n\nSimpleVsPolySummary[, lapply(.SD, \\(x) \n                             if(is.numeric(x)) scales::percent(x, accuracy = 0.01)\n                             else x)]\n\n    predictor r2_simple r2_poly change\n 1:      medv    15.08%  42.02% 26.94%\n 2:       dis    14.41%  27.78% 13.37%\n 3:       nox    17.72%  29.70% 11.98%\n 4:     indus    16.53%  25.97%  9.43%\n 5:       age    12.44%  17.42%  4.98%\n 6:   ptratio     8.41%  11.38%  2.97%\n 7:       tax    33.96%  36.89%  2.93%\n 8:        rm     4.81%   6.78%  1.97%\n 9:        zn     4.02%   5.82%  1.81%\n10:     lstat    20.76%  21.79%  1.03%\n11:       rad    39.13%  40.00%  0.88%\n\nfor(predictor in SimpleVsPolySummary[change >= 0.1 ,predictor]){\n \n  cor(Boston[[predictor]],Boston$crim) |>\n  round(2) |>\n  paste0(\"Crim vs \", predictor,\"\\nCorrelation :\", correlation = _) |>\n  plot(Boston[[predictor]],Boston$crim, \n       xlab = predictor, ylab = \"crim\", \n       main = _)\n   \n}"
  },
  {
    "objectID": "04-execises.html",
    "href": "04-execises.html",
    "title": "04 - Classification",
    "section": "",
    "text": "library(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(klaR)\nlibrary(poissonreg)"
  },
  {
    "objectID": "04-execises.html#custom-functions",
    "href": "04-execises.html#custom-functions",
    "title": "04 - Classification",
    "section": "Custom functions",
    "text": "Custom functions\nIn this section I place the functions that were used in many code chunks across the chapter.\n\nrbind_list_name <- function(list.DT, new_col_name, ...){\n  \n  stopifnot(is.list(list.DT))\n  stopifnot(all(sapply(list.DT, is.data.table)))\n  \n  lapply(seq_along(list.DT), \n         function(tb_i){\n           list.DT[[tb_i]][, (new_col_name) := names(list.DT)[tb_i]] }) |>\n  rbindlist(...)\n}\n\nget_class_metrics <- metric_set(sens, spec)\n\nget_class_matrix_metrics <- function(model,\n                                     split,\n                                     fit_formula,\n                                     metric_function = metric_set(sens, spec),\n                                     ...){\n  \n testing_predictions <-\n  model |>\n  last_fit(as.formula(fit_formula), split = split) |>\n  collect_predictions() \n \n truth_var <- sub(pattern = \" *~.+$\", replacement = \"\", x = fit_formula)\n \n list(conf_mat = conf_mat(testing_predictions, \n                          truth = !!truth_var, \n                          estimate = .pred_class),\n      metrics = metric_function(testing_predictions,\n                                truth = !!truth_var,\n                                estimate = .pred_class,\n                                ...) )\n}\n\n\nmodel_knn_k <- function(k){\n  \n  model <-\n    nearest_neighbor(neighbors = k) |>\n    set_mode(\"classification\") |>\n    set_engine(\"kknn\")\n  \n  return(model)\n}\n\n\nModelNaiveBayes <- \n  naive_Bayes() |>\n  set_mode(\"classification\") |> \n  set_engine(\"klaR\") |>\n  set_args(usekernel = FALSE)\n\n\nevaluate_model <- function(model, recipe_list, split){\n  \n  lapply(recipe_list, function(recipe, split){\n    workflow() |>\n      add_recipe(recipe) |>\n      add_model(model) |>\n      last_fit(split = split) |>\n      collect_metrics() |>\n      as.data.table() }, \n  split = split) |>\n  rbind_list_name(new_col_name = \"recipe\")\n  \n}"
  },
  {
    "objectID": "04-execises.html#conceptual",
    "href": "04-execises.html#conceptual",
    "title": "04 - Classification",
    "section": "Conceptual",
    "text": "Conceptual\n1\n1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\\[\n\\begin{split}\np(X) &= \\frac{e^{\\beta_{0}+\\beta_{1}X}}\n            {1+e^{\\beta_{0}+\\beta_{1}X}} \\\\\np(X) (1+e^{\\beta_{0}+\\beta_{1}X}) & = e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X)+p(X) e^{\\beta_{0}+\\beta_{1}X} & = e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X) & = e^{\\beta_{0}+\\beta_{1}X} - p(X) e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X) & = (1 - p(X))e^{\\beta_{0}+\\beta_{1}X} \\\\\n\\frac{p(X)}{1 - p(X)} & = e^{\\beta_{0}+\\beta_{1}X}\n\\end{split}\n\\]\n2\n2. It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a \\(N(\\mu_k,\\sigma^2)\\) distribution, the Bayes classiﬁer assigns an observation to the class for which the discriminant function is maximized.\n\nTo prove that both functions would provide the same class we need to check that both functions change from one \\(Y\\) class to other in the same \\(x\\) value.\nIf \\(K = 2\\) and \\(\\pi_1 = \\pi_2 = \\pi_c\\) we can show that:\n\n\\[\n\\begin{split}\np_{1}(x) & = p_{2}(x) \\\\\n\\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{l})^2}}\n& =\n\\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{l})^2}} \\\\\ne^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}\n& =\ne^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2} \\\\\n\\frac{e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}}{e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2}}\n& = 1 \\\\\n(x - \\mu_{2})^2 - (x - \\mu_{1})^2\n& = 0 \\\\\nx^2 - 2x\\mu_{2} + \\mu_{2}^2 - (x^2 - 2x\\mu_{1} + \\mu_{1}^2)\n& = 0 \\\\\n2x (\\mu_{1}- \\mu_{2}) & = \\mu_{1}^2 - \\mu_{2}^2  \\\\\nx & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2 (\\mu_{1}- \\mu_{2})} \\\\\nx & = \\frac{\\mu_1 + \\mu_2}{2}\n\\end{split}\n\\]\n\nAnd also:\n\n\\[\n\\begin{split}\n\\delta_{1}(x) & = \\delta_{2}(x) \\\\\n\\log{(\\pi_{c})}\n- \\frac{\\mu_{1}^2}{2\\sigma^2}\n+ x \\cdot \\frac{\\mu_{1}}{\\sigma^2}\n& =\n\\log{(\\pi_{c})}\n- \\frac{\\mu_{2}^2}{2\\sigma^2}\n+ x \\cdot \\frac{\\mu_{2}}{\\sigma^2} \\\\\nx (\\mu_{1} - \\mu_{2}) & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2} \\\\\nx & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2(\\mu_{1} - \\mu_{2})} \\\\\nx & = \\frac{\\mu_1 + \\mu_2}{2}\n\\end{split}\n\\]\n\nLet’s see an example visually by setting as example the next values for each \\(Y\\) class:\n\n\n\n\\(k\\)\n\\(\\sigma\\)\n\\(\\pi\\)\n\\(\\mu\\)\n\n\n\n1\n0.5\n0.5\n2\n\n\n2\n0.5\n0.5\n4\n\n\n\n\nldm_k_prop <- function(x,\n                       k,\n                       sigma = c(0.5,0.5),\n                       pi_k = c(0.5,0.5),\n                       mu = c(2, 4),\n                       logit = FALSE){\n  \n  if(logit){\n    \n    return(x * mu[k]/sigma[k]^2 - mu[k]^2/(2*sigma[k]^2) + log(pi_k[k]))\n    \n  }\n  \n  denominator <-\n    sapply(x, \\(y) sum(pi_k * (1/(sqrt(2*pi)*sigma)) * exp(-1/(2*sigma^2) * (y - mu)^2) ) )\n  \n  k_numerador <-\n   (pi_k[k]* (sqrt(2*pi)*sigma[k])^-1 * exp(- (2*sigma[k]^2)^-1 * (x - mu[k])^2))\n  \n  return(k_numerador / denominator)\n  \n}\n\n\nBasePlot <-\n  data.frame(x = 1:5) |>\n  ggplot(aes(x))+\n  scale_x_continuous(breaks = scales::breaks_width(1))+\n  theme_light()+\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n  \np1 <-\n  BasePlot +\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 1), color = \"blue\")+\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 2), color = \"red\")\n\np2 <-\n  BasePlot +\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 1, logit = TRUE), color = \"blue\")+\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 2, logit = TRUE), color = \"red\")\n\n\n(p1 / p2) +\n  plot_annotation(title = 'Comparing Proportion Function vs Logit Function of LDA',\n                  theme = theme(plot.title = element_text(face = \"bold\")) )\n\n\n\n\n3\n3. This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where \\(p = 1\\); i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, \\(X ∼ N(\\mu_k, \\sigma^2_k)\\). Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\n\\[\n\\begin{split}\np_k(x) & =\n\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi} \\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n& =\n\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi} \\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\frac{1}{\\sqrt{2\\pi}}\n     \\sum_{l=1}^{K}\\pi_l \\frac{1}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n& =\n\\frac{\\frac{\\pi_k}{\\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K} \\frac{\\pi_l}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n\\end{split}\n\\]\n\nAs the denominator is a constant for any \\(k\\), we can define: \\(g(x) = \\frac{1}{\\sum_{l=1}^{K} \\frac{\\pi_l}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}}\\)\n\n\n\\[\n\\begin{split}\np_k(x) & = g(x) \\frac{\\pi_k}{\\sigma_k}\n           e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2}\\\\\n\\log{(p_k(x))}& =\n\\log{\\left( g(x) \\frac{\\pi_k}{\\sigma_k}\n            e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2} \\right)}\\\\\n& =\n\\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} -\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2 \\\\\n& =\n\\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} -\\frac{\\mu_k^2 - 2\\mu_kx + x^2}{2\\sigma_k^2} \\\\\n& =\n\\left( \\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} - \\frac{\\mu_k^2}{2\\sigma_k^2} \\right)\n+ \\frac{\\mu_k}{\\sigma_k^2} \\cdot x - \\frac{1}{2\\sigma_k^2} \\cdot x^2 \\\\\n\\end{split}\n\\]\n4\n4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.\nA\n(A) Suppose that we have a set of observations, each with measurements on \\(p = 1\\) feature, X. We assume that X is uniformly (evenly) distributed on \\([0, 1]\\). Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of the range of \\(X\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X = 0.6\\), we will use observations in the range \\([0.55, 0.65]\\). On average, what fraction of the available observations will we use to make the prediction?\n\nset.seed(123)\n\nUnifDistVar1 <-\n  data.table(sample = 1:1000\n  )[, .(x1 = runif(1000)),\n    by = \"sample\"\n  ][, .(prop = mean(x1 %between% c(0.6 - 0.1/2, 0.6 + 0.1/2))),\n    by = \"sample\"]\n  \nmean(UnifDistVar1$prop)\n\n[1] 0.100297\n\n\nB\n(B) Now suppose that we have a set of observations, each with measurements on \\(p = 2\\) features, \\(X_1\\) and \\(X_2\\). We assume that \\((X_1, X_2)\\) are uniformly distributed on \\([0, 1] \\times [0, 1]\\). We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of \\(X_2\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X_1 = 0.6\\) and \\(X_2 = 0.35\\), we will use observations in the range \\([0.55, 0.65]\\) for \\(X_1\\) and in the range \\([0.3, 0.4]\\) for \\(X_2\\). On average, what fraction of the available observations will we use to make the prediction?\n\nset.seed(123)\n\nUnifDistVar2 <-\n  UnifDistVar1[, .(x1 = runif(1000),\n                   x2 = runif(1000)),\n               by = \"sample\"\n  ][ , .(prop = mean(x1 %between% c(0.6 - 0.1/2, 0.6 + 0.1/2) &\n                     x2 %between% c(0.35 - 0.1/2, 0.35 + 0.1/2))),\n     by = \"sample\"]\n\nmean(UnifDistVar2$prop)\n\n[1] 0.009943\n\n\nC\n(C) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n\n0.1^100\n\n[1] 1e-100\n\n\nD\n(D) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.\n\nAs p gets lager every point has more specifications to meet and the number of point which meet them decrease.\nE (missing)\n(E) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For \\(p = 1\\), \\(2\\), and \\(100\\), what is the length of each side of the hypercube? Comment on your answer.\nNote: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When \\(p = 1\\), a hypercube is simply a line segment, when \\(p = 2\\) it is a square, and when \\(p = 100\\) it is a 100-dimensional cube.\n5\n5. We now examine the differences between LDA and QDA.\nA\n(A) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nType of set to test\nPerform better\n\n\n\ntraining set\n\nQDA, as it will model some noise\n\n\ntest set\n\nLDA, as decision boundary is lineal\n\n\nB\n(B) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nType of set to test\nPerform better\n\n\n\ntraining set\n\nQDA, as it will model some noise\n\n\ntest set\n\nQDA, as decision boundary is non-linear\n\n\nC\n(C) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n\nIf the Bayes decision boundary is non-linear, QDA could improve its test prediction accuracy as the sample size n increases. When the model has more examples it has less changes to overfit the data.\nD\n(D) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n\n\nFALSE, as QDA would model some noise that could affect the prediction accuracy.\n6\n6. Suppose we collect data for a group of students in a statistics class with variables \\(X_1\\) = hours studied, \\(X_2\\) = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0 = −6\\), \\(\\hat{\\beta}_1 = 0.05\\) and \\(\\hat{\\beta}_2 = 1\\).\nA\n(A) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.\n\nLogisticCoef <- -6 + 0.05 * 40 + 1* 3.5\n\nexp(LogisticCoef)/(1+exp(LogisticCoef))\n\n[1] 0.3775407\n\n\nB\n(B) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\n\\[\n\\begin{split}\n\\log{ \\left( \\frac{p(X)}{1 - p(X)} \\right)} & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\n\\log{ \\left( \\frac{0.5}{1 - 0.5} \\right)} & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\n0 & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\nX_2 & = \\frac{-\\beta_0-\\beta_{2}X_2}{\\beta_{1}} \\\\\nX_2 & = \\frac{-(-6)-(1 \\times 3.5)}{0.05} \\\\\nX_2 & = 50 \\text{ horas}\n\\end{split}\n\\]\n7\n7. Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was \\(\\overline{X}_y = 10\\), while the mean for those that didn’t was \\(\\overline{X}_n = 0\\). In addition, the variance of X for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(x = 4\\) last year.\n\\[\n\\begin{split}\np_{k}(x) & = \\frac{\\pi_{k} f_{k}(x)}\n                  {\\sum_{l=1}^{K} \\pi_{l} f_{l}(x)} \\\\\np_{y}(4) & =\n\\frac{0.8 \\times\n      \\left(\n      \\frac{e^{\\frac{-(4-10)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n      \\right)}\n{0.2 \\times\n\\left(\n\\frac{e^{\\frac{-(4-0)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n\\right)\n+\n0.8 \\times\n\\left(\n\\frac{e^{\\frac{-(4-10)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n\\right)\n} \\\\\np_{k}(4) & = 0.75\n\\end{split}\n\\]\n\nldm_k_prop(4,\n           k = 1,\n           sigma = c(6, 6),\n           pi_k = c(0.8, 0.2),\n           mu = c(10, 0)) |>\n  scales::percent(accuracy = 0.01)\n\n[1] \"75.19%\"\n\n\n8\n8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures.\n\n\nModel\nTraining Error Rate\nTest Error Rate\n\n\n\nLogistic regression\n20%\n30%\n\n\n1-nearestbors\n0%\n36%\n\n\n\nBased on these results, which method should we prefer to use for classification of new observations? Why?\n\nWe should use the Logistic Regression as it has a lower test error rate than the logistic regression.\n9\n9. This problem has to do with odds.\nA\n(A) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\\[\n\\begin{split}\n\\frac{p(X)}{1 - p(X)} & = Y \\\\\np(X) & = Y - Yp(X) \\\\\np(X) + Yp(X) & = Y \\\\\np(X) & = \\frac{Y}{1+Y} = \\frac{0.37}{1.37} = 0.27\n\\end{split}\n\\]\nB\n(B) Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?\n\nscales::percent(0.16/(1-0.16), accuracy = 0.01) \n\n[1] \"19.05%\"\n\n\n10 (missing)\n10. Equation 4.32 derived an expression for \\(\\log{ \\left( \\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=K|X=x)} \\right)}\\) in the setting where \\(p > 1\\), so that the mean for the \\(k\\)th class, \\(\\mu_k\\), is p-dimensional vector, and the shared covariance \\(\\Sigma\\) is a \\(p \\times p\\) matrix. However, in the setting with \\(p=1\\), (4.32) takes a simpler form, since the means \\(\\mu_1,\\dots,\\mu_K\\) and the variance \\(\\sigma^2\\) are scalars In this simpler setting, repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(\\pi_k\\), \\(\\pi_k\\), \\(\\mu_k\\), \\(\\mu_K\\), and \\(\\sigma^2\\).\n\\[\n\\begin{split}\n\\log{ \\left( \\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=K|X=x)} \\right)}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                & =\n\\end{split}\n\\]\n11 (missing)\n11. Work out the detailed forms of \\(a_k\\), \\(b_{kj}\\) , and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(\\pi_k\\), \\(\\pi_k\\), \\(\\mu_k\\), \\(\\mu_K\\), \\(\\Sigma_k\\), and \\(\\Sigma_K\\).\n12\n12. Suppose that you wish to classify an observation X ∈ R into apples and oranges. You fit a logistic regression model and find that\n\\[\n\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x) =\n\\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1x)}\n{1 + \\exp(\\hat{\\beta}_0+\\hat{\\beta}_1x)}\n\\]\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\\[\n\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x) =\n\\frac{\\exp(\\hat{\\alpha}_{\\text{orange}0}+\n           \\hat{\\alpha}_{\\text{orange}1}x)}\n{\\exp(\\hat{\\alpha}_{\\text{orange}0}+\n      \\hat{\\alpha}_{\\text{orange}1}x)+\n\\exp(\\hat{\\alpha}_{\\text{apple}0}+\n      \\hat{\\alpha}_{\\text{apple}1}x)}\n\\]\nA\n(A) What is the log odds of orange versus apple in your model?\n\\[\n\\log{\n  \\left(\n    \\frac{\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x)}\n         {1 - \\widehat{\\text{Pr}}(Y = \\text{orange}|X=x)}\n  \\right)} =\n\\hat{\\beta}_0+\\hat{\\beta}_1x\n\\]\nB\n(B) What is the log odds of orange versus apple in your friend’s model?\n\\[\n\\log{\n  \\left(\n    \\frac{\\exp(\\hat{\\alpha}_{\\text{orange}0} + \\hat{\\alpha}_{\\text{orange}1}x)}\n         {\\exp(\\hat{\\alpha}_{\\text{apple}0} + \\hat{\\alpha}_{\\text{apple}1}x)}\n  \\right)} =\n(\\hat{\\alpha}_{\\text{orange}0} - \\hat{\\alpha}_{\\text{apple}0}) +\n(\\hat{\\alpha}_{\\text{orange}1} - \\hat{\\alpha}_{\\text{apple}1}) x\n\\]\nC\n(C) Suppose that in your model, \\(\\hat{\\beta}_0 = 2\\) and \\(\\hat{\\beta}_1 = -1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\n\n\\(\\hat{\\beta}_0 = 2\\) and \\(\\hat{\\beta}_1 = -1\\)\n\nD\n(D) Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat{\\alpha}_{\\text{orange}0} = 1.2\\), \\(\\hat{\\alpha}_{\\text{orange}1} = −2\\), \\(\\hat{\\alpha}_{\\text{apple}0} = 3\\), \\(\\hat{\\alpha}_{\\text{apple}1} = 0.6\\). What are the coefficient estimates in your model?\n\n\n\\(\\hat{\\beta}_0 = 1.8\\) and \\(\\hat{\\beta}_1 = -2.6\\)\n\nE\n(E) Finally, suppose you apply both models from (D) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\n\nBoth models will predict the same class for every test observation."
  },
  {
    "objectID": "04-execises.html#applied",
    "href": "04-execises.html#applied",
    "title": "04 - Classification",
    "section": "Applied",
    "text": "Applied\n13\n13. This question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\nA\n(A) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n\nWeeklyDT <- \n  as.data.table(ISLR2::Weekly\n  # We want to measure the prob of going Up rather than Down\n  )[, Direction := factor(Direction, levels = c(\"Up\",\"Down\"))\n  ][,`:=`(n_weeks = .N,\n          year_week = seq_len(.N)),\n    by = \"Year\"\n  # As 1990 has less week that the rest of year we assume that\n  # we are missing some weeks the beginning of the year rather than \n  # the end if the year\n  ][n_weeks < 52L, \n    year_week := year_week + (52L - n_weeks)]\n\n\nggplot(WeeklyDT,aes(Year, Volume, color = Direction))+\n  geom_point(alpha = 0.3)+\n  geom_smooth(se = FALSE)+\n  scale_y_log10()+\n  labs(title = \"Year and Volume are highly correlated\",\n       subtitle = \"So we can not use both in the same model\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\nggplot(WeeklyDT,aes(year_week, Volume, color = Direction))+\n  geom_point(alpha = 0.3)+\n  geom_smooth(se = FALSE)+\n  scale_y_log10()+\n  scale_x_continuous(breaks = scales::breaks_width(10))+\n  labs(title = \"The volume was higher for Down Direction\",\n       subtitle = \"In the first 35 weeks\",\n       y = \"Log10(Volume)\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\nmelt(WeeklyDT,\n     measure.vars = patterns(\"^Lag\\\\d$\"),\n     variable.name = \"Lag_name\",\n     value.name = \"Lag_value\") |>\n  ggplot(aes(Volume, Lag_value, color = Direction))+\n  geom_point(alpha = 0.3)+\n  geom_smooth(se = FALSE)+\n  scale_x_log10()+\n  facet_wrap(vars(Lag_name), scales = \"free_y\")+\n  labs(title = \"There the lags can help to differenciate between Up and Down\",\n       x = \"Log10(Volume)\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\n\nB\n(B) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\nLogisticLagModelFit <-\n  logistic_reg() |>\n  fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = WeeklyDT)\n\nLogisticLagTrainPred <-\n  cbind(WeeklyDT, augment(LogisticLagModelFit, new_data = NULL))\n\nsummary(LogisticLagModelFit$fit)\n\n\nCall:\nstats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n    Lag5 + Volume, family = stats::binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4579  -1.0849  -0.9913   1.2565   1.6949  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept) -0.26686    0.08593  -3.106   0.0019 **\nLag1         0.04127    0.02641   1.563   0.1181   \nLag2        -0.05844    0.02686  -2.175   0.0296 * \nLag3         0.01606    0.02666   0.602   0.5469   \nLag4         0.02779    0.02646   1.050   0.2937   \nLag5         0.01447    0.02638   0.549   0.5833   \nVolume       0.02274    0.03690   0.616   0.5377   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1496.2  on 1088  degrees of freedom\nResidual deviance: 1486.4  on 1082  degrees of freedom\nAIC: 1500.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nLag2 it’s significant.\nC\n(C) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n\nconf_mat(LogisticLagTrainPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction  Up Down\n      Up   557  430\n      Down  48   54\n\nget_class_metrics(LogisticLagTrainPred,\n                  truth = Direction,\n                  estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.921\n2 spec    binary         0.112\n\n\n\nBy checking the specificity rate we can see that the model is making a bad job identifying when the number is going to get Down.\nD\n(D) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n\nWeeklyYearSplit <-\n  WeeklyDT[, list(analysis = .I[Year <= 2008],\n                  assessment = .I[Year > 2008]) |>\n             make_splits(data = .SD)]\n\nWeeklyYearTraining <- training(WeeklyYearSplit)\n\nWeeklyYearLogisticResults <-\n  logistic_reg() |>\n  get_class_matrix_metrics(split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearLogisticResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   56   34\n      Down  5    9\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.918\n2 spec    binary         0.209\n\n\nE\n(E) Repeat (d) using LDA.\n\nWeeklyYearLdaResults <-\n  discrim_linear() |>\n  get_class_matrix_metrics(split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearLdaResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   56   34\n      Down  5    9\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.918\n2 spec    binary         0.209\n\n\nF\n(F) Repeat (d) using QDA.\n\nWeeklyYearQdaResults <-\n  discrim_quad() |>\n  get_class_matrix_metrics(split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearQdaResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   61   43\n      Down  0    0\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary             1\n2 spec    binary             0\n\n\nG\n(G) Repeat (d) using KNN with K = 1.\n\nWeeklyYearKnnResults <-\n  get_class_matrix_metrics(model_knn_k(1),\n                           split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearKnnResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   30   21\n      Down 31   22\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.492\n2 spec    binary         0.512\n\n\nH\n(H) Repeat (d) using naive Bayes.\n\nWeeklyYearNbResults <-\n  get_class_matrix_metrics(ModelNaiveBayes,\n                           split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearNbResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   61   43\n      Down  0    0\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary             1\n2 spec    binary             0\n\n\nI\n(I) Which of these methods appears to provide the best results on this data?\n\nlist(cbind(WeeklyYearLogisticResults$metrics, model = \"logistic\"),\n     cbind(WeeklyYearLdaResults$metrics, model = \"lda\"),\n     cbind(WeeklyYearQdaResults$metrics, model = \"qda\"),\n     cbind(WeeklyYearNbResults$metrics, model = \"nb\")) |>\n  rbindlist() |>\n  (\\(DT) DT[.metric == \"spec\"][order(-.estimate)])()\n\n   .metric .estimator .estimate    model\n1:    spec     binary 0.2093023 logistic\n2:    spec     binary 0.2093023      lda\n3:    spec     binary 0.0000000      qda\n4:    spec     binary 0.0000000       nb\n\n\n\nThe best models are Logistic Regression and the Linear Discriminant Analysis.\nJ\n(J) Experiment with diﬀerent combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classiﬁer.\n\nLet’s define different recipes to evaluate\n\n\nWeeklyRecipeBaseFormula <-\n  recipe(Direction ~  Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume + year_week,\n         data = WeeklyYearTraining)\n\nWeeklyRecipeLogVolume <-\n  WeeklyRecipeBaseFormula |>\n  step_log(Volume, base = 10)\n\nWeeklyRecipeInteractions <-\n  WeeklyRecipeLogVolume |>\n  step_interact(~ Volume:year_week + Volume:starts_with(\"Lag\")) \n\n\nWeeklyRecipeList <-\n  list(\"BaseFormula\" = WeeklyRecipeBaseFormula,\n       \"LogVolume\" = WeeklyRecipeLogVolume,\n       \"Interactions\" = WeeklyRecipeInteractions)\n\n\nThe models to fit\n\n\nWeeklyModelList <-\n  c(1L, seq.int(10L, 100L, 10L)) |>\n  (\\(x) structure(x, names = paste(\"knn\",x)) )() |>\n  lapply(model_knn_k) |>\n  append(list(\"logistic\" = logistic_reg(),\n              \"lda\" = discrim_linear(),\n              \"qda\" = discrim_quad(),\n              bayes = ModelNaiveBayes)) \n\n\nPerforming the evaluation\n\n\nWeeklyEvaluationResults <-\n  lapply(WeeklyModelList,\n         FUN = evaluate_model,\n         recipe_list = WeeklyRecipeList,\n         split = WeeklyYearSplit) |>\n  rbind_list_name(new_col_name = \"model\", fill = TRUE)\n  \n \nWeeklyEvaluationResults[, dcast(.SD, recipe+model ~.metric,\n                                value.var = \".estimate\")\n][order(-roc_auc)\n][1:5]\n\n\n\n         recipe    model  accuracy   roc_auc\n1: Interactions logistic 0.5961538 0.6164697\n2: Interactions      lda 0.6057692 0.6157072\n3:    LogVolume   knn 30 0.5865385 0.6023637\n4:    LogVolume   knn 40 0.5769231 0.5981700\n5:    LogVolume   knn 50 0.5769231 0.5932139\n\n\n\nIn this case the Logistic Regressions has the best general results after adding interactions to the model.\n\n\nWeeklyFittedWorkFlow <-\n  workflow() |>\n  add_recipe(WeeklyRecipeInteractions) |>\n  add_model(logistic_reg()) |>\n  fit(data = WeeklyYearTraining) \n\nWeeklyFittedWorkFlow |>\n  augment(new_data = testing(WeeklyYearSplit)) |>\n  get_class_metrics(truth = Direction,\n                    estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.639\n2 spec    binary         0.535\n\n\n\nThe model can predict if the Direction would go “Up” 64 of 100 times correctly and predict “Down” correctly 54 of 100 times, that is little bit better than guessing.\n\n\nWeeklyFittedWorkFlow$fit$fit$fit |>\n  tidy() |>\n  as.data.table() |>\n  (\\(DT) DT[order(-abs(estimate))])() |>\n  head(5) |>\n  ggplot(aes(estimate, reorder(term,estimate)))+\n  geom_col()+\n  theme_classic()\n\n\n\n\n14\n14. In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\nA\n(A) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may ﬁnd it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\n\nAutoDT <- \n  as.data.table(ISLR2::Auto\n  )[,`:=`(mpg01 = factor(mpg > median(mpg), levels = c(\"TRUE\",\"FALSE\")),\n          origin = c(\"American\",\"European\",\"Japanese\")[origin],\n          cylinders = factor(cylinders))]\n\nB\n(B) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your ﬁndings.\n\nExploring categorical variables\n\n\nAutoDT[, .(prob = mean(mpg01 == \"TRUE\")),  \n       by = \"cylinders\"] |>\n  ggplot(aes(cylinders, prob)) +\n  geom_col(aes(fill = prob > 0.6), alpha = 0.8) +\n  labs(title = \"Cars with 4 and 5 cyclinders are more efficient\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nAutoDT[, .(prob = mean(mpg01 == \"TRUE\")),\n       by = \"origin\"] |>\n  ggplot(aes(origin, prob)) +\n  geom_col(aes(fill = prob < 0.4), alpha = 0.8) +\n  labs(title = \"American Cars are less efficient\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"firebrick3\", \"FALSE\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nExploring numerical variables\n\n\nAutoDT[, .(prob = mean(mpg01 == \"TRUE\")),\n       by = \"year\"] |>\n  ggplot(aes(year, prob)) +\n  geom_point(color = \"#AAAAAA\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"dodgerblue3\")+\n  labs(title = \"New cars are more efficient on average\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"firebrick3\", \"FALSE\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(mpg01, displacement))+\n  geom_boxplot(aes(fill = mpg01), alpha = 0.6)+\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The Engine displacement is lower for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(mpg01, horsepower))+\n  geom_boxplot(aes(fill = mpg01), alpha = 0.6)+\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The Engine horsepower is lower for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(mpg01, weight))+\n  geom_boxplot(aes(fill = mpg01), alpha = 0.6)+\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The weight is lower for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(acceleration, fill = factor(mpg01,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, position = \"identity\", bins = 15)+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The acceleration is a little bit higher for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\nC\n(C) Split the data into a training set and a test set.\n\nset.seed(1224)\n\nAutoSplit <- initial_split(AutoDT, strata = mpg01)\n\nAutoTraining <- training(AutoSplit)\n\nD\n(D) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nAutoRecipe <- \n  recipe(mpg01 ~ cylinders + origin + year + displacement + horsepower + weight + acceleration,\n         data = AutoTraining) |>\n  step_dummy(cylinders, origin) |>\n  # I couldn't perform QDA with a higher threshold\n  step_corr(all_numeric_predictors(), threshold = 0.59)\n\nworkflow() |>\n  add_recipe(AutoRecipe) |>\n  add_model(discrim_linear()) |>\n  last_fit(split = AutoSplit) |>\n  collect_predictions() |>\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1122449\n\n\nE\n(E) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nworkflow() |>\n  add_recipe(AutoRecipe) |>\n  add_model(discrim_quad()) |>\n  last_fit(split = AutoSplit) |>\n  collect_predictions() |>\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1428571\n\n\nF\n(F) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nworkflow() |>\n  add_recipe(AutoRecipe) |>\n  add_model(logistic_reg()) |>\n  last_fit(split = AutoSplit) |>\n  collect_predictions() |>\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1122449\n\n\nG\n(G) Perform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nworkflow() |>\n  add_recipe(AutoRecipe) |>\n  add_model(ModelNaiveBayes) |>\n  last_fit(split = AutoSplit) |>\n  collect_predictions() |>\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1020408\n\n\nH\n(H) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?\n\nAutoKnnTestError <-\n  lapply(1:15, function(k){\n    \n    workflow() |>\n      add_recipe(AutoRecipe) |>\n      add_model(model_knn_k(k)) |>\n      last_fit(split = AutoSplit) |>\n      collect_predictions() |>\n      summarise(k = k,\n                test_error = mean(.pred_class != mpg01))\n    \n  }) |>\n  rbindlist() |>\n  arrange(test_error)\n\nggplot(AutoKnnTestError, aes(k, test_error))+\n  geom_line(color = \"#AAAAAA\")+\n  geom_point(aes(color = test_error == min(test_error)))+\n  expand_limits(y = 0)+\n  scale_color_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The lowest error starts with k = 12 and keep constant\")+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\n\n\nhead(AutoKnnTestError, 1L)\n\n    k test_error\n1: 12  0.1020408\n\n\n15\n15. This problem involves writing functions.\nA\n(A) Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute \\(2^3\\) and print out the results.\n\nPower <- \\() 2^3\n\nPower()\n\n[1] 8\n\n\nB\n(B) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a.\n\nPower2<-function(x, a) { print(x^a)}\n\nPower2(3,8)\n\n[1] 6561\n\n\nC\n(C) Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^{3}\\).\n\nPower2(c(10,8,131),c(3, 17, 3))\n\n[1] 1.000000e+03 2.251800e+15 2.248091e+06\n\n\nD\n(D) Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line:\n\nPower3 <- function(x, a) {\n  result <- x^a\n  return(result)\n}\n\nE\n(E) Now using the Power3() function, create a plot of \\(f(x) =x^2\\). The x-axis should display a range of integers from 1 to 10, and the y-axis should display \\(x^2\\). Label the axes appropriately, and use an appropriate title for the ﬁgure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log = \"x\", log = \"y\", or log = \"xy\" as arguments to the plot() function.\n\nggplot(data.frame(x = 1:10), aes(x))+\n  geom_function(fun  = \\(x) Power3(x,2), color= \"blue\")+\n  scale_y_log10()+\n  scale_x_continuous(breaks = breaks_width(1))+\n  labs(title = \"Plotting a function with ggplot2\", y = \"log10(x^2)\")+\n  theme_classic()\n\n\n\n\nF\n(F) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a ﬁxed a and for a range of values of x. For instance, if you call\n\nPlotPower(1:10,3)\n\nthen a plot should be created with an x-axis taking on values 1, 2,…,10, and a y-axis taking on values \\(1^3, 2^3, \\dots, 10^3\\).\n\nPlotPower <- function(x, a){\n  \n  ggplot(data.frame(x_values = x), aes(x_values))+\n  geom_function(fun  = \\(x) Power3(x, a), color= \"blue\")+\n  scale_y_log10()+\n  labs(title = \"Plotting a function with ggplot2\", \n       y =paste0(\"log10(x^\",a,\")\") )+\n  theme_classic()\n  \n}\n\nPlotPower(1:100,2)\n\n\n\n\n16\n16. Using the Boston data set, ﬁt classiﬁcation models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes, and KNN models using various subsets of the predictors. Describe your ﬁndings.\nHint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set.\n\nLet’s transform the data.\n\n\nBostonDT <- \n  as.data.table(Boston\n  )[,`:=`(high_crim = factor(crim > median(crim),levels = c(\"TRUE\",\"FALSE\")),\n          rad = factor(rad),\n          crim = NULL)]\n\nset.seed(2014)\n\nBostonSplit <- initial_split(BostonDT, strata = high_crim)\n\nBostonTraining <- training(BostonSplit)\n\nBostonRecipeAllVars <-\n  recipe(high_crim ~ . , data = BostonTraining) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_corr(all_numeric_predictors(), threshold = 0.8)\n\nBostonTrainingPrep <-\n  prep(BostonRecipeAllVars, training = BostonTraining) |>\n  bake(new_data = NULL) |>\n  as.data.table()\n\n\nVariable with differences\n\n\nggplot(BostonTrainingPrep, aes(zn, fill =  factor(high_crim,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, bins = 8, position = \"identity\")+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The crime is higher in zones with less residential land\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim, indus))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher in zones with\\na greater proportion of non-retail stores\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim, nox))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher in zones with higher concentration of nitrogen oxides\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  age))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher in old dwelling\",\n       y = \"Proportion of dwelling built prior to 1940\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  dis))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher as get closer to the employment centres\",\n       y = \"Proportion of dwelling built prior to 1940\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nBostonTraining[, .(prob = mean(high_crim == \"TRUE\"),\n                   x = sum(high_crim == \"TRUE\"),\n                   n = .N),\n               by = \"rad\"\n  # Using Jeffeys CI base on Beta distribution\n  ][, `:=`(lower = qbeta(0.025, 0.5+x, 0.5 +n-x),\n           higher = qbeta(0.975, 0.5+x, 0.5 +n-x),\n           rad = reorder(rad, prob))] |>\n  ggplot(aes(rad, prob, group = \"1\")) +\n  geom_ribbon(aes(ymin = lower, ymax = higher),\n                 fill = \"gray90\")+\n  geom_line(color = \"dodgerblue3\") +\n  geom_point(size = 2, color = \"gray40\")+\n  labs(title = \"The crime has a positive correlation with the index of accessibility\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  lstat))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher as lower status increases\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  medv))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher as the dwelling is cheaper\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nVariable without differences\n\n\nBostonTrainingPrep[, .(prob = mean(high_crim == \"TRUE\"),\n                       x = sum(high_crim == \"TRUE\"),\n                       n = .N),\n                   by = .(chas = factor(chas))\n  # Using Jeffeys CI base on Beta distribution\n  ][, `:=`(lower = qbeta(0.025, 0.5+x, 0.5 +n-x),\n           higher = qbeta(0.975, 0.5+x, 0.5 +n-x))] |>\n  ggplot(aes(prob, chas)) +\n  geom_errorbarh(aes(xmin = lower, xmax = higher),\n                 color = \"#AAAAAA\",\n                 height = .3)+\n  geom_point(aes(color = chas), size = 3)+\n  labs(title = \"We don't have enough evidence to confirm a significan\\ndifference between tracts with or without river\") +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  scale_color_manual(values = c(\"1\" = \"dodgerblue3\", \"0\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim, rm))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime level is the same no matter the number of rooms\",\n       y = \"Number of Rooms\",\n       x = \"Crime over the median\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(ptratio, fill =  factor(high_crim,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, bins = 6, position = \"identity\")+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The crime level is the same no matter the pupil-teacher ratio\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(black, fill =  factor(high_crim,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, bins = 12, position = \"identity\")+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The crime level is the same the proportion of blacks by town\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nLet’s define different recipes to evaluate\n\n\nBostonRecipeSignificant <-\n  recipe(high_crim ~ zn + indus + nox + age + dis + rad + lstat + medv,\n         data = BostonTraining) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_corr(all_numeric_predictors(), threshold = 0.8)\n\n\nBostonRecipeList <-\n  list(\"AllVars\" = BostonRecipeAllVars,\n       \"Significant\" = BostonRecipeSignificant)\n\n\nThe models to fit\n\n\nBostonModelList <-\n  c(1L, seq.int(10L, 100L, 10L)) |>\n  (\\(x) structure(x, names = paste(\"knn\",x)) )() |>\n  lapply(model_knn_k) |>\n  # We couldn't fit data with QDA or a Naive Bayes as the variables\n  # has the variable has zero variance in some cases\n  append(list(\"logistic\" = logistic_reg(),\n              \"lda\" = discrim_linear())) \n\n\nPerforming the evaluation\n\n\nBostonEvaluationResults <-\n  lapply(BostonModelList,\n         FUN = evaluate_model,\n         recipe_list = BostonRecipeList,\n         split = BostonSplit) |>\n  rbind_list_name(new_col_name = \"model\", fill = TRUE)\n  \n \nBostonEvaluationResults[, dcast(.SD,recipe+model ~.metric,\n                                value.var = \".estimate\")\n][order(-roc_auc)\n][1:5]\n\n\n\n        recipe    model  accuracy   roc_auc\n1:     AllVars   knn 10 0.9375000 0.9655762\n2:     AllVars logistic 0.9375000 0.9641113\n3: Significant logistic 0.9296875 0.9635010\n4:     AllVars   knn 40 0.9375000 0.9619141\n5: Significant   knn 20 0.9296875 0.9619141\n\n\n\nAs we can see the the Logistic Regression and the KNN can predict really well this variable no matter if we take all the predictor or the ones highlighted during the exploration process. In that case would use the Logistic Regression with the Significant recipe.\n\n\nBostonFittedWorkFlow <-\n  workflow() |>\n  add_recipe(BostonRecipeSignificant) |>\n  add_model(logistic_reg()) |>\n  fit(data = BostonTraining) \n\nBostonFittedWorkFlow$fit$fit$fit |>\n  tidy() |>\n  filter(term != \"(Intercept)\") |>\n  ggplot(aes(abs(estimate), reorder(term,abs(estimate))))+\n  geom_col(aes(fill = estimate >= 0), alpha = 0.8)+\n  scale_fill_manual(values = c(\"FALSE\" = \"firebrick3\", \n                               \"TRUE\" = \"dodgerblue3\"))+\n  scale_x_log10(labels = comma_format(accuracy = 0.01))+\n  theme_classic()+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "05-execises.html",
    "href": "05-execises.html",
    "title": "05 - Resampling Methods",
    "section": "",
    "text": "library(scales)\nlibrary(tidymodels)\nlibrary(data.table)"
  },
  {
    "objectID": "05-execises.html#conceptual",
    "href": "05-execises.html#conceptual",
    "title": "05 - Resampling Methods",
    "section": "Conceptual",
    "text": "Conceptual\n1\n1. Using basic statistical properties of the variance, as well as single-variable calculus, derive (5.6). In other words, prove that \\(\\alpha\\) given by (5.6) does indeed minimize \\(\\text{Var}(\\alpha X + (1-\\alpha)Y)\\).\nBy taking as a reference the Propagation section of the Variance Wikipedia post.\n\\[\n\\begin{split}\n\\text{Var}(\\alpha X + (1-\\alpha)Y) & =  \n        \\alpha^2 \\text{Var}(X)+\n        (1-\\alpha)^2  \\text{Var}(Y) +\n        2 \\alpha (1-\\alpha)   \\text{Cov}(X,Y) \\\\\n    & = \\alpha^2      \\text{Var}(X)+\n        (1 - 2 \\alpha + \\alpha^2) \\text{Var}(Y) +\n         (2 \\alpha-2\\alpha^2)   \\text{Cov}(X,Y) \\\\\n    & = \\alpha^2      \\text{Var}(X)+\n        \\text{Var}(Y) - 2 \\alpha \\text{Var}(Y) + \\alpha^2 \\text{Var}(Y)+\n         2 \\alpha \\text{Cov}(X,Y) - 2\\alpha^2 \\text{Cov}(X,Y)    \\\\   \n    & = [\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)] \\alpha^2 +\n        2[ \\text{Cov}(X,Y) - \\text{Var}(Y)] \\alpha + \\text{Var}(Y)\n\\end{split}\n\\]\nOnce we have the function, we can derivative using the derivative of power and solve the equation.\n\\[\n\\begin{split}\n2[\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)] \\alpha + 2[ \\text{Cov}(X,Y) - \\text{Var}(Y)] & =  0 \\\\\n2[\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)] \\alpha & = 2[\\text{Var}(Y) - \\text{Cov}(X,Y)] \\\\\n\\alpha  = \\frac{\\text{Var}(Y) - \\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)}\n\\end{split}\n\\]\n2\n2. We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.\nA\n(A) What is the probability that the ﬁrst bootstrap observation is not the jth observation from the original sample? Justify your answer.\nThe probability of an observation to be in any position of the bootstrap sample is \\(1/n\\) and the opposite \\(1 - 1/n\\).\nB\n(B) What is the probability that the second bootstrap observation is not the jth observation from the original sample?\nThe probability it’s the same (\\(1 - 1/n\\)) as we are sampling with replacement.\nC\n(C) Argue that the probability that the jth observation is not in the bootstrap sample is \\((1 - 1/n)^n\\).\nAs the probability of the jth observation for avoiding each position in bootstrap sample is \\(1 - 1/n\\) to get the probability in that situation we should use \\((1 - 1/n)^n\\) as the probabilities are independent.\nD\n(D) When \\(n = 5\\), what is the probability that the jth observation is in the bootstrap sample?\nAs \\((1 - 1/5)^5\\) represent the probability that an observation won’t appear.\n\\[\n1 - (1 - 1/5)^5 = 0.6723\n\\]\nE\n(E) When \\(n = 100\\), what is the probability that the jth observation is in the bootstrap sample?\n\\[\n1 - (1 - 1/100)^{100} = 0.6340\n\\]\nF\n(F) When \\(n = 10,000\\), what is the probability that the jth observation is in the bootstrap sample?\n\\[\n1 - (1 - 1/10000)^{10000} = 0.6321\n\\]\nG\n(G) Create a plot that displays, for each integer value of n from \\(1\\) to \\(100,000\\), the probability that the jth observation is in the bootstrap sample. Comment on what you observe.\n\nggplot(data.frame(x = 1:1e4))+\n  geom_function(aes(x), fun = \\(x) 1 - (1 - 1/x)^x, \n                color = \"blue\", linewidth = 0.8)+\n  geom_hline(yintercept = 1 - 1/exp(1), linetype = 2,\n             color = as.character(round(1 - 1/exp(1), 4)))+\n  expand_limits(y = 0)+\n  labs(x = \"Number of observations\",\n       y = \"Probability\",\n       title = \"Probability an observation is in the bootstrap sample\")+\n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     breaks = breaks_width(0.1))+\n  scale_x_continuous(labels = comma_format(accuracy = 1))+\n  theme_light()+\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank(),\n        plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\nH\n(H) We will now investigate numerically the probability that a bootstrap sample of size \\(n = 100\\) contains the jth observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n\nset.seed(2018)\n\nvapply(1:1e4, \n       FUN = \\(x) 4L %in% sample.int(100L, 100L, replace = TRUE),\n       FUN.VALUE = TRUE) |>\n  mean()\n\n[1] 0.6329\n\n\nAs we can see the probability es really close to \\(0.6340\\).\n3\n3. We now review k-fold cross-validation.\nA\n(A) Explain how k-fold cross-validation is implemented.\nIt involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The ﬁrst fold is treated as a validation set, and the method is ﬁt on the remaining \\(k - 1\\) folds.\nB\n(B) What are the advantages and disadvantages of k-fold cross validation relative to the validation set approach and the LOOCV?\n\n\nMain Characteristics\nValidation Set Approach\nLOOCV\n\n\n\nAccuracy in estimating the testing error\nLower\nLower\n\n\nTime efficiency\nHigher\nLower\n\n\nProportion of data used to train the models (bias mitigation)\nLower\nHigher\n\n\nEstimation variance\n-\nHigher\n\n\n4\n4. Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\nI would use to bootstrap method to re-sample the original data set many times, fit a statistical learning method on each re-sample, predict the value based on the predictors we want to study and calculate the standard deviation of the response, which is a good approximation of the standard error as we can on page 210."
  },
  {
    "objectID": "05-execises.html#applied",
    "href": "05-execises.html#applied",
    "title": "05 - Resampling Methods",
    "section": "Applied",
    "text": "Applied\n5\n5. In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n\nDefaultFormulaToFit <- as.formula(\"default ~ balance + income\")\n\nA\n(A) Fit a logistic regression model that uses income and balance to predict default.\n\nDefaultFittedModel <-\n  logistic_reg() |>\n  fit(DefaultFormulaToFit, data = ISLR::Default)\n\nDefaultFittedModel\n\nparsnip model object\n\n\nCall:  stats::glm(formula = default ~ balance + income, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      balance       income  \n -1.154e+01    5.647e-03    2.081e-05  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n\nB\n(B) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n\nSplit the sample set into a training set and a validation set.\n\n\nset.seed(4)\n\nDefaultSplit <- initial_split(ISLR::Default, prop = 0.5, strata = default)\n\nDefaultSplit\n\n<Training/Testing/Total>\n<5000/5000/10000>\n\n\n\nFit a multiple logistic regression model using only the training observations.\n\n\nDefaultTrainingModel <-\n  logistic_reg() |>\n  fit(DefaultFormulaToFit, data = training(DefaultSplit))\n\nDefaultTrainingModel\n\nparsnip model object\n\n\nCall:  stats::glm(formula = default ~ balance + income, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      balance       income  \n -1.153e+01    5.598e-03    2.384e-05  \n\nDegrees of Freedom: 4999 Total (i.e. Null);  4997 Residual\nNull Deviance:      1484 \nResidual Deviance: 813.8    AIC: 819.8\n\n\n\nObtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\n\n\nDefaultTestPredictions <-\n  augment(DefaultTrainingModel, new_data = testing(DefaultSplit))\n\nDefaultTestPredictions\n\n# A tibble: 5,000 × 7\n   default student balance income .pred_class .pred_No .pred_Yes\n   <fct>   <fct>     <dbl>  <dbl> <fct>          <dbl>     <dbl>\n 1 No      No         529. 35704. No             1.00  0.000446 \n 2 No      Yes        920.  7492. No             0.998 0.00202  \n 3 No      Yes        809. 17600. No             0.999 0.00138  \n 4 No      No           0  29275. No             1.00  0.0000198\n 5 No      No         237. 28252. No             1.00  0.0000728\n 6 No      No         607. 44995. No             0.999 0.000859 \n 7 No      No           0  50265. No             1.00  0.0000326\n 8 No      No         486. 61566. No             0.999 0.000649 \n 9 No      No        1095. 26465. No             0.992 0.00844  \n10 No      No         954. 32458. No             0.996 0.00444  \n# ℹ 4,990 more rows\n\n\n\nCompute the validation set error, which is the fraction of the observations in the validation set that are misclassiﬁed.\n\n\nDefaultTestPredictions |>\n  summarize(`Test error rate` = mean(default != .pred_class))\n\n# A tibble: 1 × 1\n  `Test error rate`\n              <dbl>\n1            0.0242\n\n\nC\n(C) Repeat the process in (b) three times, using three diﬀerent splits of the observations into a training set and a validation set. Comment on the results obtained.\n\nDefaultBasedResults <-\n  lapply(8:10, \n         model_recipe = recipe(DefaultFormulaToFit, data = ISLR::Default),\n         \n         FUN = \\(seed, model_recipe){\n           set.seed(seed)\n           \n           split <- initial_split(ISLR::Default, prop = 0.5, strata = default)\n           \n           workflow() |>\n             add_model(logistic_reg()) |>\n             add_recipe(model_recipe) |>\n             last_fit(split = split) |>\n             collect_predictions() |>\n             summarize(seed_used = seed,\n                       `test_error_rate` = mean(.pred_class != default)) }) |>\n  rbindlist()\n\nDefaultBasedResults\n\n   seed_used test_error_rate\n1:         8          0.0262\n2:         9          0.0240\n3:        10          0.0282\n\n\nD\n(D) Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.\nAdding the student variable as a dummy one doesn’t make a big impact on the prediction accurency.\n\nDefaultDummyRecipe <- \n  recipe(default ~ ., data = ISLR::Default) |>\n  step_dummy(student)\n\n\nDefaultDummyResults <- \n  lapply(8:10, \n         model_recipe = DefaultDummyRecipe,\n         \n         FUN = \\(seed, model_recipe){\n           set.seed(seed)\n           \n           split <- initial_split(ISLR::Default, prop = 0.5, strata = default)\n           \n           workflow() |>\n             add_model(logistic_reg()) |>\n             add_recipe(model_recipe) |>\n             last_fit(split = split) |>\n             collect_predictions() |>\n             summarize(seed_used = seed,\n                       `test_error_rate_dummy` = mean(.pred_class != default)) }) |>\n  rbindlist()\n\n\nDefaultBasedResults[DefaultDummyResults, on = \"seed_used\"\n][, diff := comma(test_error_rate_dummy - test_error_rate, accuracy = 0.0001)][]\n\n   seed_used test_error_rate test_error_rate_dummy    diff\n1:         8          0.0262                0.0260 -0.0002\n2:         9          0.0240                0.0248  0.0008\n3:        10          0.0282                0.0278 -0.0004\n\n\n6\n6. We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coeﬃcients in two diﬀerent ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\nA\n(A) Using the summary() and glm() functions, determine the estimated standard errors for the coeﬃcients associated with income and balance in a multiple logistic regression model that uses both predictors.\n\nDefaultGlmSummary <- tidy(DefaultFittedModel)\n\nDefaultGlmSummary\n\n# A tibble: 3 × 5\n  term           estimate  std.error statistic   p.value\n  <chr>             <dbl>      <dbl>     <dbl>     <dbl>\n1 (Intercept) -11.5       0.435         -26.5  2.96e-155\n2 balance       0.00565   0.000227       24.8  3.64e-136\n3 income        0.0000208 0.00000499      4.17 2.99e-  5\n\n\nB\n(B) Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coeﬃcient estimates for income and balance in the multiple logistic regression model.\nTo create the function it’s optimal\nC\n(C) Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coeﬃcients for income and balance.\n\nset.seed(15)\n\nDefaultBootstrapsSe <- \n  as.data.table(bootstraps(ISLR::Default, times = 500)\n  )[, logistic_reg() |> \n      fit(DefaultFormulaToFit, \n          data = analysis(splits[[1L]])) |>\n      tidy(),\n    by = \"id\"\n  ][, .(SE = sd(estimate)),\n    by = \"term\"]\n\n\nDefaultBootstrapsSe\n\n          term           SE\n1: (Intercept) 4.230130e-01\n2:     balance 2.203893e-04\n3:      income 4.895823e-06\n\n\nD\n(D) Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\nAs you can see bellow the results are really close to each other.\n\nleft_join(DefaultBootstrapsSe,\n          DefaultGlmSummary[,c(\"term\",\"std.error\")],\n          by = \"term\") |>\n  mutate(diff = SE - std.error)\n\n          term           SE    std.error          diff\n1: (Intercept) 4.230130e-01 4.347564e-01 -1.174338e-02\n2:     balance 2.203893e-04 2.273731e-04 -6.983879e-06\n3:      income 4.895823e-06 4.985167e-06 -8.934457e-08\n\n\n7\n7. In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classiﬁcation problems, the LOOCV error is given in (5.4).\nA\n(A) Fit a logistic regression model that predicts Direction using Lag1 and Lag2.\n\nWeeklyModel <-\n  logistic_reg() |>\n  fit(Direction ~ Lag1 + Lag2, data = ISLR::Weekly)\n\nWeeklyModel\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Direction ~ Lag1 + Lag2, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)         Lag1         Lag2  \n    0.22122     -0.03872      0.06025  \n\nDegrees of Freedom: 1088 Total (i.e. Null);  1086 Residual\nNull Deviance:      1496 \nResidual Deviance: 1488     AIC: 1494\n\n\nB\n(B) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the ﬁrst observation.\n\nWeeklyModelNotFirst <-\n  logistic_reg() |>\n  fit(Direction ~ Lag1 + Lag2, data = ISLR::Weekly[-1L,])\n\nWeeklyModelNotFirst\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Direction ~ Lag1 + Lag2, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)         Lag1         Lag2  \n    0.22324     -0.03843      0.06085  \n\nDegrees of Freedom: 1087 Total (i.e. Null);  1085 Residual\nNull Deviance:      1495 \nResidual Deviance: 1487     AIC: 1493\n\n\nC\n(C) Use the model from (b) to predict the direction of the ﬁrst observation. You can do this by predicting that the ﬁrst observation will go up if P(Direction = \"Up\" | Lag1, Lag2) > 0.5. Was this observation correctly classiﬁed?\nNo, it wasn’t.\n\nWeeklyModelNotFirst |>\n  augment(new_data = ISLR::Weekly[1L,] )\n\n# A tibble: 1 × 12\n   Year  Lag1  Lag2  Lag3   Lag4  Lag5 Volume Today Direction .pred_class\n  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl> <fct>     <fct>      \n1  1990 0.816  1.57 -3.94 -0.229 -3.48  0.155 -0.27 Down      Up         \n# ℹ 2 more variables: .pred_Down <dbl>, .pred_Up <dbl>\n\n\nD\n(D) Write a for loop from \\(i = 1\\) to \\(i = n\\), where n is the number of observations in the data set, that performs each of the following steps:\n\nFit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.\nCompute the posterior probability of the market moving up for the ith observation.\nUse the posterior probability for the ith observation in order to predict whether or not the market moves up.\nDetermine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.\n\n\nWeeklyLoocv <- loo_cv(ISLR::Weekly)\n\nsetDT(WeeklyLoocv)\n\nWeeklyLoocvPredictions <-\n  WeeklyLoocv[, training(splits[[1L]]), by = \"id\"\n  ][, .(model = .(logistic_reg() |>\n                    fit(Direction ~ Lag1 + Lag2, data = .SD))),\n    by = \"id\"\n  ][WeeklyLoocv[, testing(splits[[1L]]), by = \"id\"],\n    on = \"id\"\n  ][, .pred_class := predict(model[[1L]], new_data = .SD, type = \"class\"),\n    by = \"id\"\n  ][, is_error := Direction != .pred_class]\n\nE\n(E) Take the average of the n numbers obtained in (4d) in order to obtain the LOOCV estimate for the test error. Comment on the results.\n\nmean(WeeklyLoocvPredictions$is_error)\n\n[1] 0.4499541\n\n\n8\nA\n(A) Generate a simulated data set as follows. In this data set, what is n and what is p? Write out the model used to generate the data in equation form.\n\nset.seed(1)\nx <- rnorm(100)\ny <- x- 2*x^2 + rnorm(100)\n\nSimulatedDt <- data.table(x, y)\n\nn: 100 and p: 1.\nB\n(B) Create a scatterplot of X against Y. Comment on what you ﬁnd.\nThe values follows a function of second degree.\n\nggplot(SimulatedDt, aes(x,  y))+\n  geom_point()+\n  geom_smooth(se = FALSE)+\n  theme_light()\n\n\n\n\nC\n(C) Set a random seed, and then compute the LOOCV errors that result from ﬁtting the following four models using least squares. Note you may ﬁnd it helpful to use the data.frame() function to create a single data set containing both X and Y .\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\epsilon\\)\n\nwe don’t need to set a seed when performing LOOCV, as we don’t do anything at random.\n\ncollect_loo_testing_error <- function(formula,\n                                      loo_split,\n                                      metric_function = rmse,\n                                      ...){\n  # Validations\n  stopifnot(\"There is no espace between y and ~\" = formula %like% \"[A-Za-z]+ \")\n  stopifnot(\"loo_split must be a data.table object\" = is.data.table(loo_split))\n  \n  predictor <- sub(pattern = \" .+\", replacement = \"\", formula)\n  formula_to_fit <- as.formula(formula)\n  \n  Results <-\n    loo_split[, training(splits[[1L]]), by = \"id\"\n    ][, .(model = .(lm(formula_to_fit, data = .SD))),\n      by = \"id\"\n    ][loo_split[, testing(splits[[1L]]), by = \"id\"],\n      on = \"id\"\n    ][, .pred := predict(model[[1L]], newdata = .SD),\n      by = \"id\"\n    ][,  metric_function(.SD, truth = !!predictor, estimate = .pred, ...) ]\n \n  setDT(Results)\n  \n  \n  if(formula %like% \"degree\"){\n    \n    degree <- gsub(pattern = \"[ A-Za-z,=\\\\~()]\", replacement = \"\", formula)\n    \n    Results <- \n      Results[,.(degree = degree, \n                .metric, \n                .estimator, \n                .estimate)]\n    \n  }\n  \n  return(Results)\n    \n}\n\n\n# Creating the rplit object\nSimulatedDtSplit <- loo_cv(SimulatedDt)\n\n# Transforming to data.table\nsetDT(SimulatedDtSplit)\n\npaste0(\"y ~ poly(x, degree=\", 1:4, \")\") |>\n  lapply(collect_loo_testing_error,\n         loo_split = SimulatedDtSplit) |>\n  rbindlist()\n\n   degree .metric .estimator .estimate\n1:      1    rmse   standard 2.6996595\n2:      2    rmse   standard 0.9682064\n3:      3    rmse   standard 0.9780705\n4:      4    rmse   standard 0.9766805\n\n\nD\n(D) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?\nThe results will be the same as LOOCV don’t perform any random process.\nE\n(E) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.\nThe model with the lowest test error is the model using as a base the second grade evacuation. It’s what we were expecting, as we know the true form of the original function it’s a second degree one and adding more flexibility to the model just over-fit it.\nF\n(F) Comment on the statistical signiﬁcance of the coeﬃcient estimates that results from ﬁtting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\nFor the first model the predictor it’s significant.\n\nlm(y ~ poly(x, degree= 1), data = SimulatedDt) |>\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 1), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5161 -0.6800  0.6812  1.5491  3.8183 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           -1.550      0.260  -5.961 3.95e-08 ***\npoly(x, degree = 1)    6.189      2.600   2.380   0.0192 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.6 on 98 degrees of freedom\nMultiple R-squared:  0.05465,   Adjusted R-squared:  0.045 \nF-statistic: 5.665 on 1 and 98 DF,  p-value: 0.01924\n\n\nFor the second model the predictors are very significant.\n\nlm(y ~ poly(x, degree= 2), data = SimulatedDt) |>\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 2), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9650 -0.6254 -0.1288  0.5803  2.2700 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           -1.5500     0.0958  -16.18  < 2e-16 ***\npoly(x, degree = 2)1   6.1888     0.9580    6.46 4.18e-09 ***\npoly(x, degree = 2)2 -23.9483     0.9580  -25.00  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.958 on 97 degrees of freedom\nMultiple R-squared:  0.873, Adjusted R-squared:  0.8704 \nF-statistic: 333.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nThe additional element of the function is not significant.\n\nlm(y ~ poly(x, degree= 3), data = SimulatedDt) |>\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 3), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9765 -0.6302 -0.1227  0.5545  2.2843 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           -1.55002    0.09626 -16.102  < 2e-16 ***\npoly(x, degree = 3)1   6.18883    0.96263   6.429 4.97e-09 ***\npoly(x, degree = 3)2 -23.94830    0.96263 -24.878  < 2e-16 ***\npoly(x, degree = 3)3   0.26411    0.96263   0.274    0.784    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9626 on 96 degrees of freedom\nMultiple R-squared:  0.8731,    Adjusted R-squared:  0.8691 \nF-statistic: 220.1 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nThe additional element of the function is not significant.\n\nlm(y ~ poly(x, degree= 4), data = SimulatedDt) |>\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 4), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0550 -0.6212 -0.1567  0.5952  2.2267 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           -1.55002    0.09591 -16.162  < 2e-16 ***\npoly(x, degree = 4)1   6.18883    0.95905   6.453 4.59e-09 ***\npoly(x, degree = 4)2 -23.94830    0.95905 -24.971  < 2e-16 ***\npoly(x, degree = 4)3   0.26411    0.95905   0.275    0.784    \npoly(x, degree = 4)4   1.25710    0.95905   1.311    0.193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9591 on 95 degrees of freedom\nMultiple R-squared:  0.8753,    Adjusted R-squared:  0.8701 \nF-statistic: 166.7 on 4 and 95 DF,  p-value: < 2.2e-16\n\n\n9\n9. We will now consider the Boston housing data set, from the ISLR2 library.\nA\n(A) Based on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat{\\mu}\\).\n\nBostonMedvMean <- mean(ISLR2::Boston$medv)\n\nBostonMedvMean\n\n[1] 22.53281\n\n\nB\n(B) Provide an estimate of the standard error of \\(\\hat{\\mu}\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n\nBostonMedvSeEstimation <- sd(ISLR2::Boston$medv)/sqrt(nrow(ISLR2::Boston))\n\nC\n(C) Now estimate the standard error of \\(\\hat{\\mu}\\). using the bootstrap. How does this compare to your answer from (b)?\nBoth estimations are really close.\n\n# Using the infer package as just need to estimate\n# a single number\n\nset.seed(123)\n\nBostonMedvBootstrap <-\n  ISLR2::Boston |>\n  specify(medv ~ NULL) |>\n  generate(reps = 5000, type = \"bootstrap\") |>\n  calculate(stat = \"mean\") \n\nBostonMedvBootstrap |>\n  summarize(Se_bootstrap = sd(stat)) |>\n  mutate(Se_estimation = BostonMedvSeEstimation,\n         diff = Se_bootstrap - Se_estimation)\n\n# A tibble: 1 × 3\n  Se_bootstrap Se_estimation    diff\n         <dbl>         <dbl>   <dbl>\n1        0.414         0.409 0.00481\n\n\nD\n(D) Based on your bootstrap estimate from (c), provide a 95 % conﬁdence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv). Hint: You can approximate a 95 % conﬁdence interval using the formula [\\(\\hat{\\mu} - 2\\text{SE}(\\hat{\\mu}), \\hat{\\mu} + 2\\text{SE}(\\hat{\\mu})\\) .\n\nget_ci(BostonMedvBootstrap, \n       point_estimate = BostonMedvMean,\n       level = 0.95,\n       type = \"se\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1     21.7     23.3\n\n\nE\n(E) Based on this data set, provide an estimate, \\(\\hat{\\mu}_{med}\\), for the median value of medv in the population.\n\nmedian(ISLR2::Boston$medv)\n\n[1] 21.2\n\n\nF\n(F) We now would like to estimate the standard error of \\(\\hat{\\mu}_{med}\\). Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your ﬁndings.\nThe intervals for the median seams to be a little bit lower than the ones for the average. It seems the distributions of medv is right skewed.\n\nset.seed(77)\n\nISLR2::Boston |>\n  specify(medv ~ NULL) |>\n  generate(reps = 5000, type = \"bootstrap\") |>\n  calculate(stat = \"median\") |>\n  get_ci(level = 0.95,\n         type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1     20.5     21.9\n\n\nRight skewed has been confirmed.\n\nggplot(ISLR2::Boston, aes(medv))+\n  geom_histogram(fill = \"dodgerblue3\", \n                 alpha = 0.9, bins = 15)+\n  theme_light()+\n  theme(panel.grid = element_blank())\n\n\n\n\nG\n(G) Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity \\(\\hat{\\mu}_{0.1}\\). (You can use the quantile() function.)\n\nquantile(ISLR2::Boston$medv, probs = 0.1)\n\n  10% \n12.75 \n\n\nH\n(H) Use the bootstrap to estimate the standard error of \\(\\hat{\\mu}_{0.1}\\). Comment on your ﬁndings.\nThe standard error is slightly larger relative to \\(\\hat{\\mu}_{0.1}\\), but it is still small.\n\nset.seed(77)\n\nISLR2::Boston |>\n  specify(medv ~ NULL) |>\n  generate(reps = 5000, type = \"bootstrap\") |>\n  group_by(replicate) |>\n  summarize(medv_tenth_percentile = quantile(medv, probs = 0.1)) |>\n  summarize(se_medv_tenth_percentile = sd(medv_tenth_percentile))\n\n# A tibble: 1 × 1\n  se_medv_tenth_percentile\n                     <dbl>\n1                    0.507"
  }
]