[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Statistical Learning (Non Official Solution Book)",
    "section": "",
    "text": "Preface\nThe purpose of creating this book is share the lessons learnt by reading and completing the exercises in the book.\nIt also could work as summary of the main point of the book."
  },
  {
    "objectID": "modeling-process.html#business-science-problem-framework-bspf",
    "href": "modeling-process.html#business-science-problem-framework-bspf",
    "title": "1  Strategy to implement machine learning solutions",
    "section": "1.1 Business Science Problem Framework (BSPF)",
    "text": "1.1 Business Science Problem Framework (BSPF)\n\n1.1.1 View Business as a Machine\nIn this part we need to make sure that we are selecting a repetitive and measurable problem or improvement opportunity.\n\nIsolate business unit\n\n\nDefine objetives\n\n\nDefine machine in terms of people and processes\n\n\nCollect outcomes in terms of feedback\n\n\nFeedback identiﬁes problems\n\n\n\n1.1.2 Understand the Drivers\n\nInvestigate if objectives are being met\n\n\nSynthesize outcomes\n\n\nHypothesize drivers\n\n\n\n1.1.3 Measure the Drivers\n\nCollect data\n\n\nDevelop KPIs\n\n\n\n1.1.4 Uncover Problems & Opportunities\n\nEvaluate performance vs KPIs\n\n\nHighlight potential problem areas\n\n\nReview process and consider what could be missed or needed to answer questions\n\n\n\n1.1.5 Encode Algorithms\n\nDevelop algorithms to predict and explain problem\n\n\nTie ﬁnancial value of individual decisions to optimize for proﬁt\n\n\nUse recommendation algorithms to improve decisions\n\n\n\n1.1.6 Measure Results\n\nCapture outcomes after decision making system is implemented\n\n\nSynthesize results in terms of good and bad outcomes identifying what was done and what happened\n\n\nVisualize outcomes over time to determine progress\n\n\n\n1.1.7 Report Financial Impact\n\nMeasure actual results\n\n\nTie to ﬁnancial beneﬁts\n\n\nReport ﬁnancial beneﬁt of algorithms to key stakeholders"
  },
  {
    "objectID": "modeling-process.html#modeling-process",
    "href": "modeling-process.html#modeling-process",
    "title": "1  Strategy to implement machine learning solutions",
    "section": "1.2 Modeling Process",
    "text": "1.2 Modeling Process\nAccording to Hands-on Machine Learning with R, we need to follow the next process to develop successful models:\n\n\n\nGeneral predictive machine learning process\n\n\n\nSplit the data by using simple random sampling for regression problems and stratified sampling for classification problems or if the response variable deviates strongly from normality in a regression problem. As result be will need to define 2 sets:\n\n\nTraining set: To develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose our final model. The proportion of data used in the set depends the amount of data that we have, if we have a lot data (n &gt;100K) we can 60%, but if we don’t have much we can use 80%.\nTest set: To estimate an unbiased assessment of the model’s performance, which we refer to as the generalization error.\n\n\nApply an intensive EDA to your data using graphics and unsupervised models in other to:\n\n\nRemove unnecessary variables\nHandle missing values\nRe-coding variable names and values\n\n\nEven though that Lasso and tree-based methods are resistant to non-informative predictors it better to remove them as with more features often becomes harder to interpret and is costly to compute. Based on the context we have 2 kind of variables to remove:\n\n\nZero variance variables: The feature only contains a single unique value, provides no useful information to a model.\nNear-zero variance variables: The fraction of unique values over the sample size is low (say \\(\\leq 10\\)%) and the ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say \\(\\geq 20\\)%). You can use step_nzv to apply it.\nHigh dimensional problem: It is an alternative approach to filter out non-informative features without manually removing them. It can be used to represent correlated variables with a smaller number of uncorrelated features (called principle components). Use step_pca or step_pls to reduce the dimension of our features.\n\n\nPerform imputation if required.\nSolve the problems related to numeric features based on the model as they can affect GLMs, regularized regression, KNN, support vector machines and neural networks.\n\n\nSkewed: It can affect GLMs and regularized models. The recommended way it’s to use sqrt,log or Box-Cox for positive or step_YeoJohnson if the variable has negative numbers.\n\nWide range in magnitudes: What are the largest and smallest values across all features and do they span several orders of magnitude?. It is often a good idea to standardize the features. Standardizing features includes centering and scaling so that numeric variables have zero mean and unit variance, which provides a common comparable unit of measure across all the variables.\n\n\nTransform categorical features as most models require that the predictors take numeric form. In this context, we can find:\n\n\nLevels with very few observations: To solve this we can collapse, or “lumping” these into a lesser number of categories by using the step_other and defining a threshold value to implement. Another alternative could be using a likelihood or effect encoding with the embed::step_lencode_glm function or the target encoding process which consists in replacing a categorical value with the mean (regression) or proportion (classification) of the target variable.\nModels don’t manage categories: The most common is referred to as dummy encoding or one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value and them removing the first level column. We can apply this transformation using step_dummy, after standardizing the numeric variables.\nTransform factors: If you have some ordered categorical features you can transform them using step_ordinalscore() or step_integer().\n\n\nSolve class imbalance problem by apply one of the next techniques:\n\n\nDown-sampling: If we have many observations, we can keep all samples in the rare class and randomly selecting an equal number of samples in the abundant class.\nUp-sampling: If we don’t have many observations, we can increasing the size of rarer samples by using repetition or bootstrapping.\nSMOTE: It’s a combination of over- and under-sampling is often successful and a common approach is known as Synthetic Minority Over-Sampling Technique. We can use this method with the function step_smote from the themis recipes extension package.\n\n\nUse the training set to train the model. In most of the cases you will need to define a function Y ~ X, but in some cases you will need variable with the predictors and another with the response or define the in argument with character names the variables to use as predictors and the variable to use a response.\nConfirm the performance of the model using resampling methods like k-fold cross validation which average k test errors, providing us with an approximation of the error we might expect on unseen data. Making sure to don’t fall into the data leakage by performing the feature engineering part in isolation of each resampling iteration.\nUse the estimated test error to perform hyperparameter tuning across a grid search like\n\n\nFull Cartesian grid search: He assesses every hyperparameter value manually defined.\nRandom grid searches: It explores randomly selected hyperparameter values from a range of possible values, early stopping which allows you to stop a grid search once reduction in the error stops."
  },
  {
    "objectID": "missing-values.html#visualizing-missing-values",
    "href": "missing-values.html#visualizing-missing-values",
    "title": "2  Dealing with missingness",
    "section": "2.1 Visualizing missing values",
    "text": "2.1 Visualizing missing values\nIt is important to understand the distribution of missing values in a data set in order to determine the best approach for preprocessing.\nvis_miss(AmesHousing::ames_raw, cluster = TRUE)"
  },
  {
    "objectID": "missing-values.html#imputation",
    "href": "missing-values.html#imputation",
    "title": "2  Dealing with missingness",
    "section": "2.2 Imputation",
    "text": "2.2 Imputation\nImputation is the process of replacing a missing value with a substituted, “best guess” value. Imputation should be one of the first feature engineering steps you take as it will affect any downstream preprocessing\n\n2.2.1 Estimated statistic\nAn elementary approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace NAs and add a column to indicate if a particular value was inputted.\names_recipe %&gt;%\n  step_medianimpute(Gr_Liv_Area)\n\n\n2.2.2 K-nearest neighbor\nK-nearest neighbor (KNN) imputes values by identifying observations with missing values, then identifying other observations that are most similar based on the other available features, and using the values from these nearest neighbor observations to impute missing values. KNN imputation is best used on small to moderate sized data sets as it becomes computationally burdensome with larger data sets.\names_recipe %&gt;%\n  step_knnimpute(all_predictors(), neighbors = 6)\n\n\n2.2.3 K-nearest neighbor\nSimilar to KNN imputation, observations with missing values are identified and the feature containing the missing value is treated as the target and predicted using bagged decision trees.\names_recipe %&gt;%\n  step_bagimpute(all_predictors())"
  },
  {
    "objectID": "model-performance.html#reducible-and-irreducible-error",
    "href": "model-performance.html#reducible-and-irreducible-error",
    "title": "\n3  Understanding model performance\n",
    "section": "\n3.1 Reducible and irreducible error",
    "text": "3.1 Reducible and irreducible error\nThe goal when we are analyzing data is to find a function that based on some Predictors and some random noise could explain the Response variable.\n\\[\nY = f(X) + \\epsilon\n\\]\n\\(\\epsilon\\) represent the random error and correspond to the irreducible error as it cannot be predicted using the Predictors in regression models. It would have a mean of 0 unless are missing some relevant Predictors.\nIn classification models, the irreducible error is represented by the Bayes Error Rate.\n\\[\n1 -  E\\left(\n     \\underset{j}{max}Pr(Y = j|X)\n     \\right)\n\\]\nAn error is reducible if we can improve the accuracy of \\(\\hat{f}\\) by using a most appropriate statistical learning technique to estimate \\(f\\).\nThe challenge to achieve that goal it’s that we don’t at the beginning how much of the error correspond to each type.\n\\[\n\\begin{split}\nE(Y-\\hat{Y})^2 & = E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\\n               & = \\underbrace{[f(X)- \\hat{f}(X)]^2}_\\text{Reducible} +\n                   \\underbrace{Var(\\epsilon)}_\\text{Irredicible}\n\\end{split}\n\\]\nThe reducible error can be also spitted in two parts:\n\nVariance refers to the amount by which \\(\\hat{f}\\) would change if we estimate it using a different training data set. If a method has high variance then small changes in the training data can result in large changes of \\(\\hat{f}\\).\nSquared bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model as for example a linear model.Bias is the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict.\n\n\\[\nE(y_{0} - \\hat{f}(x_{0}))^2 =\nVar(\\hat{f}(x_{0})) +\n[Bias(\\hat{f}(x_{0}))]^2 +\nVar(\\epsilon)\n\\]\n\n\n\n\n\n\nOur challenge lies in ﬁnding a method for which both the variance and the squared bias are low."
  },
  {
    "objectID": "model-performance.html#types-of-models",
    "href": "model-performance.html#types-of-models",
    "title": "\n3  Understanding model performance\n",
    "section": "\n3.2 Types of models",
    "text": "3.2 Types of models\n\n\nParametric methods\n\nMake an assumption about the functional form. For example, assuming linearity.\nEstimate a small number parameters based on training data.\nAre easy to interpret.\nTend to outperform non-parametric approaches when there is a small number of observations per predictor.\n\n\n\nNon-parametric methods\n\nDon’t make an assumption about the functional form, to accurately ﬁt a wider range of possible shapes for \\(f\\).\nNeed a large number of observations in order to obtain an accurate estimate for \\(f\\).\nThe data analyst must select a level of smoothness (degrees of freedom)."
  },
  {
    "objectID": "model-performance.html#evaluating-model-performance",
    "href": "model-performance.html#evaluating-model-performance",
    "title": "\n3  Understanding model performance\n",
    "section": "\n3.3 Evaluating model performance",
    "text": "3.3 Evaluating model performance\nTo evaluate how good works a models we need to split the available data in two parts.\n\n\nTraining data: Used to fit the model.\n\nTest data: Used to confirm how well the model works with new data.\n\n\n3.3.1 Regression models\n\n\nGround Truth vs. Predicts Plot: In this plot we represent the predictions on the x-axis and the outcome on the y-axis in a scatted plot with a line with slope = 1. If the model predicted perfectly, all the points would be along this line. If you see regions where the points are entirely above or below the line it demonstrates that the errors are correlated with the value of the outcome. Some possible problems could be:\n\nDon’t having all the important variables in your model\nYou need an algorithm that can find more complex relationships in the data\n\n\n\n\n\n\n\n\n\nResidual Plots: It plots the residuals (\\(y_i - \\hat{y}_i\\)) against the predictions (\\(\\hat{y}_i\\)).In a model with no systematic errors, the errors will be evenly distributed between positive and negative, but When there are systematic errors, there will be clusters of all positive or all negative residuals.\n\n\n\n\n\n\n\nGain curve plot: It is useful when sorting the instances is more important than predicting the exact outcome value like happens when predicting probabilities, where:\n\nThe x-axis shows the fraction of total houses as sorted by the model.\nThe y-axis shows the fraction of total accumulated outcome magnitude.\nThe diagonal line represents the gain curve if the houses are sorted randomly. T\nThe wizard curve represents what a perfect model would trace out.\nThe blue curve is what our model traces out.\nA relative gini coefficient close to one shows that the model correctly sorts high unemployment situations from lower ones.\n\n\n\n\nCode# Creating a data.frame\nset.seed(34903490)\ny = abs(rnorm(20)) + 0.1\nx = abs(y + 0.5*rnorm(20))\nfrm = data.frame(model=x, value=y)\n\n# Getting the predicted top 25% most valuable points\n# as sorted by the model\ngainx = 0.25  \n\n# Creating a function to calculate the label for the annotated point\nlabelfun = function (gx, gy) {\n  pctx = gx*100\n  pcty = gy*100\n\n  paste(\"The predicted top \", pctx, \"% most valuable points by the model\\n\",\n        \"are \", pcty, \"% of total actual value\", sep='')\n}\n\nWVPlots::GainCurvePlotWithNotation(\n  frm,\n  xvar = \"model\",\n  truthVar = \"value\",\n  title = \"Example Gain Curve with annotation\",\n  gainx = gainx,\n  labelfun = labelfun\n)\n\n\n\n\n\n\nTest Root Mean Squared Error (RMSE): It takes the square root of the MSE metric so that your error is in the same units as your response variable.Objective: minimize\n\n\nTo know if the RMSE of our model is high or low can compare it with the standard deviation of the outcome (sd(y)), then if the RMSE being smaller than the standard deviation that demonstrates that the model tends to estimate better than simply taking the average.\n\\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\]\n\n\nMean squared error (MSE): The squared component results in larger errors having larger penalties. Objective: minimize\n\n\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\n\n\n\n\n\n\\(R^2\\): This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). But if have too many limitations and You should not place too much emphasis on this metric. Objective: minimize\n\n\n$$\nR^2 = 1 - {_} $$\n\n\nRoot mean squared logarithmic error (RMSLE): When your response variable has a wide range of values, large response values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. Objective: minimize\n\n\n\\[\n\\text{RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=i}^n (\\log{(y_i + 1)} - \\log{(\\hat{y}_i + 1)})^2}\n\\]\n\n\nMean absolute error (MAE): Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values. Objective: minimize\n\n\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n (|y_i - \\hat{y}_i|)\n\\]\n\n\nDeviance: If the response variable distribution is Gaussian, then it will be approximately equal to MSE. When not, it usually gives a more useful estimate of error. It is often used with classification models and compares a saturated model (i.e. fully featured model) to an unsaturated model (i.e. intercept only or average) to provide the degree to which a model explains the variation in a set of data. Objective: minimize\n\n\n\n\n3.3.2 Classification models\n\n\nError (misclassification) rate: It represents the overall error. Objective: minimize\n\n\n\\[\nI(y_{0} \\neq \\hat{y}_{0}) =\n\\begin{cases}\n    1 & \\text{If } y_{0} \\neq \\hat{y}_{0} \\\\\n    0 & \\text{If } y_{0} = \\hat{y}_{0}\n\\end{cases}\n\\]\n\\[\n\\text{Ave}(I(y_{0} \\neq \\hat{y}_{0}))\n\\]\n\n\nMean per class error: This is the average error rate for each class. If your classes are balanced this will be identical to misclassification. Objective: minimize\n\n\n\\[\n\\begin{split}\n\\text{Ave}(& \\text{Ave}(I(y_{0} \\neq \\hat{y}_{0}))_1, \\\\\n           & \\text{Ave}(I(y_{0} \\neq \\hat{y}_{0}))_2, \\\\\n           & \\dots, \\\\\n           & \\text{Ave}(I(y_{0} \\neq \\hat{y}_{0}))_\\text{n-class})\n\\end{split}\n\\]\n\n\nMean squared error (MSE): Computes the distance from 1 to the probability assign by the model to the correct category (\\(\\hat{p}\\)). The squared component results in large differences in probabilities for the true class having larger penalties. Objective: minimize\n\n\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^n (1 - \\hat{p}_i)^2\n\\]\n\nCross-entropy (aka Log Loss or Deviance): Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class, it disproportionately punishes predictions where we predict a small probability for the true class (having high confidence in the wrong answer is really bad). Objective: minimize.\nGini index: Mainly used with tree-based methods and commonly referred to as a measure of purity where a small value indicates that a node contains predominantly observations from a single class. Objective: minimize\nConfusion Matrix: Compares actual categorical levels (or events) to the predicted categorical levels\n\n\n\n\n\nSome metrics related with the confusion matrix that need to be maximized are:\n\n\n\nAccuracy: Overall, how often is the classifier correct? Opposite of misclassification above. \\(\\frac{\\text{TP} + \\text{TN}}{N + P}\\).\n\nPrecision: For the number of predictions that we made, how many were correct? \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\).\n\nSensitivity (aka recall): For the events that occurred, how many did we predict? \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\).\n\nSpecificity: How accurately does the classifier classify actual negative events? \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\\).\n\nArea under the curve (AUC): A good binary classifier will have high precision and sensitivity.To capture this balance, we often use a ROC (receiver operating characteristics) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve.\n\n\n\n\n\n\nYou can more metrics in the next table."
  },
  {
    "objectID": "resampling-methods.html#introduction",
    "href": "resampling-methods.html#introduction",
    "title": "\n4  Resampling methods\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nThe Resampling Methods are indispensable to obtain additional information about a fitted model.\nIn this chapter, we will explore the methods:\n\n\nCross-validation\n\nUsed to estimate the test error in order to evaluate a model’s performance (model assessment).\nThe location of the minimum point in the test error curve to select between several models or find the best of level of ﬂexibility to one model (model selection).\n\n\n\nBootstrap\n\nEstimates the uncertainty associated with a given value or statistical learning method. For example, it can estimate the standard errors of the coeﬃcients from a linear regression ﬁt"
  },
  {
    "objectID": "resampling-methods.html#cross-validation",
    "href": "resampling-methods.html#cross-validation",
    "title": "\n4  Resampling methods\n",
    "section": "\n4.2 Cross-Validation",
    "text": "4.2 Cross-Validation\n\n4.2.1 Training error limitations\nThe training error trends to underestimate the real model’s error.\n\n\n\n\nTo solve that problem we can hold out a subset of the training observations from the ﬁtting process.\n\n4.2.2 Validation Set Approach\nSplits randomly the available set of observations into two parts, a training set and a validation set.\n\n\n\n\nAs we split the data randomly we will have a different estimation of the error rate based on the seed set in R.\n\n\n\n\n\n\n\n\n\n\nMain Characteristics\nLevel\n\n\n\nAccuracy in estimating the testing error\nLow\n\n\nTime efficiency\nHigh\n\n\nProportion of data used to train the models (bias mitigation)\nLow\n\n\nEstimation variance\n-\n\n\n\nCoding example\nYou can learn more about tidymodels and this book in ISLR tidymodels labs by Emil Hvitfeldt.\n\n# Loading functions and data\nlibrary(tidymodels)\nlibrary(ISLR)\nlibrary(data.table)\n\n\n# Defining the model type to train\nLinealRegression &lt;- linear_reg() |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"lm\")\n\n\n# Creating the rplit object\nset.seed(1)\nAutoValidationSplit &lt;- initial_split(Auto, strata = mpg, prop = 0.5)\n\nAutoValidationTraining &lt;- training(AutoValidationSplit)\nAutoValidationTesting &lt;- testing(AutoValidationSplit)\n\n\nlapply(1:10, function(degree){\n  \n  recipe_to_apply &lt;- \n    recipe(mpg ~ horsepower, data = AutoValidationTraining) |&gt;\n    step_poly(horsepower, degree = degree)\n  \n  workflow() |&gt;\n    add_model(LinealRegression) |&gt;\n    add_recipe(recipe_to_apply) |&gt;\n    fit(data = AutoValidationTraining) |&gt;\n    augment(new_data = AutoValidationTesting) |&gt;\n    rmse(truth = mpg, estimate = .pred) |&gt;\n    transmute(degree = degree,\n              .metric, \n              .estimator, \n              .estimate) }) |&gt;\n  rbindlist()\n\n    degree .metric .estimator .estimate\n     &lt;int&gt;  &lt;char&gt;     &lt;char&gt;     &lt;num&gt;\n 1:      1    rmse   standard  5.058317\n 2:      2    rmse   standard  4.368494\n 3:      3    rmse   standard  4.359262\n 4:      4    rmse   standard  4.351128\n 5:      5    rmse   standard  4.299916\n 6:      6    rmse   standard  4.319319\n 7:      7    rmse   standard  4.274027\n 8:      8    rmse   standard  4.330195\n 9:      9    rmse   standard  4.325196\n10:     10    rmse   standard  4.579561\n\n\n\n4.2.3 Leave-One-Out Cross-Validation (LOOCV)\nThe statistical learning method is ﬁt on the \\(n-1\\) training observations, and a prediction \\(\\hat{y}_1\\) is made for the excluded observation to calculate \\(\\text{MSE}_1 = (y_1-\\hat{y}_1)^2\\). Then it repeats the process \\(n\\) times and estimate the test error rate.\n\n\n\n\nBased on the average of \\(n\\) test estimates it reports the test error rate.\n\\[\n\\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n\\text{MSE}_i\n\\]\n\n\n\n\n\n\n\n\n\n\nMain Characteristics\nLevel\n\n\n\nAccuracy in estimating the testing error\nHigh\n\n\nTime efficiency\nLow\n\n\nProportion of data used to train the models (bias mitigation)\nHigh\n\n\nEstimation variance\nHigh\n\n\n\nAmazing shortcuts\nThere some cases where we can perform this technique just fitting one model with all the observations. there are listed in the next table.\n\n\n\n\n\n\n\nModel\nFormula\nDescription\n\n\n\nLineal or polynomial regression\n\\(\\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_i}{1-h_i} \\right)^2\\)\n- \\(\\hat{y}_i\\): Refers to the ith ﬁtted value from the original least squares ﬁt.  - \\(h_i\\) Refers to the leverage of \\(x_i\\) as measure of the rarity of each value.\n\n\nSmoothing splines\n\\(\\text{RSS}_{cv} (\\lambda) = \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{g}_{\\lambda} (x_i)} {1 - \\{ \\mathbf{S_{\\lambda}} \\}_{ii}} \\right]^2\\)\n- \\(\\hat{g}_\\lambda\\): Refers the smoothing spline function ﬁtted to all of the training observations.\n\n\nCoding example\n\ncollect_loo_testing_error &lt;- function(formula,\n                                      loo_split,\n                                      metric_function = rmse,\n                                      ...){\n  # Validations\n  stopifnot(\"There is no espace between y and ~\" = formula %like% \"[A-Za-z]+ \")\n  stopifnot(\"loo_split must be a data.table object\" = is.data.table(loo_split))\n  \n  predictor &lt;- sub(pattern = \" .+\", replacement = \"\", formula)\n  formula_to_fit &lt;- as.formula(formula)\n  \n  Results &lt;-\n    loo_split[, training(splits[[1L]]), by = \"id\"\n    ][, .(model = .(lm(formula_to_fit, data = .SD))),\n      by = \"id\"\n    ][loo_split[, testing(splits[[1L]]), by = \"id\"],\n      on = \"id\"\n    ][, .pred := predict(model[[1L]], newdata = .SD),\n      by = \"id\"\n    ][,  metric_function(.SD, truth = !!predictor, estimate = .pred, ...) ]\n \n  setDT(Results)\n  \n  \n  if(formula %like% \"degree\"){\n    \n    degree &lt;- gsub(pattern = \"[ A-Za-z,=\\\\~()]\", replacement = \"\", formula)\n    \n    Results &lt;- \n      Results[,.(degree = degree, \n                .metric, \n                .estimator, \n                .estimate)]\n    \n  }\n  \n  return(Results)\n    \n}\n\n\n# Creating the rplit object\nAutoLooSplit &lt;- loo_cv(Auto)\n\n# Transforming to data.table\nsetDT(AutoLooSplit)\n\npaste0(\"mpg ~ poly(horsepower, degree=\", 1:10, \")\") |&gt;\n  lapply(collect_loo_testing_error,\n         loo_split = AutoLooSplit) |&gt;\n  rbindlist()\n\n    degree .metric .estimator .estimate\n    &lt;char&gt;  &lt;char&gt;     &lt;char&gt;     &lt;num&gt;\n 1:      1    rmse   standard  4.922552\n 2:      2    rmse   standard  4.387279\n 3:      3    rmse   standard  4.397156\n 4:      4    rmse   standard  4.407316\n 5:      5    rmse   standard  4.362707\n 6:      6    rmse   standard  4.356449\n 7:      7    rmse   standard  4.339706\n 8:      8    rmse   standard  4.354440\n 9:      9    rmse   standard  4.366764\n10:     10    rmse   standard  4.414854\n\n\n\n4.2.4 k-Fold Cross-Validation\nInvolves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The ﬁrst fold is treated as a validation set, and the method is ﬁt on the remaining \\(k-1\\) folds.\n\n\n\n\nBased on the average of \\(k\\) test estimates it reports the test error rate.\n\\[\n\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{i=1}^k\\text{MSE}_i\n\\]\n\n\n\n\n\n\n\n\n\n\nMain Characteristics\nLevel\n\n\n\nAccuracy in estimating the testing error\nHigh\n\n\nTime efficiency\nRegular\n\n\nProportion of data used to train the models (bias mitigation)\nRegular\n\n\nEstimation variance\nRegular\n\n\n\nAccording to Hands-on Machine Learning with R, as \\(k\\) gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease.\nNote: The book recommends using \\(k = 5\\) or \\(k = 10\\).\nCoding example\n\nAutoKFoldRecipe &lt;- \n  recipe(mpg ~ horsepower, data = Auto) |&gt;\n  step_poly(horsepower, degree = tune())\n\nAutoTuneReponse &lt;-\n  workflow() |&gt;\n  add_recipe(AutoKFoldRecipe) |&gt;\n  add_model(LinealRegression) |&gt;\n  tune_grid(resamples = vfold_cv(Auto, v = 10), \n            grid = tibble(degree = seq(1, 10)))\n\nshow_best(AutoTuneReponse, metric = \"rmse\")\n\n# A tibble: 5 × 7\n  degree .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1      7 rmse    standard    4.33    10   0.144 Preprocessor07_Model1\n2      6 rmse    standard    4.34    10   0.143 Preprocessor06_Model1\n3      8 rmse    standard    4.35    10   0.145 Preprocessor08_Model1\n4      5 rmse    standard    4.35    10   0.146 Preprocessor05_Model1\n5      9 rmse    standard    4.36    10   0.149 Preprocessor09_Model1\n\nautoplot(AutoTuneReponse)\n\n\n\n\n\n4.2.5 LOOCV vs 10-Fold CV accuracy\n\n\nTrue test MSE: Blue solid line\n\nLOOCV: Black dashed line\n\n10-fold CV: Orange solid line"
  },
  {
    "objectID": "resampling-methods.html#bootstrap",
    "href": "resampling-methods.html#bootstrap",
    "title": "\n4  Resampling methods\n",
    "section": "\n4.3 Bootstrap",
    "text": "4.3 Bootstrap\nBy taking many samples from a population we can obtain the Sampling Distribution of a value, but in many cases we just can get a single sample from the population. In theses cases, we can resample the data with replacement to generate many samples from one sample, creating a Bootstrap Distribution of a value.\n\n\n\n\nAs you can see bellow the center of the Bootstrap Distribution must of the time differs from the center of the Sampling Distribution, but its very accurate at estimating the dispersion of the value.\n\n\n\n\n\n4.3.1 Coding example\n\nAutoBootstraps &lt;- bootstraps(Auto, times = 500)\n\nboot.fn &lt;- function(split) {\n  LinealRegression |&gt; \n    fit(mpg ~ horsepower, data = analysis(split)) |&gt;\n    tidy()\n}\n\nAutoBootstraps |&gt;\n  mutate(models = map(splits, boot.fn)) |&gt;\n  unnest(cols = c(models)) |&gt;\n  group_by(term) |&gt;\n  summarise(low = quantile(estimate, 0.025),\n            mean = mean(estimate),\n            high = quantile(estimate, 0.975),\n            sd = sd(estimate))\n\n# A tibble: 2 × 5\n  term           low   mean   high      sd\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 38.5   40.0   41.7   0.851  \n2 horsepower  -0.173 -0.158 -0.145 0.00734"
  },
  {
    "objectID": "generalized-linear-models.html#linear-regresion",
    "href": "generalized-linear-models.html#linear-regresion",
    "title": "\n5  Generalized linear models (GLM)\n",
    "section": "\n5.1 Linear regresion",
    "text": "5.1 Linear regresion\n\n5.1.1 Getting the Least Squares Line of a sample\nAs the population regression line is unobserved the least squares line of a sample is a good estimation. To get it we need to follow the next steps:\n\nDefine the function to fit.\n\n\\[\n\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x\n\\]\n\nDefine how to calculate residuals.\n\n\\[\ne_{i} = y_{i} - \\hat{y}_{i}\n\\]\n\nDefine the residual sum of squares (RSS).\n\n\\[\nRSS = e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\n\\]\n\nUse calculus or make estimation with a computer to find the coefficients that minimize the RSS.\n\n\\[\n\\hat{\\beta}_{1} = \\frac{\\Sigma_{i=1}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})}\n                       {\\Sigma_{i=1}^{n}(x_{i}-\\overline{x})}\n, \\quad\n\\hat{\\beta}_{0} = \\overline{y} - \\hat{\\beta}_{1}\\overline{x}\n\\]\n\n5.1.2 Getting confident intervarls of coeffients\nTo estimate the population regression line we can calculate conﬁdence intervals for sample coefficients, to define a range where we can find the population values with a defined confidence level.\n\nIf we want to use 95% of confidence we need to know that after taking many samples only 95% of the intervals produced with this confident level would have the true value (parameter).\n\nTo generate confident intervals we would need to calculate the variance of the random error.\n\\[\n\\sigma^2 = Var(\\epsilon)\n\\]\nBut as we can not calculate that variance an alternative can be to estimate it based on residuals if they meet the next conditions:\n\nEach residual have common variance \\(\\sigma^2\\), so the variances of the error terms shouldn’t have any relation with the value of the response.\nResiduals are uncorrelated. For example, if \\(\\epsilon_{i}\\) is positive, that provides little or no information about the sign of \\(\\epsilon_{i+1}\\).\n\nIf not, we would end underestimating the true standard errors, reducing the probability a given confident level to contain the true value of the parameter and underrating the p-values associated with the model.\n\\[\n\\sigma \\approx RSE = \\sqrt{\\frac{RSS}{(n-p-1)}}\n\\]\nNow we can calculate the standard error of each coefficient and calculate the confident intervals.\n\\[\nSE(\\hat{\\beta_{0}})^2 = \\sigma^2\n                       \\left[\\frac{1}{n}+\n                             \\frac{\\overline{x}^2}\n                                  {\\Sigma_{i=1}^{n} (x_{i}-\\overline{x})^2}\n                       \\right]\n\\]\n\\[\nSE(\\hat{\\beta_{1}})^2 = \\frac{\\sigma^2}\n                             {\\Sigma_{i=1}^{n} (x_{i} - \\overline{x})^2}\n\\]\n\\[  \n\\hat{\\beta_{1}} \\pm 2 \\cdot SE(\\hat{\\beta_{1}}), \\quad \\hat{\\beta_{0}} \\pm 2 \\cdot SE(\\hat{\\beta_{0}})\n\\]\n\n5.1.3 Insights to extract\nConfirm the relationship between the Response and Predictors\nUse the regression overall P-value (based on the F-statistic) to confirm that at least one predictor is related with the Response and avoid interpretative problems associated with the number of observations (n) or predictors (p).\n\\[\nH_{0}: \\beta_{1} = \\beta_{2} = \\dots = \\beta_{p} = 0\n\\]\n\\[\nH_{a}: \\text{at least one } \\beta_{j} \\text{ is non-zero}\n\\]\nAccuracy of the model (relationship strength)\nIf we want to know how well the model fits to the data we have two options:\n\nResidual standard error (RSE): Even if the model were correct, the actual values of \\(\\hat{y}\\) would differ from the true regression line by approximately this units, on average. To get the percentage error we can calculate \\(RSE/\\overline{x}\\)\nThe \\(R^2\\) statistic: The proportion of variance explained by taking as a reference the total sum of squares (TSS).\n\n\\[\nTSS = \\Sigma(y_{i} - \\overline{y})^2\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS}\n\\]\n\\[\nR^2 =\n\\begin{cases}\n    Cor(X, Y)^2  & \\text{Simple Lineal Regresion} \\\\\n    Cor(Y,\\hat{Y})^2 & \\text{Multipline Lineal Regresion}\n\\end{cases}\n\\]\nConfirm the relationship between the Response and each predictor\nTo answer that we can test if a particular subset of q of the coefficients are zero.\n\\[\nH_{0}: \\beta_{p-q+1} = \\beta_{p-q+2} = \\dots = \\beta_{p} = 0\n\\]\nIn this case, F-statistic reports the partial eﬀect of adding a extra variable to the model (the order matters) to apply a variable selection technique. The classical approach is to:\n\nFit a model for each variable combination \\(2^p\\).\nSelect the best model based on Mallow’s Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted \\(R^2\\) or plot various model outputs, such as the residuals, in order to search for patterns.\n\nBut just think that if we have \\(p = 30\\) we will have \\(2^{30} = =1,073,741,824\\ models\\) to fit, that it’s too much. Some alternative approaches for this task:\n\nForward selection\nBackward selection (cannot be used if p &gt;n)\nMixed selection\nSize of association between each predictor and the response.\nTo check that we need to see the \\(\\hat{\\beta}_{j}\\) confident intervals as the real \\(\\beta_{j}\\) is in that range.\nPredicting future values\nIf we want to predict the average response \\(f(X)\\) we can use the confident intervals, but if we want to predict an individual response \\(Y = f(X) + \\epsilon\\) we need to use prediction intervals as they account for the uncertainty associated with \\(\\epsilon\\), the irreducible error.\n\n5.1.4 Standard linear regression model assumptions\n\nThe additivity assumption means that the association between a predictor \\(X_{j}\\) and the response \\(Y\\) does not depend on the values of the other predictors, as it happens when there is a interaction (synergy) effect\nThe linearity assumption states that the change in the response Y associated with a one-unit change in \\(X_{j}\\) is constant, regardless of the value of \\(X_{j}\\).\n\nIncluding an interaction term\nThis approach relax the additivity assumption that models usually have.\n\n2 quantitative variables\n\nIt consist in adding an extra coefficient which multiplies two or more variables.\n\\[\n\\begin{split}\nY & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\beta_{3} X_{1} X_{2} + \\epsilon \\\\\n  & = \\beta_{0} + (\\beta_{1} + \\beta_{3} X_{2}) X_{1} + \\beta_{2} X_{2} + \\epsilon \\\\\n  & = \\beta_{0} + \\tilde{\\beta}_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon\n\\end{split}\n\\]\nAfter adding the interaction term we could interpret the change as making one of the original coefficient a function of the another variable. Now we could say that \\(\\beta_{3}\\) represent the change of \\(X_{1}\\) effectiveness associated with a one-unit increase in \\(X_{2}\\).\nIt very important that we keep hierarchical principle, which states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant as it would alter the meaning of the interaction.\n\n1 quantitative and 1 qualitative variable\n\nIf \\(X_{1}\\) is quantitative and \\(X_{2}\\) is qualitative:\n\\[\n\\hat{Y} =\n\\begin{cases}\n    (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3})X_{1} & \\text{if }X_{2} \\text{ is TRUE}\\\\\n    \\beta_{0} + \\beta_{1}X_{1}                             & \\text{if }X_{2} \\text{ is FALSE}\n\\end{cases}\n\\]\nAdding the \\(\\beta_{3}\\) interaction allow the line to change the line slope based on \\(X_{2}\\) and not just a different intercept.\n\n\n\n\nPolynomial regression\nThis approach relax the linearity assumption that models usually have. It consist in including transformed versions of the predictors.\n\\[\n\\begin{split}\nY & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} \\\\\n  & = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{1}^2\n\\end{split}\n\\]\n\n5.1.5 Possible problems\nThe target doesn’t follow a normal distribution\nWe have two ways to detect this problem:\n\nPlot the target variable with a histogram\n\n\n\n\n\n\nPlotting the residual and checking the plot creates a funnel shape, demonstrating a non-constant variance (heteroscedasticity) of error terms.\n\n\n\n\n\nTo solve this problem we can apply transformations to the target variable.\n\nApplying a concave function such as \\(\\log{Y}\\) or \\(\\sqrt{Y}\\). If the target value has 0s we can define the argument offset as 1.\n\names_recipe &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_log(all_outcomes(), offset = 0)\n\nApplying Box Cox transformation is more flexible than the log transformation and will find an appropriate transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution. Its lambda (\\(\\lambda\\)) value varies from -5 to 5 and it’s selected based on the traning data.\n\n\\[\ny(\\lambda) =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda}, \\text{if } \\lambda \\neq 0 \\\\\n\\log{(Y)}, \\text{if } \\lambda = 0\n\\end{cases}\n\\]\n\nIf your data consists of values \\(y \\leq 1\\) use the Yeo-Johnson transformation, which is very similar to the Box-Cox one.\n\names_recipe &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_YeoJohnson(all_outcomes())\nTo make correct interpretations of our predictions it’s important to remember that we need to revert this transformation after getting the prediction.\nCollinearity\nCollinearity refers to the situation in which two or more predictor variables are closely related (highly correlated) to one another. It reduces the accuracy of the estimates of the regression coeﬃcients and causes the standard error for \\(\\hat{\\beta}_{j}\\) to grow. That reduce the power of the hypothesis test,that is, the probability of correctly detecting a non-zero coeﬃcient.\nLooking at the correlation matrix of the predictors could be usefull, but it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation (multicollinearity).\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nThe best way to assess multicollinearity is to compute the variance inﬂation factor (VIF), which is the ratio of the variance of \\(\\hat{\\beta}_{j}\\) when ﬁtting the full model divided by the variance of \\(\\hat{\\beta}_{j}\\) if ﬁt on its own with 1 as its lowest value and 5 or 10 as problematic values of collinearity\n1. Drop one of the problematic variables from the regression.   2. Combine the collinear variables together into a single predictor\n\n\n\\[\n\\text{VIF}(\\hat{\\beta}_{j}) = \\frac{1}\n                                   {1 - R_{X_{j}|X_{-j}}^2}\n\\]\nWhere \\(R_{X_{j}|X_{-j}}^2\\) is the \\(R^2\\) from a regression of \\(X_{j}\\) onto all of the other predictors.\nNon-linearity of the response-predictor relationships\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nPlot the residuals versus predicted values \\(\\hat{y}_{i}\\). Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\nA simple approach is to use non-linear transformations of the predictors, such as \\(\\log{X}\\), \\(\\sqrt{X}\\), and \\(X^2\\), in the regression model\n\n\n\n\n\n\nCorrelation of error terms\nIf there is correlation among the errors, then the estimated standard errors of the coefficients will be biased leading to prediction intervals being narrower than they should be.\n\n\n\n\n\n\nDetection method\nSolutions\n\n\n1. Plot the residuals from our model as a function of time or execution order. If the errors are uncorrelated, then there should be no discernible pattern.   2. Check if some observation have been exposed to the same environmental factors\nGood experimental design is crucial in order to mitigate these problems\n\nOutliers\nAn outlier is a point for which \\(y_{i}\\) is far from the value predicted by the model. Sometimes, they have little effect on the least squares line, but over estimate the RSE making bigger p-values of the model and under estimate the \\(R^2\\).\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nPlot the studentized residuals, computed by dividing each residual \\(e_{i}\\) by its estimated standard error. Then search for points which absolute value is greater than 3\nThey can be removed if it has occurred due to an error in data collection. Otherwise, they may indicate a deficiency with the model, such as a missing predictor.\n\n\n\n\n\n\nHigh-leverage points\nObservations with high leverage have an unusual value for \\(x_{i}\\). High leverage observations tend to have a sizable impact on the estimated regression line and any problems with these points may invalidate the entire fit.\n\n\n\n\n\n\nDetection method\nSolutions\n\n\nCompute the leverage statistic. Find an observation with higher value than mean, represented by \\((p + 1)/n\\). Leverage values are always between \\(1/n\\) and \\(1\\)\n\nMake sure that the value is correct and not a data collection problem\n\n\n\\[\nh_{i} = \\frac{1}{n} +\n        \\frac{(x_{i} - \\overline{x})^2}\n              {\\Sigma_{i'=1}^n(x_{i'} - \\overline{x})^2}\n\\]\nIn a multiple linear regression, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors.\n\n\n\n\n\n5.1.6 Avoid using for classification problems\nThere are better model to achieve that kind of situation. For example, he linear discriminant analysis (LDA) procedure the same response of a linear regression for a binary problem. Other reasons are:\n\nA regression method cannot accommodate a qualitative response with more than two classes.\nA regression method will not provide meaningful estimates of \\(Pr(Y|X)\\) as some of our estimates might be outside the [0, 1] interval.\n\n5.1.7 Benefits of Replacing plain least squares ﬁtting\n\nPrediction Accuracy: If n is not much larger than p, then there can be a lot of variability in the least squares ﬁt, resulting in overﬁtting and consequently poor predictions on future observations not used in model training. And if p &gt;n, then there is no longer a unique least squares coeﬃcient estimate: the variance is inﬁnite so the method cannot be used at all. As an alternative, We could reduce the variance by increasing in bias (constraining and shrinking).\nModel Interpretability:There are some methods that can exclude irrelevant variables from a multiple regression model (feature selection or variable selection).\n\n5.1.8 Coding example\nTo perform Linear Regression we just need to create the model specification by using lm engine.\n\nlibrary(ISLR2)\nlibrary(tidymodels)\n\nset.seed(123)\nBikeshareSpit &lt;- initial_split(Bikeshare)\n\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nlm_rec_spec &lt;- \n  recipe(bikers ~ mnth + hr + workingday + temp + weathersit,\n         data = Bikeshare ) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nworkflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(lm_rec_spec) %&gt;%\n  last_fit(split = BikeshareSpit) %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      76.1   Preprocessor1_Model1\n2 rsq     standard       0.671 Preprocessor1_Model1"
  },
  {
    "objectID": "generalized-linear-models.html#subset-selection",
    "href": "generalized-linear-models.html#subset-selection",
    "title": "\n5  Generalized linear models (GLM)\n",
    "section": "\n5.2 Subset Selection",
    "text": "5.2 Subset Selection\nThis approach involves identifying a subset of the p predictors that we believe to be related to the response. We then ﬁt a model using least squares on the reduced set of variables.\n\n5.2.1 Best Subset Selection\nIt ﬁts all p models that contain exactly one predictor, all \\(\\left( \\begin{array}{c} p \\\\ 2 \\end{array} \\right) = p (p-1)/2\\) models that contain exactly two predictors, and so forth. Then it selects the best model based on smallest RSS or the largest \\(R^2\\).\n\n\nAlgorithm 6.1\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which represent the sample mean for each observation.\nFor \\(k = 1, 2, \\dots, p\\):\n\n\nFit all \\(\\left( \\begin{array}{c} p \\\\ k \\end{array} \\right)\\) models that contain exactly k predictors.\nPick the best among these \\(\\left( \\begin{array}{c} p \\\\ k \\end{array} \\right)\\) models using the smallest RSS or the deviance for classification (negative two times the maximized log-likelihood), and call it \\(\\mathcal{M}_k\\)\n\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\dots, \\mathcal{M}_p\\) using cross- validated prediction error, \\(C_p\\) (AIC), BIC or adjusted \\(R^2\\).\n\n\n\nThis method is really computational expensive as it needs to fit \\(2^p\\) models. Just think that if your data has 20 predicts, then there are over one million possibilities. Thus an enormous search space can also lead to overﬁtting and high variance of the coeﬃcient estimates.\n\n5.2.2 Stepwise Selection: Forward Stepwise Selection\nIt begins with a model containing no predictors, and then adds the predictors who gives the greatest additional improvement to the ﬁt, one-at-a-time, until all of the predictors are in the model, as result we will need to fit \\(1+p(p+1)/2\\) models.\n\n\nAlgorithm 6.2\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which represent the sample mean for each observation.\nFor \\(k = 0, \\dots, p-1\\):\n\n\nConsider all \\(p-k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\nChoose the best among these \\(p-k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is deﬁned as having smallest RSS.\n\n\nSelect a single best model from among \\(\\mathcal{M}_k, \\dots, \\mathcal{M}_p\\) using cross- validated prediction error, \\(C_p\\) (AIC), BIC or adjusted \\(R^2\\).\n\n\n\nThough it tends to do well in practice, it is not guaranteed to ﬁnd the best possible model out of all \\(2^p\\) models containing subsets of the p predictors, but can be used even if \\(n &lt; p\\).\n\n5.2.3 Stepwise Selection: Backward Stepwise Selection\nIt begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time. As result we will need to fit \\(1+p(p+1)/2\\) models\n\n\nAlgorithm 6.3\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains all p predictors.\nFor \\(k = p, p -1, \\dots , 1\\):\n\n\nConsider all k models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k-1\\) predictors.\nChoose the best among these k models, and call it \\(\\mathcal{M}_{k-1}\\). Here best is deﬁned as having smallest RSS.\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\dots, \\mathcal{M}_p\\) using cross- validated prediction error, \\(C_p\\) (AIC), BIC or adjusted \\(R^2\\).\n\n\n\nThis method don’t guarantee to yield the best model containing a subset of the p predictors and we need to cofirm that \\(n \\geq p\\).\n\n5.2.4 Stepwise Selection: Hybrid Approaches\nIn this method variables adds variables sequentially, but after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model ﬁt. Such an approach attempts to more closely mimic Best Subset Selection while retaining the computational advantages of forward and backward stepwise selection.\n\n5.2.5 Choosing the Optimal Model\nAs the model which contains all of the predictors will always have the smallest RSS, we need to estimate the test error rate by:\n\nMaking an adjustment to the training error to account for the bias due to overﬁtting.\nUsing either a validation set approach or a cross-validation approach.\n\nLet’s see the methods that relay on correcting the training error:\n\n\n\n\n\n\n\nMethod\nFormula\nInterpretation\n\n\n\n\\(C_p\\)\n\\(C_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2)\\)\n\n\\(2d\\hat{\\sigma}^2\\) represent the penalty of adding new predictors. This method is a good estimation of test MSE if the \\(\\hat{\\sigma}^2\\) is an unbiased estimate of the \\(\\sigma^2\\)\n\n\n\nAkaike information criterion\n\\(\\text{AIC} = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2)\\)\nThe book omits irrelevant constants to show that \\(C_p\\) and \\(\\text{AIC}\\) are proportional to each other\n\n\nBayesian information criterion\n\\(\\text{BIC} = \\frac{1}{n}(\\text{RSS} +\\log(n) d \\hat{\\sigma}^2)\\)\nAfter omitting irrelevant constants, we can see that \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), as consequence, the metric trends to add a heavier penalty on models with many variables than the \\(C_p\\) and tent to select models with fewer predictors\n\n\nAdjusted \\(R^2\\)\n\n\\(\\text{Adjusted} \\; R^2 = 1 - \\frac{\\text{RSS}/(n - d -1)}{\\text{TSS}/(n-1)}\\)\nA large value of adjusted \\(R^2\\) indicates a model with a small test error, even though the metric doesn’t rely on rigorous theoretical justiﬁcations.\n\n\n\nWhere:\n\n\n\\(n\\): Number of observations\n\n\\(d\\): Number of predictors\n\n\\(\\hat{\\sigma}^2\\): Estimate of the variance of the error \\(\\epsilon\\) associated with each response"
  },
  {
    "objectID": "generalized-linear-models.html#shrinkage",
    "href": "generalized-linear-models.html#shrinkage",
    "title": "\n5  Generalized linear models (GLM)\n",
    "section": "\n5.3 Shrinkage",
    "text": "5.3 Shrinkage\nThis approach involves ﬁtting a model involving all p predictors and shrinks the estimated coeﬃcients towards zero. Depending on what type of shrinkage is performed, some of the coeﬃcients may be estimated exactly at zero, performing some variable selection.\n\n5.3.1 Ridge Regression\nThe method rather than using RSS as the metric to minimize with the regression coefficient \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\), it is modified by adding a shrinkage penalty that the effect of estimating \\(\\beta_j\\) towards zero when the coefficient is close to 0:\n\\[\nRSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nWhere:\n\n\n\\(\\lambda\\): It’s a tuning parameter \\(\\geq 0\\) that can be calculated using cross-validation.\n\n\\(\\text{RSS}\\): Present the residual standard error.\n\nAs result, the ridge regression will produce a diﬀerent set of coeﬃcient estimates for each \\(\\lambda\\) value, \\(\\hat{\\beta}_\\lambda^R\\). By plotting the new coefficients against the penalty used we can see how the coefficients move towards zero without reaching the absolute 0, but may also be useful to plot the \\(\\ell_2 \\; \\mathcal{norm}\\) (\\(\\| \\beta \\|_2 = \\sqrt{\\sum_{j=1}^p \\beta^2}\\)) proportion of the ridge vs least squares coefficients, to compute the proportion in which the ridge regression coeﬃcient estimate have been shrunken towards zero.\n\n\n\n\nAs the \\(\\lambda\\) to select is sensible to the predictors scaling, it’s a good practice scaling the predictors using the next formula, to make all the predictors to have an standard deviation of 1:\n\\[\n\\tilde{x}_{ij} =\n\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}} =\n\\frac{x_{ij}}{\\sigma_j}\n\\]\nWhen the number of variables p is almost as large as the number of observations n the least squares estimates will be extremely variable and this method increase the bias by reducing the flexibility through \\(\\lambda\\) to reduce the variance and find the lower error rate, without making many computations as happens the best subset selection method.\n\n\n\n\n\n5.3.2 Lasso Regression\nRidge Regression don’t set the coefficient exactly to zero (unless \\(\\lambda = \\infty\\)). That doesn’t affect the model accuracy but doesn’t provide any help when we have to interpret a model with many predicts. To over come that problem, the Lasso Regression performs variable selection based on minimization of the next function:\n\\[\nRSS + \\lambda \\sum_{j=1}^p |\\beta_j|\n\\]\nWhere:\n\n\n\\(\\lambda\\): It’s a tuning parameter \\(\\geq 0\\) that can be calculated using cross-validation.\n\n\\(\\text{RSS}\\): Present the residual standard error.\n\nAs consequence, the method uses the \\(\\ell_1\\) penalty (\\(\\| \\beta \\|_1 = \\sum_{j=1}^p |\\beta|\\)), instead of the \\(\\ell_2\\).\n\n\n\n\n\n5.3.3 Coding example\nTo perform a Ridge or Lasso regression we need to use the function linear_reg and define the mixture argument depending on the regression we want to perform.\n\n\nModel\nMixture\n\n\n\nRidge Regression\nmixture = 0\n\n\n\nLasso Regression\nmixture = 1\n\n\n\n\nAs both regression depend on the penalty parameter we need to use cross-validation to estimate the best one. But, if you want to explore the parameter by yourself you has the next options.\n\nFit a model and explore different penalties using tidy, augment, predict or autoplot functions.\n\n\nHitters &lt;- \n  as_tibble(Hitters) %&gt;%\n  filter(!is.na(Salary))\n\nridge_fit &lt;- \n  linear_reg(mixture = 0, penalty = 0) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  fit(Salary ~ ., data = Hitters)\n\ntidy(ridge_fit, penalty = 11498)\n\n# A tibble: 20 × 3\n   term         estimate penalty\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) 407.        11498\n 2 AtBat         0.0370    11498\n 3 Hits          0.138     11498\n 4 HmRun         0.525     11498\n 5 Runs          0.231     11498\n 6 RBI           0.240     11498\n 7 Walks         0.290     11498\n 8 Years         1.11      11498\n 9 CAtBat        0.00314   11498\n10 CHits         0.0117    11498\n11 CHmRun        0.0876    11498\n12 CRuns         0.0234    11498\n13 CRBI          0.0242    11498\n14 CWalks        0.0250    11498\n15 LeagueN       0.0866    11498\n16 DivisionW    -6.23      11498\n17 PutOuts       0.0165    11498\n18 Assists       0.00262   11498\n19 Errors       -0.0206    11498\n20 NewLeagueN    0.303     11498\n\npredict(ridge_fit, new_data = Hitters, penalty = 11498)\n\n# A tibble: 263 × 1\n   .pred\n   &lt;dbl&gt;\n 1  533.\n 2  553.\n 3  620.\n 4  487.\n 5  559.\n 6  440.\n 7  446.\n 8  453.\n 9  620.\n10  615.\n# ℹ 253 more rows\n\nautoplot(ridge_fit)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()\n\n\n\n\nLet’s have a example using the tidymodels approach. By following the next steps:\n\nSplitting the data in testing and training data.\n\n\nset.seed(18)\nHitters_split &lt;- initial_split(Hitters, strata = \"Salary\")\n\nHitters_train &lt;- training(Hitters_split)\nHitters_test &lt;- testing(Hitters_split)\n\n\nDefine the workflow to use.\n\n\nridge_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_predictors())\n\nridge_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 0) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\")\n\nridge_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(ridge_recipe) %&gt;% \n  add_model(ridge_spec)\n\n\nUse cross-validation to estimate the testing error and tune the penalty.\n\n\nset.seed(40)\nHitters_fold &lt;- vfold_cv(Hitters_train, v = 10)\n\n\nDefine the penalties to check. Where the range indicates the limit of \\(x\\) in the function \\(10^x\\) and the level the number of step to complete the range.\n\n\npenalty_grid &lt;- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\n\nLet’s fit our models.\n\n\ntune_res &lt;- \n  tune_grid(ridge_workflow,\n            resamples = Hitters_fold, \n            grid = penalty_grid)\n\n\nCheck the test error change in a plot or just export a table.\n\n\nautoplot(tune_res)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()\n\n\n\ncollect_metrics(tune_res) %&gt;%\n  filter(.metric == \"rsq\") %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 50 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    910. rsq     standard   0.454    10  0.0647 Preprocessor1_Model40\n 2    569. rsq     standard   0.454    10  0.0646 Preprocessor1_Model39\n 3   1456. rsq     standard   0.453    10  0.0646 Preprocessor1_Model41\n 4    356. rsq     standard   0.453    10  0.0644 Preprocessor1_Model38\n 5   2330. rsq     standard   0.452    10  0.0645 Preprocessor1_Model42\n 6    222. rsq     standard   0.450    10  0.0640 Preprocessor1_Model37\n 7   3728. rsq     standard   0.450    10  0.0643 Preprocessor1_Model43\n 8   5964. rsq     standard   0.449    10  0.0642 Preprocessor1_Model44\n 9   9541. rsq     standard   0.448    10  0.0641 Preprocessor1_Model45\n10    139. rsq     standard   0.448    10  0.0634 Preprocessor1_Model36\n# ℹ 40 more rows\n\n\n\nSelect the best model configuration and update the workflow to use that parameter.\n\n\nbest_penalty &lt;- select_best(tune_res, metric = \"rsq\")\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1    910. Preprocessor1_Model40\n\nridge_final &lt;- finalize_workflow(ridge_workflow, best_penalty)\nridge_final\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 910.298177991523\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\nFit the final model and validate the performance.\n\n\nridge_final_fit &lt;- fit(ridge_final, data = Hitters_train)\n\naugment(ridge_final_fit, new_data = Hitters_test) %&gt;%\n  rsq(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.514\n\n\nTo perform a Lasso regression we don’t need to repeat all the code.\n\nlasso_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\")\n\nlasso_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(ridge_recipe) %&gt;% \n  add_model(lasso_spec)\n\nlasso_penalty_grid &lt;- grid_regular(penalty(range = c(-3, 2)), levels = 50)\n\nlasso_tune_res &lt;- \n  tune_grid(lasso_workflow,\n            resamples = Hitters_fold, \n            grid = lasso_penalty_grid)\n\nautoplot(lasso_tune_res)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()"
  },
  {
    "objectID": "generalized-linear-models.html#dimension-reduction",
    "href": "generalized-linear-models.html#dimension-reduction",
    "title": "\n5  Generalized linear models (GLM)\n",
    "section": "\n5.4 Dimension Reduction",
    "text": "5.4 Dimension Reduction\nThis method projects the \\(p\\) predictors into an \\(M\\)-dimensional subspace. If \\(Z_1, Z_2, \\dots, Z_M\\) represent \\(M &lt; p\\) lineal combinations \\(Z_m = \\sum_{j=1}^p \\phi_{jm}X_j\\) of ALL our original predictors based on some constants \\(\\phi_{1m}, \\phi_{2m}, \\dots, \\phi_{pm}\\), then we can use the new variables to fit a linear regression model by least squares.\n\\[\ny_i = \\theta_0 + \\sum_{m=1}^M \\theta_m z_{im} + \\epsilon_i,\n\\qquad i=1, \\dots, n.\n\\]\nThis is not a feature selection method as each of the M principal components used in the regression is a linear combination of all p of the original features.In this sense, PCR is more closely related to ridge regression than to the lasso.\nTo select the \\(\\phi_{jm}\\)’s we will discuss two different ways:\n\n5.4.1 Principal Components Regression (PCR)\nThe PCR assumes that the directions in which \\(X_1, \\dots, X_M\\) show the most variation are the directions that are associated with \\(Y\\). If it’s that is true then fitting the model to \\(Z_1, \\dots, Z_m\\) will lead better results than using the original variables \\(X_1, \\dots, X_p\\)\nTo perform a principal components analysis (PCA):\n\nIt’s recommended to standardize each predictor to have the same scale.\n\n\\[\n\\tilde{x}_{ij} =\n\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}} =\n\\frac{x_{ij}}{\\sigma_j}\n\\]\n\nSelect as the ﬁrst principal component the variable where the data vary the most.\n\n\n\n\n\n\nProject the observations on the first component, to get the largest possible variance\n\n\n\n\n\n\nThen maximize the \\(\\text{Var}(\\phi_{11} \\times (x_1-\\overline{x_1}) + \\phi_{21} \\times (x_2-\\overline{x_2}))\\) where \\(\\phi_{11}^2 + \\phi_{21}^2 = 1\\) to the get the principal component loadings. As result \\(Z_1\\) it’s a weighted average of the to variables.\nRepeat the process until having p distinct principal components perpendicular to the previews one.\n\nIn general, this method performs better when we just need to use few principal components.\n\n\n\n\n\n\n\n\nThe number of principal components, M, is typically chosen by cross-validation.\n\n5.4.2 Partial Least Squares (PLS)\nThe PLS approach attempts to ﬁnd directions that help explain both the response and the predictors by placing the highest weight on the variables that are most strongly related to the response.\n\nStandardize the predictors and response.\n\n\\[\n\\tilde{x}_{ij} =\n\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}} =\n\\frac{x_{ij}}{\\sigma_j}\n\\]\n\nCompute the ﬁrst direction \\(Z_1\\) by setting each \\(\\phi_{j1}\\) equal to the coeﬃcient from the simple linear regression of \\(Y \\sim X_j\\), which it’s also proportional to the correlation.\nAdjust each of the variables for \\(Z_1\\), by regressing each variable on \\(Z_1\\) and taking residuals.\nCompute \\(Z_2\\) using this orthogonalized data (residuals) in exactly the same fashion as \\(Z_1\\) was computed based on the original data and repete the process \\(M\\) times.\nUse least squares to ﬁt a linear model to predict \\(Y\\) using \\(Z_1, \\dots, Z_M\\)\n\nTo select the number \\(M\\) we can use cross-validation.\n\n5.4.3 PCR vs PLS\nThe next figure illustrates how each method work.\n\n\n\n\nThe next figure illustrates that the first two PCs when using - PCR: Has very little relationship to the response variable. - PLS: Has a much stronger association to the response variable.\n\n\n\n\n\n\n\n\n\n\nPerformance Note\n\n\n\nIn practice, it often performs no better than ridge regression or PCR. While the supervised dimension reduction of PLS can reduce bias, it also has the potential to increase variance, so that the overall beneﬁt of PLS relative to PCR is a wash.\n\n\n\n5.4.4 Coding example\nTo run a PCR or a PLS we just need to set a lineal model.\n\nlm_spec &lt;- \n  linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nAnd tune our recipe to determinate the threshold and the number of components to use num_comp.\n\npca_recipe &lt;- \n  recipe(formula = Salary ~ ., data = Hitters_train) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors(), \n           threshold = tune(),\n           num_comp = tune())\n\npca_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(pca_recipe) %&gt;% \n  add_model(lm_spec)\n\nthreshold_grid &lt;- \n  grid_regular(threshold(), \n               num_comp(c(1, 20)),\n               levels = 5)\n\npca_tune_res &lt;- \n  tune_grid(pca_workflow,\n            resamples = Hitters_fold, \n            grid = threshold_grid)\n\nAs we can see below the number of component don’t have any effect over the test error.\n\nautoplot(pca_tune_res)+\n  scale_x_log10(labels = scales::comma_format(accuracy = 1))+\n  theme_light()\n\n\n\nbest_threshold &lt;- select_best(pca_tune_res, metric = \"rmse\")\nbest_threshold\n\n# A tibble: 1 × 3\n  num_comp threshold .config              \n     &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                \n1        1      0.75 Preprocessor04_Model1\n\nfinal_pcr_fit &lt;-\n  finalize_workflow(pca_workflow, best_threshold) |&gt;\n  fit(data = Hitters_train)\n\nfinal_pcr_fit |&gt;\n  augment(new_data = Hitters_test) |&gt;\n  rmse(truth = Salary, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        402.\n\n\nBut thanks to the DALEXtra package we can interpret these complex models.\n\nlibrary(DALEXtra)\n\nset.seed(1807)\nexplain_tidymodels(\n  final_pcr_fit, \n  data = Hitters_train %&gt;% select(-Salary), \n  y = Hitters_train$Salary,\n  label = \"RDA\",\n  verbose = FALSE\n) %&gt;% \n  model_parts() |&gt;\n  plot()"
  },
  {
    "objectID": "generalized-linear-models.html#poisson-regression",
    "href": "generalized-linear-models.html#poisson-regression",
    "title": "\n5  Generalized linear models (GLM)\n",
    "section": "\n5.5 Poisson Regression",
    "text": "5.5 Poisson Regression\nIf \\(Y \\in \\{ 0, 1, 2, 3, \\dots \\}\\) that could be the result after counting a particular event the linear regression might not meet our needs as it could bring negative numbers. The Poisson Distribution follow the next function:\n\\[\nPr(Y = k) = \\frac{e^{-\\lambda} \\lambda^{k}}\n                 {k!}\n\\]\nWhere: - \\(\\lambda\\) must be greater than 0. It represents the expected number of events \\(E(Y)\\) and variance related \\(Var(Y)\\) - \\(k\\) represent the number of events that we want to evaluate base of \\(\\lambda\\). Its numbers should be greater or equal to 0.\nSo, it makes sense that the value that we want to predict with our regression would be \\(\\lambda\\), by using next structure:\n\\[\n\\log{ \\left( \\lambda(X_1, X_2, \\dots , X_p)  \\right)} = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\nIf select this model we need to be aware how to interpret the coefficients. For example, if \\(\\beta_1 = -0.08\\) for a categorical variable, we can conclude by calculating \\(e^{-0.08}\\) that 92.31% of events of the base line related to \\(\\beta_0\\) would happen.\n\n5.5.1 Coding example\nTo perform Poisson Regression we just need to create the model specification by loading the poissonreg package and using glm engine.\n\nlibrary(poissonreg)\n\npois_spec &lt;- poisson_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glm\")\n\npois_rec_spec &lt;- lm_rec_spec\n\nworkflow() %&gt;%\n  add_model(pois_spec) %&gt;%\n  add_recipe(pois_rec_spec) %&gt;%\n  last_fit(split = BikeshareSpit) %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      69.7   Preprocessor1_Model1\n2 rsq     standard       0.725 Preprocessor1_Model1"
  },
  {
    "objectID": "generalized-linear-models.html#logistic-regression",
    "href": "generalized-linear-models.html#logistic-regression",
    "title": "\n5  Generalized linear models (GLM)\n",
    "section": "\n5.6 Logistic Regression",
    "text": "5.6 Logistic Regression\nIt models the probability (\\(p(X) = Pr(Y=1|X)\\)) that Y belongs to a particular category given some predictors by assuming that \\(Y\\) follows a Bernoulli Distribution. This model calculates the probability using the logistic function which produce a S form between 0 and 1:\n\\[\np(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X}}\n            {1+e^{\\beta_{0}+\\beta_{1}X}}\n\\]\n\n\n\n\nAs the functions returns probabilities is responsibility of the analyst to define a threshold to make classifications.\n\n5.6.1 Estimating coefficients\nTo estimate \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) the method used is called as maximum likelihood which consists in maximizing the likelihood function. It is important to clarify that the least squares approach is in fact a special case of maximum likelihood.\n\\[\n\\ell(\\beta_{0}, \\beta_{1}) = \\prod_{i:y_{i} = 1}p(x_{i})\\prod_{i':y_{i'} = 0}p(1-x_{i'})\n\\]\n\n5.6.2 Multiple regression\nWe also can generalize the logistic function as you can see bellow.\n\\[\np(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}\n            {1+e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}\n\\]\n\n5.6.3 Interpreting the model\nTo understand how each variable influence the probability \\(p(X)\\), we need to manipulate the logistic function until having a lineal combination on the right site.\n\\[\n\\underbrace{ \\log{ \\left( \\overbrace{\\frac{p(X)}{1 - p(X)}}^\\text{odds ratio} \\right)} }_\\text{log odds or logit} = \\beta_{0}+\\beta_{1}X\n\\]\nAs we can see, the result of the linear combination is the \\(\\log\\) of the odds ratio, known as log odd or logit.\nAn odds ratio of an event presents the likelihood that the event will occur as a proportion of the likelihood that the event won’t occur. It can take any value between \\(0\\) and \\(\\infty\\), where low probabilities are close to \\(0\\), higher to \\(\\infty\\) and equivalents ones are equals to 1. For example, if we have an \\(\\text{odds ratio} = 2\\), we can say that it’s 2 times more likely that the event happens rather than not.\nApplying \\(\\log{(\\text{odds ratio})}\\) makes easier to compare the effect of variables as values below 1 become negative numbers of the scale of possible numbers and 1 becomes 0 for non-significant ones. To have an idea, an odds ratio of 2 has the same effect as 0.5, which it’s hard to see at first hand, but if we apply the \\(\\log\\) to each value we can see that \\(\\log{(2)} = 0.69\\) and \\(\\log{(0.5)} = -0.69\\).\nAt end, \\(p(X)\\) will increase as \\(X\\) increases if \\(\\beta_{1}\\) is positive despite the relationship between each other isn’t a linear one.\nUnderstanding a confounding paradox\n\n\n\n\n\n\nSimple Regression\nMultiple Regression\n\n\n\n\n\n\n\nThe positive coeﬃcient for student indicates that for over all values of balance and income, a student is more likely to default than a non-student.\nThe negative coeﬃcient for student indicates that for a ﬁxed value of balance and income, a student is less likely to default than a non-student.\n\n\n\n\n\n\n\nThe problem relays on the fact that student and balance are correlated. In consequence, a student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!\n\n\n\n\nMultinomial Logistic Regression\nWe also can generalize the logistic function to support more than 2 categories (\\(K &gt; 2\\)) by defining by convention the last category \\(K\\) as a baseline.\nFor \\(k = 1, \\dotsc,K-1\\) we use function.\n\\[\nPr(Y = k|X= x) = \\frac{e^{\\beta_{k0}+\\beta_{k1}x_{1}+\\dots+\\beta_{kp}x_{p}}}\n                      {1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_{l1}x_{1}+\\dots+\\beta_{lp}x_{p}}}\n\\]\nFor \\(k=K\\), we use the function.\n\\[\nPr(Y = K|X= x) = \\frac{1}\n                      {1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}+\\beta_{l1}x_{1}+\\dots+\\beta_{lp}x_{p}}}\n\\] And after some manipulations we can show that \\(\\log\\) of the probability of getting \\(k\\) divided by the probability of the baseline is equivalent to a linear combinations of the functions parameters.\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|X= x)}{Pr(Y = K|X= x)} \\right)} = \\beta_{k0}+\\beta_{k1}x_{1}+\\dots+\\beta_{kp}x_{p}\n\\]\nIn consequence, each coefficient represent a measure of how much change the probability from the baseline probability.\n\n5.6.4 Model limitatios\nThere are models that could make better classifications when:\n\nThere is a substantial separation between the \\(Y\\) classes.\nThe predictors \\(X\\) are approximately normal in each class and the sample size is small.\nWhen the decision boundary is not lineal.\n\n5.6.5 Coding example\nTo perform Logistic Regression we just need to create the model specification by loading the discrim package and using MASS engine.\n\nSmarket_train &lt;- \n  Smarket %&gt;%\n  filter(Year != 2005)\n\nSmarket_test &lt;- \n  Smarket %&gt;%\n  filter(Year == 2005)\n\nlr_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nSmarketLrPredictions &lt;-\n  lr_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |&gt;\n  augment(new_data = Smarket_test) \n\n\nconf_mat(SmarketLrPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n\naccuracy(SmarketLrPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.560"
  },
  {
    "objectID": "generative-classification-models.html#linear-discriminant-analysis-lda",
    "href": "generative-classification-models.html#linear-discriminant-analysis-lda",
    "title": "\n6  Generative Models for Classiﬁcation\n",
    "section": "\n6.1 Linear Discriminant Analysis (LDA)",
    "text": "6.1 Linear Discriminant Analysis (LDA)\nThis model assumes that:\n\nThe density function of \\(X\\) for each \\(Y\\) class \\(f_{k}\\) follows a Normal (Gaussian) distribution within each class. Even though, it is often remarkably robust to model violations like Boolean variables.\n\n\\(X\\) has a different mean across all \\(Y\\) classes \\(\\mu_{1}^2 \\neq \\dots \\neq \\mu_{k}^2\\).\n\n\\(X\\) has a common variance across all \\(Y\\) classes \\(\\sigma_{1}^2 = \\dots = \\sigma_{k}^2\\).\n\nTo understand how the model calculates its parameters, let’s see the discriminant function when the number of predictors is \\(p=1\\) and the number of \\(Y\\) classes is \\(K=2\\).\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{ \\left( p_{x}(x) \\right)} \\\\\n              & = \\log{(\\pi_{k})}\n                - \\frac{\\mu_{k}^2}{2\\sigma^2}\n                + x \\cdot \\frac{\\mu_{k}}{\\sigma^2}\n\\end{split}\n\\]\nIn this function, it’s clear that a class \\(k\\) has more possibilities to be selected as mean of \\(x\\) for that particular class increases and its variance decreases. It is also important to take in consideration the effect of \\(\\log{(\\pi_{k})}\\), in consequence the proportion of classes also influence the results.\nIf we want to extend the model to work with \\(p \\geq 1\\) we also need to consider that:\n\nEach individual predictor follows a one-dimensional normal distribution\nThere is some correlation between each pair of predictors\n\nAs result, the discriminant function is:\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{\\pi_{k}}  - \\frac{1}{2} \\mu_{k}^T \\Sigma^{-1} \\mu_{k} \\\\\n                & \\quad + x^T \\Sigma^{-1} \\mu_{k}\n\\end{split}                      \n\\]\n\n\nWhere:\n\n\n\\(x\\) refers to a vector the current value of each \\(p\\) element.\n\n\\(\\mu\\) refers to a vector with the mean of each predictor.\n\n\\(\\Sigma\\) refers to the covariance matrix \\(p \\times p\\) of \\(\\text{Cov}(X)\\).\n\n\n\nThe model also can be extended to handle \\(K &gt; 2\\) after defining the \\(K\\) class as the baseline, we can extend the discriminant function to have the next form:\n\\[\n\\begin{split}\n\\delta_{k}(x) & = \\log{ \\left(\n                        \\frac{Pr(Y = k|K=x)}\n                             {Pr(Y=K|X=x)}\n                      \\right)} \\\\\n              & = \\log{ \\left( \\frac{\\pi_{k}}{\\pi_{K}} \\right)}\n                  - \\frac{1}{2} (\\mu_{k} + \\mu_{K})^T \\Sigma^{-1} (\\mu_{k} - \\mu_{K}) \\\\\n              & \\quad + x^{T} \\Sigma^{-1} (\\mu_{k} - \\mu_{K})\n\\end{split}\n\\] ### Coding example\nTo perform LDA we just need to create the model specification by loading the discrim package and using MASS engine.\n\nlibrary(tidymodels)\nlibrary(ISLR) \nlibrary(discrim)\n\nSmarket_train &lt;- \n  Smarket %&gt;%\n  filter(Year != 2005)\n\nSmarket_test &lt;- \n  Smarket %&gt;%\n  filter(Year == 2005)\n\nlda_spec &lt;- discrim_linear() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"MASS\")\n\nSmarketLdaPredictions &lt;-\n  lda_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |&gt;\n  augment(new_data = Smarket_test) \n\n\nconf_mat(SmarketLdaPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   35  35\n      Up     76 106\n\naccuracy(SmarketLdaPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.560"
  },
  {
    "objectID": "generative-classification-models.html#quadratic-discriminant-analysis-qda",
    "href": "generative-classification-models.html#quadratic-discriminant-analysis-qda",
    "title": "\n6  Generative Models for Classiﬁcation\n",
    "section": "\n6.2 Quadratic Discriminant Analysis (QDA)",
    "text": "6.2 Quadratic Discriminant Analysis (QDA)\nLike LDA, the QDA classiﬁer plugs estimates for the parameters into Bayes’ theorem in order to perform prediction results and assumes that:\n\nThe observations from each class are drawn from a Gaussian distribution\nEach class has its own covariance matrix, \\(X \\sim N(\\mu_{k}, \\Sigma_{k})\\)\n\n\nUnder this assumption, the Bayes classiﬁer assigns an observation \\(X = x\\) to the class for which \\(\\delta_{k}(x)\\) is largest.\n\\[\n\\begin{split}\n\\delta_{k}(x) = & \\quad \\log{\\pi_{k}}\n                - \\frac{1}{2} \\log{|\\Sigma_{k}|}\n                - \\frac{1}{2} \\mu_{k}^T \\Sigma_{k}^{-1}\\mu_{k} \\\\\n              & + x^T \\Sigma_{k}^{-1} \\mu_{k} \\\\\n              & - \\frac{1}{2} x^T \\Sigma_{k}^{-1} x\n\\end{split}                  \n\\]\nIn consequence, QDA is more flexible than LDA and has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes or when we need non-linear decision boundaries.\nThe model also can be extended to handle \\(K &gt; 2\\) after defining the \\(K\\) class as the baseline, we can extend the discriminant function to have the next form:\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|K=x)}{Pr(Y=K|X=x)} \\right)} =\na_k + \\sum_{j=1}^{p}b_{kj}x_{j} +\n      \\sum_{j=1}^{p} \\sum_{l=1}^{p} c_{kjl} x_{j}x_{l}\n\\]\nWhere \\(a_k\\), \\(b_{kj}\\) and \\(c_{kjl}\\) are functions of \\(\\pi_{k}\\), \\(\\pi_{K}\\), \\(\\mu_{k}\\), \\(\\mu_{K}\\), \\(\\Sigma_{k}\\) and \\(\\Sigma_{K}\\)\n\n6.2.1 Coding example\nTo perform QDA we just need to create the model specification by loading the discrim package and using MASS engine.\n\nqda_spec &lt;- discrim_quad() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"MASS\")\n\nSmarketQdaPredictions &lt;-\n  qda_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |&gt;\n  augment(new_data = Smarket_test) \n\n\nconf_mat(SmarketQdaPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   30  20\n      Up     81 121\n\naccuracy(SmarketQdaPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.599"
  },
  {
    "objectID": "generative-classification-models.html#naive-bayes",
    "href": "generative-classification-models.html#naive-bayes",
    "title": "\n6  Generative Models for Classiﬁcation\n",
    "section": "\n6.3 Naive Bayes",
    "text": "6.3 Naive Bayes\nTo estimate \\(f_{k}(X)\\) this model assumes that Within the kth class, the p predictors are independent (correlation = 0) and as consequence:\n\\[\nf_{k}(x) = f_{k1}(x_{1}) \\times f_{k2}(x_{2}) \\times \\dots \\times f_{kp}(x_{p})\n\\]\nEven thought the assumption might not be true, the model often leads to pretty decent results, especially in settings where n is not large enough relative to p for us to eﬀectively estimate the joint distribution of the predictors within each class. It has been used to classify text data, for example, to predict whether an email is spam or not.\nTo estimate the one-dimensional density function \\(f_{kj}\\) using training data we have the following options:\n\nWe can assume that \\(X_{j}|Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2)\\)\n\nWe can estimate the distribution by defining bins and creating a histogram\nWe can estimate the distribution by use a kernel density estimator\nIf \\(X_{j}\\) is qualitative, we can count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\n\nThe model also can be extended to handle \\(K &gt; 2\\) after defining the \\(K\\) class as the baseline, we can extend the function to have the next form:\n\\[\n\\log{ \\left( \\frac{Pr(Y = k|K=x)}{Pr(Y=K|X=x)} \\right)} =\n\\log{ \\left(\n        \\frac{\\pi_{k}}\n             {\\pi_{K}}\n      \\right)}\n+\n\\log{ \\left(\n        \\frac{\\prod_{j=1}^{p} f_{kj}(x_{j}) }\n             {\\prod_{j=1}^{p} f_{Kj}(x_{j}) }\n      \\right)}\n\\]\n\n6.3.1 The infrequent problem\nThe method has the problem that if you don’t an example for a particular event in your training set it would estimate the probability of that event as 0.\n\nThe solution to this problem involves adding a small number, usually ‘1’, to each event and outcome combination to eliminate this veto power. This is called the Laplace correction or Laplace estimator. After adding this correction, each Venn diagram now has at least a small bit of overlap; there is no longer any joint probability of zero.\n\n\n6.3.2 Pre-processing\nThis method works better with categories, so if your data has numeric data try to bin it in categories by:\n\nTurning an Age variable in the ‘child’ or ‘adult’ categories\nTurning geographic coordinates into geographic regions like ‘West’ or ‘East’\nTurning test scores into four groups by percentile\nTurning hour into ‘morning’, ‘afternoon’ and ‘evening’\nTurning temperature into ‘cold’, ‘warm’ and ‘hot’\n\nAs this method works really well when we have few examples and many predictors we can transform text documents into a Document Term Matrix (DTM) using a bag-of-words model with package like tidytext or tm.\n\n6.3.3 Coding example\nTo perform Naive Bayes we just need to create the model specification by loading the discrim package and using klaR engine. We can apply Laplace correction by setting Laplace = 1 in the parsnip::naive_Bayes function.\n\nnb_spec &lt;- naive_Bayes() %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"klaR\") %&gt;% \n  set_args(usekernel = FALSE) \n\nSmarketNbPredictions &lt;-\n  nb_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |&gt;\n  augment(new_data = Smarket_test) \n\nconf_mat(SmarketNbPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down  Up\n      Down   28  20\n      Up     83 121\n\naccuracy(SmarketNbPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.591"
  },
  {
    "objectID": "flexible-regression.html#based-on-linear-regression",
    "href": "flexible-regression.html#based-on-linear-regression",
    "title": "7  Flexible Regression Models",
    "section": "7.1 Based on linear regression",
    "text": "7.1 Based on linear regression\n\n7.1.1 Polynomial regression\nIt extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power.\nAs result, if the response is a numeric variable we can fit our model to follow the next form:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\n\n\n\n\n\nOn the other hand, we can use the logistic regression and apply the same structure to predict the probability of particular class:\n\\[\n\\Pr(y_i &gt; 250|x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d)}\n                          {1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d)}\n\\]\n\n\n\n\n\n\n\n7.1.2 Piecewise constant regression\nThey cut the range of a variable into K distinct regions (known as bins) in order to produce a qualitative variable. This has the eﬀect of ﬁtting a piecewise constant function.\nIf we define the cutpoints as \\(c_1, c_2, \\dots, c_K\\) in the range of X, we can create dummy variables to represent each range. For example, if \\(c_1 \\leq x_i &lt; c_2\\) is TRUE then \\(C_1(x_i) = 1\\) and then we need to repeat that process for each value of \\(X\\) and range. As result we can fit a lineal regression based on the new variables.\n\\[\ny_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i)  \\dots + \\beta_K C_K(x_i) + \\epsilon_i\n\\]\n\n\n\n\n\nOn the other hand, we can use the logistic regression and apply the same structure to predict the probability of particular class:\n\\[\n\\Pr(y_i &gt; 250|x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i)  \\dots + \\beta_K C_K(x_i)}\n                          {1 + \\exp(\\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i)  \\dots + \\beta_K C_K(x_i))}\n\\]\n\n\n\n\n\n\n\n7.1.3 Piecewise polynomials regression (Natural Spine)\nIt consist in ﬁtting separate low-degree polynomials over diﬀerent regions of X. For example, a piecewise cubic polynomial with a single knot at a point c takes the form.\n\\[\ny_i =\n  \\begin{cases}\n    \\beta_{01} + \\beta_{11} x_i + \\beta_{21} x_i^2 + \\beta_{31} x_i^3 + \\epsilon_i &  \\text{if } x_i&lt;c\\\\\n    \\beta_{02} + \\beta_{12} x_i + \\beta_{22} x_i^2 + \\beta_{32} x_i^3 + \\epsilon_i &  \\text{if } x_i \\geq c\n  \\end{cases}\n\\]\nAs each polynomial has four parameters, we are using a total of 8 degrees of freedom in ﬁtting that model. By using that model with Wagedata, we can see a problem as the model used was too flexible and to solve it we need to constrain it to be continuous at age = 50.\n But as you could see after applying the continuity constraint the plot still present an unnatural V-shape that can solve by apply the continuity constraint to the first and second derivative of the function, the end with 5 degrees of freedom model.\n\n\n\n\n\nIn this context, a natural spline refers to a regression spline with the additional constraints of maintaining linearity at the boundaries."
  },
  {
    "objectID": "flexible-regression.html#smoothing-splines",
    "href": "flexible-regression.html#smoothing-splines",
    "title": "7  Flexible Regression Models",
    "section": "7.2 Smoothing splines",
    "text": "7.2 Smoothing splines\nThey arise as a result of minimizing a residual sum of squares criterion subject to a smoothness penalty.\nIn this method rather than trying to minimize \\(\\text{RSS} = \\sum_{i=1}^n (y_i - g(x_i))^2\\), we try to find a function \\(g(x)\\), known as smoothing spline, which could minimize the following expression based on the \\(\\lambda\\) nonnegative tuning parameter.\n\\[\n\\underbrace{\\sum_{i=1}^n (y_i - g(x_i))^2 }_{\\text{loss function (data fitting)}} +\n\\underbrace{\\lambda \\int g''(t)^2dt}_{\\text{penalty term (g varibility)}}\n\\]\nThe second derivative of \\(g(t)\\) measure how wiggly is the function near \\(t\\), where its value is \\(0\\) when the function is a straight line as a line is perfectly smooth. The we can use integral to get total change in the function \\(g'(t)\\), over its entire range. As consequence, the larger the value of \\(\\mathbf{\\lambda}\\) , the smoother \\(\\mathbf{g}\\) will be.\n\n\n\n\n\n\nThe function \\(g(x)\\) is a natural cubic spline with knots at \\(x_1, \\dots ,x_n\\). As results the effective degrees of freedom (\\(df_{\\lambda} = \\sum_{i=1}^n \\{ \\mathbf{S}_{\\lambda} \\}_{ii}\\)) are between \\(n\\) and \\(2\\) depending on the value of \\(\\mathbf{\\lambda}\\).\n\n\n\nWe can use cross-validation to find the best value which can minimize the RSS. It turns out that the leave one-out cross-validation error (LOOCV) can be computed very eﬃciently for smoothing splines, with essentially the same cost as computing a single ﬁt, using the following formula:\n\\[\n\\text{RSS}_{cv} (\\lambda) = \\sum_{i = 1}^n (y_i - \\hat{g}_\\lambda^{(-i)} (x_i))^2 =\n                            \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{g}_{\\lambda} (x_i)}\n                                                       {1 - \\{ \\mathbf{S_{\\lambda}} \\}_{ii}}\n                                           \\right]^2\n\\]\nWhere:\n\n\\(\\hat{g}_\\lambda^{(-i)}\\): Refers to the function fitted without the ith observation \\((x_i, y_i)\\).\n\\(\\hat{g}_\\lambda\\): Refers the smoothing spline function ﬁtted to all of the training observations."
  },
  {
    "objectID": "flexible-regression.html#local-regression",
    "href": "flexible-regression.html#local-regression",
    "title": "7  Flexible Regression Models",
    "section": "7.3 Local regression",
    "text": "7.3 Local regression\nComputes the fit at target point \\(x_0\\) using only the nearly training observations by:\n\nGather the \\(s=k/n\\) closest (known as span) fraction of points. This step is very important as it controls the flexibility level can be selected using cross-validation*.\nAssign a weight \\(K_{i0} = K(x_i, x_0)\\) for each selected point based on the distance to \\(x_0\\). As lower is the distance as higher needs to be the weight.\nFind the coefficients which minimize the weighted least squares regression for the current \\(x_0\\) value.\n\n\\[\n\\sum_{i=1}^n = K_{i0}(y_i - \\beta_0 - \\beta_1x_i)^2\n\\]\n\nCalculate the fitted value of \\(x_0\\) using \\(\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1x_0\\).\n\nIn the next illustration we can see how the model works with some simulated data.\n\n\n\n\n\n\n\n\n\n\n\nIt performs poorly when we have more than 3 or 4 predictors in our model."
  },
  {
    "objectID": "flexible-regression.html#generalized-additive-models-gam",
    "href": "flexible-regression.html#generalized-additive-models-gam",
    "title": "7  Flexible Regression Models",
    "section": "7.4 Generalized Additive Models (GAM)",
    "text": "7.4 Generalized Additive Models (GAM)\nThey provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.\nTo transform each predictor we have the next options:\n\nA constant for each categorical level (step function)\nPolynomial regression\nNatural spines (optimized with least squares)\nSmoothing splines (optimized with backﬁtting)\nLocal regression\n\n\n\n\n\n\n\nBackﬁtting ﬁts a model involving multiple predictors by repeatedly updating the ﬁt for each predictor in turn, holding the others ﬁxed.\n\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\n1. It finds relationships that a lineal model would miss without applying many transformations as it can fit a non-linear \\(f_j\\) to each \\(X_j\\)   2. We can examine the eﬀect of each predictor on \\(Y\\) individually as the model is additive.   3. The smoothness of each function \\(f_j\\) can be summarized via degrees of freedom.\n1. Important interactions can be missed, but they can be added manually\n\n\n\n\n7.4.1 GAM Regression\nTo predict a numeric variable this method creates a function with the next form:\n\\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\dots + f_p(x_{ip}) + \\epsilon_i\n\\]\nBy taking the other predictor as constant we can plot effect of each function for each predictor in the Wage example:\n\n\n\n\n\n\n\n7.4.2 GAM Classification\nTo predict a categorical variable this method creates a function with the next form:\n\\[\n\\log \\left( \\frac{p(X)}{1-p(X)} \\right)= \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\dots + f_p(x_{ip})\n\\]\nBy taking the other predictor as constant we can plot effect of each function for each predictor in the Wage example:"
  },
  {
    "objectID": "non-parametric-models.html#k-nearest-neighbors-knn",
    "href": "non-parametric-models.html#k-nearest-neighbors-knn",
    "title": "\n8  Non-parametric Methods\n",
    "section": "\n8.1 K-nearest neighbors (KNN)",
    "text": "8.1 K-nearest neighbors (KNN)\nIt uses the principle of nearest neighbors to classify unlabeled examples by using the Euclidean Distance to calculate distance between the point we want to predict and \\(k\\) closest neighbors on the training data.\n\\[\nd\\left( a,b\\right)   = \\sqrt {\\sum _{i=1}^{p}  \\left( a_{i}-b_{i}\\right)^2 }\n\\]\nKNN unlike parametric models does not tell us which predictors are important, making it hard to make inferences using this model.\nThis method performs worst than a parametric as we starting adding noise predictors. In fact, we will get in the situation where for a given observation has no nearby neighbors, known as curse of dimensionality and leading to a very poor prediction of \\(f(x_{0})\\).\n\n8.1.1 Classiﬁer\nThe next function estimates the conditional probability for class \\(j\\) as the fraction of points in \\(N_{0}\\) whose response values equal \\(j\\).\n\\[\n\\text{Pr}(Y = j|X = x_{0}) = \\frac{1}{K}\n                      \\displaystyle\\sum_{i \\in N_{0}} I(y_{i} = j)\n\\]\n\nWhere\n\n\n\\(j\\) response value to test\n\n\\(x_{0}\\) is the test observation\n\n\\(K\\) the number of points in the training data that are closest to \\(x_{0}\\) and reduce the model flexibility\n\n\\(N_{0}\\) points in the training data that are closest to \\(x_{0}\\)\n\n\n\n\nThen KNN classiﬁes the test observation \\(x_{0}\\) to the class with the largest probability.\n\n\n\n\n\n8.1.2 Regression\nKNN regression estimates \\(f(x_{0})\\) using the average of all the training responses in \\(N_{0}\\).\n\\[\n\\hat{f}(x_{0}) = \\frac{1}{K}\n                      \\displaystyle\\sum_{i \\in N_{0}} y_{i}\n\\]\n\nWhere\n\n\n\\(x_{0}\\) is the test observation\n\n\\(K\\) the number of points in the training data that are closest to \\(x_{0}\\) and reduce the model flexibility\n\n\\(N_{0}\\) points in the training data that are closest to \\(x_{0}\\)\n\n\n\n\n8.1.3 Pre-processing\nTo use this method we need to make sure that all our variables are numeric. If one our variables is a factor we need to perform a dummy transformation of that variable with the recipes::step_dummy function.\nOn the other hand, as this model uses distances to make predicts it’s important to check that each feature of the input data is measured with the same range of values with the recipes::step_range function which normalize from 0 to 1 as happens with the dummy function.\n\\[\nx' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\nAnother normalization alternative is centering the predictors in \\(\\overline{x} = 0\\) with \\(S = 0\\) with the function recipes::step_normalize or the function scale() which apply the z-score normalization.\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\n8.1.4 Coding example\nTo perform K-Nearest Neighbors we just need to create the model specification by using kknn engine.\n\nlibrary(tidymodels)\nlibrary(ISLR2)\n\nSmarket_train &lt;- \n  Smarket %&gt;%\n  filter(Year != 2005)\n\nSmarket_test &lt;- \n  Smarket %&gt;%\n  filter(Year == 2005)\n\nknn_spec &lt;- nearest_neighbor(neighbors = 3) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"kknn\")\n\nSmarketKnnPredictions &lt;-\n  knn_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket_train) |&gt;\n  augment(new_data = Smarket_test) \n\nconf_mat(SmarketKnnPredictions, truth = Direction, estimate = .pred_class) \n\n          Truth\nPrediction Down Up\n      Down   43 58\n      Up     68 83\n\naccuracy(SmarketKnnPredictions, truth = Direction, estimate = .pred_class) \n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary           0.5"
  },
  {
    "objectID": "non-parametric-models.html#tree-based-methods",
    "href": "non-parametric-models.html#tree-based-methods",
    "title": "\n8  Non-parametric Methods\n",
    "section": "\n8.2 Tree-Based Methods",
    "text": "8.2 Tree-Based Methods\nThese methods involve stratifying the predictor space into a number of simple regions and then use mean or the mode response value for the training observations in the region to which it belongs.\n\n\n\n\nAs these results can be summarized in a tree, these types of approaches are known as decision tree methods, we have some important parts:\n\n\nTerminal nodes (leaves) are represented by \\(R_1\\), \\(R_2\\) and \\(R_3\\).\n\nInternal nodes refers to the points along the tree where the predictor space is split.\n\nBranches refer to the segments of the trees that connect the nodes.\n\n\n\n\n\nIt’s important to take in consideration that the order in which is presented each predictors also explain the level of importance of each variable. For example, the number of Years has a higher effect over the player’s salary than the number of Hits.\n\n8.2.1 Simple trees\nAdvantages and disadvantages\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\nSimpler to explain than linear regression thanks to its graphical representation\nSmall change in the data can cause a large change in the final estimated tree\n\n\nIt doesn’t need much preprocessing as:  - It handles qualitative predictors.  - It doesn’t require feature scaling or normalization.  - It can handle missing values and outliers\nThey aren’t so very good predicting results as:  - It prones to overfitting.  - It presents low accuracy.\n\n\nIt can be used for feature selection by defining the importance of a feature based on how early it appears in the tree and how often it is used for splitting\nThey can be biased towards the majority class in imbalanced datasets\n\n\nRegression\nTo create a decision tree we need to find the regions \\(R_1, \\dots, R_j\\) that minimize the RSS where \\(\\hat{y}_{R_j}\\) represent the mean response for the training observations within the jth box:\n\\[\nRSS = \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\n\\]\nTo define the regions we use the recursive binary splitting, which consist the predictor \\(X_j\\) and the cutpoint \\(s\\) leads to the greatest possible reduction in RSS. Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. The process continues until a stopping criterion is reached (no region contains more than five observations).\nThis method is:\n\nTop-down: It begins at the top of the tree (where all observations belong to a single region) and then successively splits the predictor space.\nGreedy: At each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n\nAs result, we could end with a very complex tree that overfits the data. To solve this, we need prune the original tree until getting a subtree that leads to the lowest test error rate by using the cost complexity pruning approach which creates differente trees based on \\(\\alpha\\).\n\\[\n\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m}) ^2  + \\alpha|T|\n\\]\nWhere:\n\n\n\\(\\alpha\\): Tunning parameter \\([0,\\infty]\\) selected using k-cross validation\n\n\n\\(|T|\\): Number of terminal nodes of the tree \\(T\\).\n\n\\(R_m\\): The subset of predictor space corresponding to the \\(m\\)th terminal node\n\n\\(\\hat{y}_{R_m}\\): Predicted response associated with \\(R_m\\)\n\n\n\n\n\n\nClassification\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\nAs we can not use RSS as a criterion for making the binary splits, the classification error rate could the fraction of the training observations in that region that do not belong to the most common class (\\(1 - \\max_k(\\hat{p}_{mk})\\)), but it turns out that classification error is not sufficiently sensitive for tree-growing and we use the next metrics as they are more sensitive to node purity (proportion of the main class on each terminal node):\n\n\n\n\n\n\nName\nFormula\n\n\n\nGini index\n\\(G = \\sum_{k = 1}^K 1 - \\hat{p}_{mk} (1 -\\hat{p}_{mk})\\)\n\n\nEntropy\n\\(D = -\\sum_{k = 1}^K 1 - \\hat{p}_{mk} \\log \\hat{p}_{mk}\\)\n\n\n\n\n\n\n\nCoding example\nhttps://app.datacamp.com/learn/tutorials/decision-trees-R\n\n# For data maninulation\nlibrary(data.table)\n\n# For modeling and visualization\nlibrary(tidymodels)\n\n#\nBoston &lt;- as.data.table(MASS::Boston)\n\npillar::glimpse(Boston)\n\nRows: 506\nColumns: 14\n$ crim    &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,…\n$ zn      &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1…\n$ indus   &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.…\n$ chas    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ nox     &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,…\n$ rm      &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,…\n$ age     &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9…\n$ dis     &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505…\n$ rad     &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ tax     &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31…\n$ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15…\n$ black   &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90…\n$ lstat   &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10…\n$ medv    &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15…\n\n\n\n8.2.2 Bagging (bootstrap aggregation)\nAs we said before, simple trees has a high variance problem \\(Var(\\hat{f}(x_{0}))\\) and bagging can help to mitigate this problem.\nWe know from the Central Limit Theorem a natural way to reduce the variance and increase the test set accuracy is taking many training sets from the population, build a separate prediction model using each training set, and average the resulting prediction, as for a given a set of \\(n\\) independent observations \\(Z_1, \\dots, Z_n\\), each with variance \\(\\sigma^2\\), the variance of the mean \\(\\overline{Z}\\) of the observations is given by \\(\\sigma^2/n\\).\n\n\n\n\nAs we generally do not have access to multiple training sets we use bootstrap to take repeated samples from the one training data set, train \\(B\\) not pruned regression trees and average the resulting predictions or select the most commonly occurring class among the \\(B\\) predictions in classification settings.\n\\[\n\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)\n\\]\nThe number of trees is not a critical as \\(B\\) will not lead to overfitting. Using \\(B = 100\\) is sufficient to achieve good performance in this example.\nOut-of-bag error estimation\nTo estimate the test error as an approximation of the Leave-one-out cross validation when \\(B\\) sufficiently large, we can take advantage of the \\(1/3\\) of observation that were out-of-bag (OOB) on each re-sample and predict the response for the \\(i\\)th observation using each of the trees in which that observation was OOB.\nThis will yield around \\(B/3\\) predictions for each of the \\(n\\) observation that we can average or take a majority vote to calculate the test error.\nVariable importance measures\nAfter using this method, we can’t represent the statistical learning procedure using a single tree, instead we can use the RSS (or the Gini index) to record the total amount that the RSS is decreased due to splits over a given predictor, averaged(or added) over all \\(B\\) trees where a large value indicates an important predictor.\n\n\n\n\n\n8.2.3 Random Forests\nPredictions from the bagged trees, has a big problem, there are highly correlated and averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities (independent).\nTo solve this problem Random Forests provide an improvement over bagged trees by decorrelates the trees. As in bagging, we build many trees based on bootstrapped training samples. But when building these decision trees random forest sample \\(m \\approx \\sqrt{p}\\) predictors to create \\(B\\) *independent trees, making the average of the resulting trees less variable and hence more reliable.\n\n\n\n\nParameters to tune\nRandom forests have the least variability in their prediction accuracy when tuning, as the default values tend to produce good results.\n\n\nNumber of trees (\\(B\\)): A good rule of thumb is to start with 10 times the number of features as the error estimate converges after some trees and computation time increases linearly with the number of trees.\n\n\n\n\n\n\n\nNumber of predictors (\\(m_{try}\\)): In ranger, \\(m_{try} = \\text{floor} \\left( \\frac{p}{3} \\right)\\) in regression problems and \\(m_{try} = \\text{floor} \\left( \\sqrt{p} \\right)\\) in classifications problems, but we can explore in the range \\([2,p]\\).\n\n\nWith few relevant predictors (e.g., noisy data) a higher number tends to perform better because it makes it more likely to select those features with the strongest signal.\nWith many relevant predictors a lower number might perform better.\n\n\n\n\n\n\n\nTree complexity (node size): The default values of \\(1\\) for classification and \\(5\\) for regression as these values tend to produce good results, but:\n\n\nIf your data has many noisy predictors and a high number of trees, then performance may improve by increasing node size (i.e., decreasing tree depth and complexity).\nIf computation time is a concern then you can often decrease run time substantially by increasing the node size.\n\nStart with three values between 1–10 and adjust depending on impact to accuracy and run time.\n\n\n\n\n\n8.2.4 Boosting\nAs well as bagging, boosting is a general approach that can be be applied to many statistical learning methods for regression or classification. Both methods create many models in order to create a single prediction \\(\\hat{f}^1, \\dots, \\hat{f}^B\\).\nTo create boosting trees, in general, we need to create then sequentially by using information from prior models and fit a new model with a modified version of the original data set. To be more specific we need to flow the following steps:\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i=y_i\\) for all \\(i\\) in the training set.\nFor \\(b = 1, 2, \\dots, B\\), repeat the next process:\n\n\nFit a tree \\(\\hat{f}^b\\) with d splits (d + 1 terminal nodes) to the training data (\\(X,r\\)). In this step is very import to see that we are fitting the new model based on the residuals.\nUpdate \\(\\hat{f}\\) by adding in a shrunken version of the new tree:\n\n\\[\n\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\n\\]\n\nUpdate the residuals\n\n\\[\nr_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\n\\]\n\nCalculate the output of the boosting model by:\n\n\\[\n\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x)\n\\]\nAs result, our model learn slowly by adding new decision trees into the fitted function in order to update the residuals on each step. As we are going to use many models, each individual model can be small by using a low \\(d\\) parameter. In general, statistical learning approaches that learn slowly tend to perform well.\nParameters to tune\n\nNumber of trees (\\(B\\)): Unlike bagging and random forests, boosting can overfit if B is too large.\nshrinkage (\\(\\lambda\\)): Controls the rate at which boosting learns, it should be a positive value its values are \\(0.01\\) or \\(0.001\\). Very small \\(\\lambda\\) can require using a very large value of B in order to achieve good performance.\nNumber of splits or interaction depth (\\(d\\)): It controls the complexity of the boosted ensemble. When \\(d=1\\) is known as a stump tree as each term involves only a single variable. Some times, stump tree works well and are eraser to interpret, but as \\(d\\) increases the number of variables used by each model increases, with \\(d\\) as limit.\n\n\n\n\n\n\n8.2.5 Bayesian additive regression trees (BART)\nThis method constructs trees in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.\nTo understand the method works we need to define some important notation:\n\n\n\\(K\\): Number of regression trees. For example \\(K\\) could be \\(100\\)\n\n\n\\(B\\): Number of iterations. For example \\(B\\) could be \\(1000\\)\n\n\n\\(\\hat{f}_k^b(x)\\): The prediction at \\(x\\) for the \\(k\\)th regression tree used in the \\(b\\)th iteration\n\n\\(\\hat{f}^b(x) = \\sum_{k=1}^K \\hat{f}_k^b(x)\\): Summed of the \\(K\\) at the end of each iteration.\n\nTo apply this method we need to follow the below steps:\n\nIn the first iteration all trees are initialized to have a single root node,the mean of the response values divided by the total number of trees, in other to predict the mean of \\(y\\) in the first iteration \\(\\hat{f}^1(x)\\).\n\n\\[\n\\hat{f}_k^1(x) = \\frac{1}{nK} \\sum_{i=1}^n y_i\n\\]\n\nCompute the predictions of the first iteration.\n\n\\[\n\\hat{f}^1(x) = \\sum_{k=1}^K \\hat{f}_k^1(x) = \\frac{1}{n} \\sum_{i=1}^n y_i\n\\]\n\nFor each of the following iterations \\(b = 2, \\dots, B\\).\n\n\n\nUpdate each tree \\(k = 1, 2, \\dots, K\\) by:\n\nComputing a partial residual for each tree with all trees but the \\(k\\)th tree.\n\n\n\n\\[\nr_i = y_i - \\sum_{k'&lt;k} \\hat{f}_{k'}^b(x_i) - \\sum_{k'&gt;k} \\hat{f}_{k'}^{b-1}(x_i)\n\\]\n\n\nBased on the partial residual BART randomly choosing a perturbation to the tree from the previous iteration \\(\\hat{f}_k^{b-1}\\) from a set of possible perturbations (adding branches, prunning branches or changing the prediction of terminal nodes) favoring ones that improve the fit to the partial residual. This guards against overfitting since it limits how “hard” we fit the data in each iteration.\n\nCompute \\(\\hat{f}^b(x) = \\sum_{k=1}^K \\hat{f}_k^b(x)\\)\n\n\n\nCompute the mean or a percentile after \\(L\\) burn-in iterations that don’t provide good results. For example \\(L\\) could be \\(200\\)\n\n\n\\[\n\\hat{f}(x) = \\frac{1}{B-L} \\sum_{b=L+1}^B \\hat{f}(x)\n\\]\nThis models works really well even without a tuning process and the random process modifications protects the metho to overfit as we increase the number of iterations, as we can see in the next chart."
  },
  {
    "objectID": "survival-analysis.html#general-concepts",
    "href": "survival-analysis.html#general-concepts",
    "title": "\n9  Time-event (Survival) Analysis and Censored Data\n",
    "section": "\n9.1 General concepts",
    "text": "9.1 General concepts\nIn this models the outcome variable is the time until an event occurs or any other numeric variable have been censored by any limitation during data collection. In consecuence, times are always positive and we need to use distributions like the Weibull distribution.\n\n\nSurvival, failure or event time \\(T\\): Represents the time at which the event of interest occurs. For instance, the time at which the patient dies or the customer cancels his or her subscription.\n\nCensoring time \\(C\\): Represents the time at which censoring occurs. For example, the time at which the patient drops out of the study or the study ends.\n\nAs result our target variable is the result of:\n\\[\nY = \\min(T, C)\n\\]\nTo know how to interpret the results we will need an indicator:\n\\[\n\\delta =\n\\begin{cases}\n   1 & \\quad \\text{if } T \\leq C \\\\\n   0 & \\quad \\text{if } T &gt; C\n\\end{cases}\n\\]\nAs result when \\(\\delta = 1\\) we observe the true survival time, and when \\(\\delta = 0\\) if we observe the censoring time. In the next example, we just could observe the event for patients 1 and 3 before ending the study.\n\n\n\n\n\n9.1.1 Case of use\n\nThe time needed for the individual to find a job again\nThe time needed for a letter to be delivered\nThe time needed for a cab to pick you up at your house after having called the cab company\nThe time needed for a customer to churn.\nWhen our measure instrument cannot report values above a certain number.\n\n9.1.2 Assumptions\nIn order to analyze survival data, we need to determine whether the following assumptions are reasonable:\n\nThe event time \\(T\\) is independent of the censoring time \\(C\\). For example, patients drop out of the cancer study early because they are very sick, that would overestimate the true average survival time. We can check this assumption by exploring the reasons related to dropouts.\nThe predictors are independent of the censoring event \\(\\delta = 0\\). For example, if in our study very sick males are more likely to drop out of the study than very sick females, that would drive the wrong conclusion that males survive longer than females.\n\n9.1.3 Censoring types\n\n\nRight censoring: It occurs when \\(T \\geq Y\\), i.e. the true event time \\(T\\) is at least as large as the observed time \\(Y\\).\n\nLeft censoring: It occurs when \\(T \\leq Y\\), i.e. the true event time \\(T\\) is less than or equal to the observed time \\(Y\\).\n\nInterval censoring: It refers to the setting in which we do not know the exact event time, but we know that it falls in some interval.\n\nRight censoring will be the main focus of this chapter."
  },
  {
    "objectID": "survival-analysis.html#kaplan-meier-survival-curve",
    "href": "survival-analysis.html#kaplan-meier-survival-curve",
    "title": "\n9  Time-event (Survival) Analysis and Censored Data\n",
    "section": "\n9.2 Kaplan-Meier Survival Curve",
    "text": "9.2 Kaplan-Meier Survival Curve\nThe survival curve (function) is defined as the probability that the event time \\(T\\) happens later than a time \\(t\\). As result, the larger the value of \\(S(t)\\), the less likely that the event would take place before time \\(t\\).\n\\[\nS(t) = \\Pr(T &gt; t)\n\\]\nTo explain how complicated can be to estimate \\(S(20) = \\Pr(T&gt;20)\\) when our target variable is a mix of event and censored times we will use the ISLR2::BrainCancer table as a example.\n\nlibrary(data.table)\n\nBrainCancer &lt;- \n  na.omit(ISLR2::BrainCancer) |&gt;\n  as.data.table()\n\npillar::glimpse(BrainCancer)\n\nRows: 87\nColumns: 8\n$ sex       &lt;fct&gt; Female, Male, Female, Female, Male, Female, Male, Male, Fema…\n$ diagnosis &lt;fct&gt; Meningioma, HG glioma, Meningioma, LG glioma, HG glioma, Men…\n$ loc       &lt;fct&gt; Infratentorial, Supratentorial, Infratentorial, Supratentori…\n$ ki        &lt;int&gt; 90, 90, 70, 80, 90, 80, 80, 80, 70, 100, 80, 90, 90, 60, 70,…\n$ gtv       &lt;dbl&gt; 6.11, 19.35, 7.95, 7.61, 5.06, 4.82, 3.19, 12.37, 12.16, 2.5…\n$ stereo    &lt;fct&gt; SRS, SRT, SRS, SRT, SRT, SRS, SRT, SRT, SRT, SRT, SRT, SRS, …\n$ status    &lt;int&gt; 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, …\n$ time      &lt;dbl&gt; 57.64, 8.98, 26.46, 47.80, 6.30, 52.75, 55.80, 42.10, 34.66,…\n\nBrainCancer[, .N,\n            keyby = .(alive_after_20_months = time &gt; 20,\n                      event_time = status)]\n\nKey: &lt;alive_after_20_months, event_time&gt;\n   alive_after_20_months event_time     N\n                  &lt;lgcl&gt;      &lt;int&gt; &lt;int&gt;\n1:                 FALSE          0    17\n2:                 FALSE          1    23\n3:                  TRUE          0    35\n4:                  TRUE          1    12\n\n\n\nTaking the total of patients who were alive after 20 months (\\(36+12=48\\)) over the total number of patients (\\(48/88 \\approx 55\\%\\)) would be a mistake as we cannot assume that event time is lower than 20 (\\(T &lt; 20\\)) for 17 censored patients.\nOmitting the \\(17\\) censored patients might sound as solution but that would under estimate the probability as a patient who was censored at \\(t = 19.9\\) likely would have survived past \\(t = 20\\), and would be better to take a advantage of that censored time.\n\nThe Kaplan-Meier method shares a solution to this problem.\n\n9.2.1 Empirical explanation\n\n\n\n\n\n\n9.2.2 Mathematical explanation\nLet’s start defining some important elements:\n\n\n\\(d_1 &lt; d_2 &lt; \\dots &lt; d_K\\): The \\(K\\) unique death times among the non-censored patients.\n\n\\(q_k\\): The number of patients who died at time \\(d_k\\).\n\n\\(r_k\\): The number of patients alive and in the study just before \\(d_k\\), at risk patients.\n\n\\(\\widehat{\\Pr}(T &gt; d_j|T &gt; d_{j-1}) = (r_j - q_j) / r_j\\): It estimates the fraction of the risk set at time \\(d_j\\) who survived past time \\(d_j\\).\n\nAnd based on the law of total probability\n\\[\n\\begin{split}\n\\Pr(T &gt; d_k) = & \\Pr(T &gt; d_k|T &gt; d_{k-1}) \\Pr(T &gt; d_{k-1}) + \\\\\n               & \\Pr(T &gt; d_k|T \\leq d_{k-1}) \\Pr(T \\leq d_{k-1})\n\\end{split}\n\\]\nBut as it is impossible for a patient to survive past time \\(d_k\\) if he or she did not survive until an earlier time \\(d_{k−1}\\), we know that \\(\\Pr(T &gt; d_k|T \\leq d_{k-1}) = 0\\) and we can simplify the function and found out that this a recurrent function.\n\\[\n\\begin{split}\n\\Pr(T &gt; d_k) = & \\Pr(T &gt; d_k|T &gt; d_{k-1}) \\Pr(T &gt; d_{k-1}) \\\\\nS(d_k) = & \\Pr(T &gt; d_k|T &gt; d_{k-1}) \\times \\dots \\times \\Pr(T &gt; d_2|T &gt; d_1) \\Pr(T &gt; d_1) \\\\\n\\hat{S}(d_k) = & \\prod_{j=1}^k \\left( \\frac{r_j - q_j}{r_j} \\right)\n\\end{split}\n\\]\nAs we can see the the example below, the Kaplan-Meier survival curve has a step-like shape as we assume that \\(\\hat{S}(t) = \\hat{S}(d_k)\\) when \\(d_k &lt; t &lt; d_{k+1}\\).\n\n\n\n\nBased on this new function we can say that the probability of survival past 20 months is \\(71\\%\\).\n\n9.2.3 Interpreting Survival Curve\nIf select a \\(t\\) value on the \\(x\\)-axis we can answer the flowing questions:\n\nWhat is the probability that a breast cancer patient survives longer than 5 years? \\(S(5)\\)\n\nOut of 100 unemployed people, how many do we expect to have a job again after 2 months? \\(1 - S(2)\\)\n\n\nBut it also works in the other sense by selecting a quantile on the \\(y\\)-axis and the checking the related value on the \\(x\\)-axis, answer\nWhat is the typical waiting time for a cab?\n\n\n\n\n\n9.2.4 Coding example\n\nlibrary(survival)\n\nBrainCancerSv &lt;- survfit(\n  Surv(time, status) ~ 1,\n  data = BrainCancer\n)\n\nsurvminer::surv_summary(BrainCancerSv) |&gt;\n  subset(select = time:surv) |&gt;\n  head()\n\n  time n.risk n.event n.censor      surv\n1 0.07     87       1        0 0.9885057\n2 1.18     86       0        1 0.9885057\n3 1.41     85       1        0 0.9768763\n4 1.54     84       0        1 0.9768763\n5 2.03     83       0        1 0.9768763\n6 3.38     82       1        0 0.9649631\n\nbroom::tidy(BrainCancerSv) |&gt;\n  base::subset(select = time:estimate) |&gt;\n  head()\n\n# A tibble: 6 × 5\n   time n.risk n.event n.censor estimate\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1  0.07     87       1        0    0.989\n2  1.18     86       0        1    0.989\n3  1.41     85       1        0    0.977\n4  1.54     84       0        1    0.977\n5  2.03     83       0        1    0.977\n6  3.38     82       1        0    0.965\n\n\nWe can plot this results using the survminer package:\n\nlibrary(survminer)\n\nggsurvplot(\n  BrainCancerSv,\n  palette = \"blue\",\n  surv.median.line = \"hv\",\n  conf.int = FALSE,\n  risk.table = \"nrisk_cumevents\",\n  legend = \"none\",\n  break.x.by = 10,\n  title = \"Brain Cancer Survival Curve\"\n)\n\n\n\n\nSome additional options to consider:\n\n\npalette: Can be used to define the colors of the curves.\n\nlinetype: Can be used to define the linetype of the curves.\n\nsurv.median.line: Can highlight the median survival time.\n\nrisk.table: Shows a table with the number of subjects at risk of dying.\n\ncumevents: Shows a table with the number of events that happened already.\n\ncumcensor: Shows a table with the number of censored observations so far.\n\ntables.height: Indicate how big the tables should be.\n\nYou event can compare to survival curves. To use the same data let’s assume for one curve that we are reporting the event time for all observations.\n\nBrainCancerSvWrong &lt;- survfit(\n  Surv(time) ~ 1,\n  data = BrainCancer\n)\n\nggsurvplot_combine(\n  list(correct = BrainCancerSv, \n       wrong = BrainCancerSvWrong)\n)\n\n\n\n\nAs results we would unter estimate the probability of survive."
  },
  {
    "objectID": "survival-analysis.html#weibull-model-survival-curve",
    "href": "survival-analysis.html#weibull-model-survival-curve",
    "title": "\n9  Time-event (Survival) Analysis and Censored Data\n",
    "section": "\n9.3 Weibull Model Survival Curve",
    "text": "9.3 Weibull Model Survival Curve\nThe Weibull model can produce a smooth survival curve by using some assumptions about the distribution, but it is very useful when we are adjusting the results based on covariates or making inferences.\n\n9.3.1 Coding example\n\n# Defining the probabilities to plot\n\nSurvivalProb &lt;- seq(.99, .01, by = -.01)\n\nplot(SurvivalProb)\n\n\n\n# Fitting the distribution\n\nBrainCancerWSC &lt;- survreg(\n  Surv(time, status) ~ 1,\n  data = BrainCancer,\n  dist = \"weibull\"\n)\n\n\n# Predicting times\n\nBrainCancerWmTime &lt;- predict(\n  BrainCancerWSC,\n  type = \"quantile\",\n  p = 1 - SurvivalProb,\n  newdata = data.frame(1)\n)\n\n\n# Create a data.frame an plot the results\n\ndata.frame(\n  time = BrainCancerWmTime,\n  surv = SurvivalProb, \n  upper = NA,\n  lower = NA,\n  std.err = NA\n) |&gt;\n  ggsurvplot_df(surv.geom = geom_line)"
  },
  {
    "objectID": "survival-analysis.html#log-rank-test",
    "href": "survival-analysis.html#log-rank-test",
    "title": "\n9  Time-event (Survival) Analysis and Censored Data\n",
    "section": "\n9.4 Log-Rank Test",
    "text": "9.4 Log-Rank Test\nIf we want to explore whether the sex is an important factor to impact the survival curve we can create a plot comparing the curve of each sex.\n\n\n\n\nFemales seem to fare a little better up to about 50 months, but then the two curves both level off to about 50%. To take a better decision we need to confirm if this difference was produced by chance or if it was statistical significant.\nAs our target variable time mixes event and censoring times we need to use a particular statistical test known as log-rank test, Mantel-Haenszel test or Cochran-Mantel-Haenszel test. In this test we need to split some the variables used in the Kaplan-Meier survival curve.\n\\[\n\\begin{split}\nr_{1k} + r_{2k} & = r_k \\\\\nq_{1k} + q_{2k} & = q_k\n\\end{split}\n\\]\nIn order to test \\(H_0 : \\text{E}(X) = 0\\) for some random variable \\(X\\), one approach is to construct a test statistic of the form\n\\[\nW = \\frac{X - \\text{E}(X)}{\\sqrt{\\text{Var}(X)}}\n\\]\nWhere:\n\\[\n\\begin{split}\nX & = \\sum_{k=1}^K q_{1k} \\\\\n\\text{E}(q_{1k}) & = \\frac{r_{1k}}{r_k} q_k \\\\\n\\text{Var}\\left( X \\right) \\approx \\sum_{k=1}^K \\text{Var} (q_{1k}) & = \\sum_{k=1}^K \\frac{q_k(r_{1k}/r_k) (1-r_{1k}/r_k) (r_k-q_k)}{r_k-1}\n\\end{split}\n\\]\nAs result\n\\[\n\\begin{split}\nW & = \\frac{\\sum_{k=1}^K(q_{1k}-\\text{E}(q_{1k}))}\n           {\\sqrt{\\sum_{k=1}^K \\text{Var} (q_{1k})}} \\\\\n& = \\frac{\\sum_{k=1}^K(q_{1k}- \\frac{r_{1k}}{r_k} q_k)}\n         {\\sqrt{\\sum_{k=1}^K \\frac{q_k(r_{1k}/r_k) (1-r_{1k}/r_k) (r_k-q_k)}{r_k-1}}}\n\\end{split}\n\\]\nWhen the sample size is large, the log-rank test statistic \\(W\\) has approximately a standard normal distribution and can be used to compute a p-value for the null hypothesis that there is no difference between the survival curves in the two groups.\nFor the ISLR2::BrainCancerthe p-value is 0.2 using the theoretical null distribution. Thus, we cannot reject the null hypothesis of no difference in survival curves between females and males."
  },
  {
    "objectID": "survival-analysis.html#regression-models-with-a-survival-response",
    "href": "survival-analysis.html#regression-models-with-a-survival-response",
    "title": "\n9  Time-event (Survival) Analysis and Censored Data\n",
    "section": "\n9.5 Regression Models With a Survival Response",
    "text": "9.5 Regression Models With a Survival Response\nFitting a linear regression to a censored data can be challenging as we want to predict \\(T\\) rather than \\(Y\\) and to overcome this difficulty need to use a sequential construction.\n\n9.5.1 Hazard Function\nThe hazard function also known as hazard rate or force of mortality is useful to estimate the risk of an event and measures the instantaneous rate (conditional probability/unit of time) at which events occur given that the event has not yet occurred for the subjects under study.\n\\[\nh(t) = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\Pr(t &lt; T \\leq t + \\Delta t| T &gt; t)}{\\Delta t}\n\\]\nWhere:\n\n\n\\(T\\): It is the (unobserved) survival time.\n\n\\(\\Delta t\\): It’s an extremely tiny number.\n\nIt has a close relational with the probability density function of \\(T\\) which shows how common or rare is any particular \\(T\\) value is likely to be:\n\\[\nf(t) = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\Pr(t &lt; T \\leq t + \\Delta t)}{\\Delta t}\n\\]\nUsing the conditional probability definition we can find how the hazard function connect the probability density function and the survival function.\n\\[\n\\begin{split}\nh(t) & = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\Pr((t &lt; T \\leq t + \\Delta t) \\cap (T &gt; t)) / \\Pr(T &gt; t)}{\\Delta t} \\\\\nh(t) & = \\lim_{\\Delta t \\rightarrow 0} \\frac{\\Pr(t &lt; T \\leq t + \\Delta t) / \\Delta t }{\\Pr(T &gt; t)}  \\\\\nh(t) & = \\frac{f(t)}{S(t)}\n\\end{split}\n\\]\nBut also\n\\[\n\\frac{\\mathrm d}{\\mathrm d x} S(t) = -f(t)\n\\]\nLet’s see a simulated example:\n\nCode# Setup\nlibrary(ggplot2)\nmu &lt;- 5\nsigma &lt;- 1\nt &lt;- 3:6\n\n# Create a data frame of values around the mean\ndf &lt;- data.frame(x = seq(mu-4*sigma, mu+4*sigma, length=1000))\ndf$Density &lt;- dnorm(df$x, mean=mu, sd=sigma)\ndf$S &lt;- pnorm(df$x, mean=mu, sd=sigma, lower.tail = FALSE)\n\n# Define hazard points to estimate\nannotations &lt;-data.frame(\n  t,\n  density = dnorm(t, mean=mu, sd=sigma) |&gt; round(2),\n  survival = pnorm(t, mean=mu, sd=sigma, lower.tail = FALSE) |&gt; round(2),\n  hazard = \n    (dnorm(t, mean=mu, sd=sigma) /\n       pnorm(t, mean=mu, sd=sigma, lower.tail = FALSE)) |&gt;\n    round(2)\n)\n\nannotations$label &lt;- paste0(\n  annotations$density,\" / \",\n  annotations$survival,\" = \", \n  annotations$hazard\n)\n\n# Plot the normal distribution\nggplot(df, aes(x=x, y=Density)) + \n  geom_blank(aes(y = Density*1.2)) +\n  geom_line() +\n  geom_area(data= subset(df, x &gt; max(t)),\n            fill=\"blue\", \n            alpha = 0.4) +\n  geom_point(data = annotations,\n             aes(x = t, y = density),\n             size = 3) +\n  geom_text(data = annotations,\n            aes(x = t, y = density, label = label),\n            vjust = -1,\n            fontface = \"bold\",\n            size = 4) +\n  labs(x=\"x\", \n       y=\"Probability Density\",\n       title = \"Hazard rate = f(t) / S(t), where f(t) is represented by a normal distribution\") +\n  scale_x_continuous(breaks = scales::breaks_width(1)) +\n  theme_classic()+\n  theme(plot.title = element_text(face = \"bold\", \n                                  margin = margin(b = 0.5, unit = \"cm\")))\n\n\n\n\n\n9.5.2 Modeling the survival time\nAs we often want to know how a treatment or the severity of an illness affects the survival of the patients.\nLineal approach\nTo use the hazard function to model the survival time as a function of the covariates (predictors), we can assume the next form for the hazard function to assure positive results:\n\\[\nh(t|x_i) = \\exp \\left( \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} \\right)\n\\]\nBased on \\(h(t|x_i)\\) we can calculate \\(S(t|x_i)\\) and maximize the next likelihood function to estimate the parameters \\(\\beta\\) assuming that the \\(n\\) observations are independent\n\\[\nL = \\prod_{i=1}^n h(y_i)^{\\delta_i} S(y_i)\n\\]\n\nWhen the \\(i\\)th observation is not censored (\\(\\delta = 1\\)) the likelihood is the probability of dying \\(f(y_i)\\) in a tiny interval around time \\(y_i\\).\nWhen the \\(i\\)th observation is censored(\\(\\delta = 0\\)) the likelihood is the probability of surviving \\(S(y_i)\\) at least until time \\(y_i\\).\nFlexible approach\nTo use the hazard function to model the survival time as a function of the covariates (predictors), we can use the proportional hazards assumption\n\\[\nh(t|x_i) = \\underbrace{h_0(t)}_\\text{Baseline Hazard}\n\\underbrace{\\exp \\left( \\sum_{j=1}^p x_{ij} \\beta_j \\right)}_\\text{Relative Risk}\n\\] Where\n\n\nBaseline Hazard \\(h_0(t) \\geq 0\\): Represent an unspecified function, so it can take any form.\n\nThe most import assumption to keep in main is that a one-unit increase in \\(x_{ij}\\) corresponds to an increase in \\(h(t|x_i)\\) by a factor of \\(\\exp(\\beta_j)\\).\n\n\n\n\nBut with the proportional hazards assumption we can not estimate \\(\\beta = (\\beta_1, \\dots, \\beta_p)^T\\) by maximizing the likelihood with out having an specify the form of \\(h_0(t)\\).\nTo solve this problem the Cox’s proportional hazards model make use of the same sequential in time logic that we used to derive the Kaplan-Meier survival curve and the log-rank test.\nWe know that if the \\(i\\)th observation is uncensored, then \\(h(t|x_i) = h_0(y_i)\\exp \\left( \\sum_{j=1}^p x_{ij} \\beta_j \\right)\\), but the total hazard at time \\(y_i\\) for the at risk observations \\(\\sum_{i': y_{i'} \\geq y_i} h_0(y_i) \\exp \\left( \\sum_{j=1}^p x_{i'j} \\beta_j \\right)\\) making possible to cancel out \\(h_0(y_i)\\) at calculating the probability that the \\(i\\)th observation is the one to fail at time \\(y_i\\):\n\\[\n\\frac{h_0(y_i)\\exp \\left( \\sum_{j=1}^p x_{ij} \\beta_j \\right)}\n{\\sum_{i': y_{i'} \\geq y_i} h_0(y_i) \\exp \\left( \\sum_{j=1}^p x_{i'j} \\beta_j \\right)}\n\\]\nThe partial likelihood correspond to the product of these probabilities over all of the uncensored observations and we can used to estimate \\(\\beta\\). If there are no tied failure times it takes the form:\n\\[\nPL(\\beta) = \\prod_{i: \\delta_i = 1} \\frac{\\exp \\left( \\sum_{j=1}^p x_{ij} \\beta_j \\right)}\n{\\sum_{i': y_{i'} \\geq y_i}\\exp \\left( \\sum_{j=1}^p x_{i'j} \\beta_j \\right)}\n\\]\nConnection With The Log-Rank Test\nIn the case of a single binary covariate, the score test for \\(H_0 : \\beta = 0\\) in Cox’s proportional hazards model is exactly equal to the log-rank test.\nExamples\nBrain Cancer Data\n\n\n\n\n\nThe estimated hazard for patients with HG Glioma is \\(e^{2.15} = 8.58\\) times greater that patients with different diagnosis if we hold all other covariates fixed.\nThe higher the Karnofsky index, the lower the chance of dying at any given point in time, to be more specific each one-unit increase in the Karnofsky index corresponds to a multiplier of \\(e^{−0.05} = 0.95\\) in the instantaneous chance of dying.\nPublication Data\n\nThey start running a log-rank test yields a very unimpressive \\(p\\)-value of \\(0.36\\) based on posres.\n\n\n\n\n\n\nThen the run a Cox model with all the predictors the posres turn to be a great predictor of the time to publication. Here we can see the KM survival curve after adjusting for all other covariates.\n\n\n\n\n\nCoding example\n\n# Training Cox Models\nBrainCancerCox &lt;- coxph(\n  Surv(time, status) ~ diagnosis + ki,\n  data = BrainCancer\n)\n\ncoef(BrainCancerCox)\n\ndiagnosisLG glioma diagnosisHG glioma     diagnosisOther                 ki \n        1.04869491         2.24981734         0.86153649        -0.06585647 \n\n# Defining the relations to plot\nBrainCancerGrid &lt;- expand.grid(\n  diagnosis = levels(BrainCancer$diagnosis),\n  ki = quantile(BrainCancer$ki, probs = c(0.50, 0.75))\n)\n\n# Compute Cox model and survival curves\nBrainCancerCoxSc &lt;- survfit(\n  BrainCancerCox,\n  data = BrainCancer,\n  newdata = BrainCancerGrid,\n  conf.type = \"none\"\n)\n\n# Use the summary of BrainCancerCoxSc\n# to take a vector of patient IDs\nBrainCancerCoxScDf &lt;- surv_summary(BrainCancerCoxSc)\npid &lt;- as.character(BrainCancerCoxScDf$strata)\n\n\n# Transforming the data to create the plot\nm_newdat &lt;- BrainCancerGrid[pid, , drop = FALSE]\nBrainCancerCoxScDfPlot &lt;- cbind(BrainCancerCoxScDf, m_newdat)\nBrainCancerCoxScDfPlot$ki &lt;- factor(BrainCancerCoxScDfPlot$ki)\n\n\n# Plot\nggsurvplot_df(\n  BrainCancerCoxScDfPlot, \n  linetype = \"ki\",\n  color = \"diagnosis\", \n  legend.title = NULL, \n  censor = FALSE\n)\n\n\n\n\nWeibull model\nThis method start assuming a distribution to describe the event time, from the following distributions.\n\nnames(survreg.distributions)\n\n [1] \"extreme\"     \"logistic\"    \"gaussian\"    \"weibull\"     \"exponential\"\n [6] \"rayleigh\"    \"loggaussian\" \"lognormal\"   \"loglogistic\" \"t\"          \n\n\nIn this model the coefficients are interpreted in the opposite way to the Cox’s Model as positive coefficients increase time until the event take place.\n\nBrainCancerWM &lt;- survreg(\n  Surv(time, status) ~ diagnosis + ki,\n  data = BrainCancer,\n  dist = \"weibull\"\n)\n\ncoef(BrainCancerWM)\n\n       (Intercept) diagnosisLG glioma diagnosisHG glioma     diagnosisOther \n        0.61306348        -0.85014728        -1.94279137        -0.82320374 \n                ki \n        0.05407582 \n\n\nOne technique to visualize numeric variables in using the 25%, 50%, and 75-% quantiles\n\n# Defining the probabilities to plot\n\nSurvivalProb &lt;- seq(.99, .01, by = -.01)\n\nplot(SurvivalProb)\n\n\n\n# Defining the relations to plot\n\nBrainCancerGrid &lt;- CJ(\n  diagnosis = levels(BrainCancer$diagnosis),\n  ki = quantile(BrainCancer$ki, probs = c(0.50, 0.75))\n)\n\nBrainCancerGrid\n\nKey: &lt;diagnosis, ki&gt;\n    diagnosis    ki\n       &lt;char&gt; &lt;num&gt;\n1:  HG glioma    80\n2:  HG glioma    90\n3:  LG glioma    80\n4:  LG glioma    90\n5: Meningioma    80\n6: Meningioma    90\n7:      Other    80\n8:      Other    90\n\n# The predict function creates column per observation in newdata\n\nBrainCancerWmTime &lt;- predict(\n  BrainCancerWM,\n  type = \"quantile\",\n  p = 1 - SurvivalProb,\n  newdata = BrainCancerGrid\n)\n\n\n# Join and pivot longer the variables in order the have\n# the columns names to run the plotting function\n\ncbind(BrainCancerGrid,\n      BrainCancerWmTime\n)[, melt(.SD,\n         id.vars = names(BrainCancerGrid),\n         variable.name = \"surv_id\",\n         value.name = \"time\")\n][,`:=`(surv = SurvivalProb[as.integer(surv_id)],\n        ki = factor(ki))\n][, c(\"upper\", \"lower\", \"std.err\", \"strata\") := NA_real_] |&gt;\n  ggsurvplot_df(surv.geom = geom_line,\n                linetype = \"ki\", \n                color = \"diagnosis\", \n                legend.title = NULL)\n\n\n\n\n\n9.5.3 Shrinkage for the Cox Model\nTo implement the “loss+penalty” formulation to our partial likelihood we use the next function:\n\\[\n- \\log\n\\left(\n  \\prod_{i: \\delta_i = 1} \\frac{\\exp \\left( \\sum_{j=1}^p x_{ij} \\beta_j \\right)}\n  {\\sum_{i': y_{i'} \\geq y_i}\\exp \\left( \\sum_{j=1}^p x_{i'j} \\beta_j \\right)}\n\\right)\n+ \\lambda P(\\beta)\n\\]\nWhere:\n\n\\(\\beta = \\beta_1, \\dots, \\beta_p\\)\n\n\\(\\lambda\\): Correspond to a non-negative tunning parameter\n\n\\(P(\\beta) = \\sum_{j=1}^p \\beta_j^2\\) ridge penalty\n\n\n\\(P(\\beta) = \\sum_{j=1}^p |\\beta_j|\\) lasso penalty\n\n\nLet’s see an example of the tuning process for a lasso-penalized Cox model:\n\n\n\n\n\n9.5.4 Model Evaluation\nComparing predicted and true survival times\nTo compare the predicted and the true survival time, we need to figure out how to:\n\nUse censored observations.\nTranslate the estimated survival curve \\(S(t|x)\\) into survival times.\n\nOne possible solution is to stratify the observations based on the coefficient estimated by following the next steps:\n\nCalculate an estimated risk score using the coefficients from the Cox’s model on test dataset.\n\n\\[\n\\hat{\\eta}_i = \\hat{\\beta}_1 x_{i1} + \\dots + \\hat{\\beta}_p x_{ip}\n\\]\n\nCategorize the observations based on their “risk”. If the model works well you should see a clear separation between each class.\n\n\n\n\n\nC-index\nI would be useful to calculate a metric like the AUC from the ROC curve, but it’s important to recall that we do not observe the event time of each observation \\(t_1, \\dots, t_n\\).\nAccording to Longato, Vettoretti, and Di Camillo (2020) an alternative is to use the Harrell’s concordance index (C-index) which quantify the probability that greater risk scores are attributed to subjects with higher change of experiencing the event.\n\\[\nC = P(\\hat{\\eta}_{i'} &gt; \\hat{\\eta}_i \\; | \\; T_{i'} &lt; T_i, \\; \\delta_{i'} = 1, \\; T_{i'} &lt; t)\n\\]\nWith the next function:\n\\[\nC = \\frac{\\sum_{i,i':y_i&gt;y_{i'}} I(\\hat{\\eta}_{i'} &gt; \\hat{\\eta}_{i}) \\delta_{i'}}{\\sum_{i,i':y_i&gt;y_{i'}} \\delta_{i'}}\n\\]\nWhere:\n\n\n\\(y_i\\) and \\(y_{i'}\\) are the observed survival times.\n\n\\(\\hat{\\eta}_i\\) and \\(\\hat{\\eta}_{i'}\\) are the estimated risk scores\n\n\\(I(\\hat{\\eta}_{i'} &gt; \\hat{\\eta}_i)\\) returns 1 if the criteria is met.\n\n\\(\\delta_{i'}\\) returns 1 if the \\(i'^{th}\\) subject’s event has been observed.\n\nSimulated example\nWhere \\(P = 1\\) and \\(\\hat{\\beta} = 2.5\\) and we have the next testing set:\n\n\nSubject\nx\nSurvival Time \\(y\\)\n\nCensoring Indicator \\(\\delta\\)\n\n\n\n\n1\n1\n3\n0\n\n\n2\n2\n4\n1\n\n\n3\n3\n5\n1\n\n\n4\n4\n6\n1\n\n\n5\n5\n7\n0\n\n\n\n\nCalculate the risk scores \\(\\hat{\\eta}\\) for each subject:\n\n\\(\\hat{\\eta}_i = \\hat{\\beta} x_i\\)\n\n\nSubject\n\\(\\hat{\\eta}\\)\n\n\n\n1\n2.5\n\n\n2\n5\n\n\n3\n7.5\n\n\n4\n10\n\n\n5\n12.5\n\n\n\n\nFor each pair of subjects (\\(i, i'\\)) where \\(y_i &gt; y_{i'}\\), calculate \\(I(\\hat{\\eta}_{i'} &gt; \\hat{\\eta}_i) \\delta_{i'}\\) and \\(\\delta_{i'}\\):\n\n\n\n\n\n\n\n\n\n\nPair (i, i’)\n\\(y_i &gt; y_{i'}\\)\n\\(I(\\hat{\\eta}_{i'} &gt; \\hat{\\eta}_i)\\)\n\\(\\delta_{i'}\\)\n\\(I(\\hat{\\eta}_{i'} &gt; \\hat{\\eta}_i) \\delta_{i'}\\)\n\n\n\n(2,1)\nTrue\n0\n0\n0\n\n\n(3,1)\nTrue\n0\n0\n0\n\n\n(3,2)\nTrue\n0\n1\n0\n\n\n(4,1)\nTrue\n0\n0\n0\n\n\n(4,2)\nTrue\n0\n1\n0\n\n\n(4,3)\nTrue\n0\n1\n0\n\n\n(5,1)\nTrue\n0\n0\n0\n\n\n(5,2)\nTrue\n0\n1\n0\n\n\n(5,3)\nTrue\n0\n1\n0\n\n\n(5,4)\nTrue\n0\n1\n0\n\n\n\n\nCalculate the concordance index C:\n\n\\[\nC = \\frac{\\sum_{i,i':y_i&gt;y_{i'}} I(\\hat{\\eta}_{i'} &gt; \\hat{\\eta}_{i}) \\delta_{i'}}{\\sum_{i,i':y_i&gt;y_{i'}} \\delta_{i'}} = \\frac{0}{5} = 0\n\\]\nCoding example\n\nlibrary(dynpred)\n\ndata.frame(\n  predictors = c(\"diagnosis\" ,\"diagnosis + ki\", \".\"),\n  c_index = c(\n    CVcindex(Surv(time, status) ~ diagnosis, data = BrainCancer)[[\"cindex\"]],\n    CVcindex(Surv(time, status) ~ diagnosis + ki, data = BrainCancer)[[\"cindex\"]],\n    CVcindex(Surv(time, status) ~ ., data = BrainCancer)[[\"cindex\"]]\n  )\n)\n\n1/872/873/874/875/876/877/878/879/8710/8711/8712/8713/8714/8715/8716/8717/8718/8719/8720/8721/8722/8723/8724/8725/8726/8727/8728/8729/8730/8731/8732/8733/8734/8735/8736/8737/8738/8739/8740/8741/8742/8743/8744/8745/8746/8747/8748/8749/8750/8751/8752/8753/8754/8755/8756/8757/8758/8759/8760/8761/8762/8763/8764/8765/8766/8767/8768/8769/8770/8771/8772/8773/8774/8775/8776/8777/8778/8779/8780/8781/8782/8783/8784/8785/8786/8787/87\n\n1/872/873/874/875/876/877/878/879/8710/8711/8712/8713/8714/8715/8716/8717/8718/8719/8720/8721/8722/8723/8724/8725/8726/8727/8728/8729/8730/8731/8732/8733/8734/8735/8736/8737/8738/8739/8740/8741/8742/8743/8744/8745/8746/8747/8748/8749/8750/8751/8752/8753/8754/8755/8756/8757/8758/8759/8760/8761/8762/8763/8764/8765/8766/8767/8768/8769/8770/8771/8772/8773/8774/8775/8776/8777/8778/8779/8780/8781/8782/8783/8784/8785/8786/8787/87\n\n1/872/873/874/875/876/877/878/879/8710/8711/8712/8713/8714/8715/8716/8717/8718/8719/8720/8721/8722/8723/8724/8725/8726/8727/8728/8729/8730/8731/8732/8733/8734/8735/8736/8737/8738/8739/8740/8741/8742/8743/8744/8745/8746/8747/8748/8749/8750/8751/8752/8753/8754/8755/8756/8757/8758/8759/8760/8761/8762/8763/8764/8765/8766/8767/8768/8769/8770/8771/8772/8773/8774/8775/8776/8777/8778/8779/8780/8781/8782/8783/8784/8785/8786/8787/87\n\n\n      predictors   c_index\n1      diagnosis 0.5883878\n2 diagnosis + ki 0.7376879\n3              . 0.7501296"
  },
  {
    "objectID": "survival-analysis.html#references",
    "href": "survival-analysis.html#references",
    "title": "\n9  Time-event (Survival) Analysis and Censored Data\n",
    "section": "\n9.6 References",
    "text": "9.6 References\n\n\n\n\nLongato, Enrico, Martina Vettoretti, and Barbara Di Camillo. 2020. “A Practical Perspective on the Concordance Index for the Evaluation and Selection of Prognostic Time-to-Event Models.” Journal of Biomedical Informatics 108: 103496. https://doi.org/https://doi.org/10.1016/j.jbi.2020.103496."
  },
  {
    "objectID": "unsupervised-learning.html#libraries-to-use",
    "href": "unsupervised-learning.html#libraries-to-use",
    "title": "\n10  Unsupervised Learning\n",
    "section": "\n10.1 Libraries to use",
    "text": "10.1 Libraries to use\n\n# Data manipulation\nlibrary(data.table)\nlibrary(recipes)\n\n# Data visualization\nlibrary(ggplot2)     \ntheme_set(theme_light())\n\n# Model creation\nlibrary(h2o)\nlibrary(cluster)\nlibrary(tidymodels)\nlibrary(tidyclust)\n\n# Extracting model information\nlibrary(broom)\nlibrary(factoextra)\nlibrary(dendextend)\nlibrary(NbClust)"
  },
  {
    "objectID": "unsupervised-learning.html#principal-components-analysis-pca",
    "href": "unsupervised-learning.html#principal-components-analysis-pca",
    "title": "\n10  Unsupervised Learning\n",
    "section": "\n10.2 Principal Components Analysis (PCA)",
    "text": "10.2 Principal Components Analysis (PCA)\n\n10.2.1 Purpose\nAs the number of variables increases checking two-dimensional scatterplots gets less insightful since they each contain just a small fraction of the total information present in the data set.\nFor example, if we see correlations between features is easy to create more general categories known as latent variable as follow:\n\nSandwich\n\ncheese - mayonnaise\nmayonnaise - bread\nbread - cheese\nbread - lettuce\n\n\nSoda\n\npepsi - coke\n7up - coke\n\n\nVegetables\n\nspinach - broccoli\npeas - potatoes\npeas - carrots\n\n\n\nAt the end this process can help to:\n\nReduce the number of featured need to describe the data\nRemove multicollinearity between features\n\n\n10.2.2 Mathematical Description\nPCA finds a low-dimensional representation of a data set that contains as much variation (information) as possible. It assumes that all dimensions can be described as a linear combination of the \\(p\\) original variables, known as principal component scores.\n\\[\nZ_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p\n\\]\nWhere:\n\n\n\\(\\phi_1 = (\\phi_{11} \\phi_{21} \\dots \\phi_{p1})^T\\) represent the loading vector for the first principal component.\n\\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\n10.2.3 Steps to apply\nTo perform a principal components analysis (PCA) need to:\n\nMake any needed transformation to tidy the data.\nRemove or impute any missing value.\nTransform or variables to be numeric by using method like one-hot encoding.\nCenter and scale all the variables in \\(\\mathbf{X}\\) to have mean zero and standard deviation one. This step is important to ensure all variables are on the same scale, particularly if they were measured in different units or have outliers. To have a visual representation of the importance of this step less try to calculate how different are two people based on their height and weight.\n\n\n\n\n\nIn both cases we got the same distance and that doesn’t make sense, so let’s standardize the values with the next function.\n\\[\n\\text{height}_\\text{scaled} = \\frac{\\text{height} - mean(\\text{height})}{sd(\\text{height})}\n\\]\n\n\n\n\n\nReturning a more intuive result.\n\n\nThe loading vector is determined by solving the following optimization problem using eigen decomposition. This identifies the direction with the largest variance in the feature space and reveals the relative contributions of the original features to the new PCs.\n\n\\[\n\\underset{\\phi_{11}, \\dots, \\phi_{p1}}{\\text{maximize}}\n\\left\\{ \\frac{1}{n} \\sum_{i = 1}^n\n\\left( \\sum_{j=1}^p \\phi_{j1} x_j \\right)^2 \\right\\}\n\\text{ subject to }\n\\sum_{j=1}^p \\phi_{j1}^2 = 1 .\n\\]\n\n\n\n\n\nRepeat the process until having \\(\\min(n-1, p)\\) distinct principal components. Each new component must be orthogonal to all previously computed principal components to ensure that each new component captures a new direction of variance assuring a new uncorrelated new component.\n\n10.2.4 R implementation\n\nGetting the data\n\n\nmy_basket &lt;- fread(\"https://koalaverse.github.io/homlr/data/my_basket.csv\")\n\n\nApply the PCA function\n\n\n\nstats\nh2o\nrecipes\n\n\n\n\nstats_pca &lt;- prcomp(\n  # Numeric data.frame o matrix\n  my_basket,\n  \n  # Set standard deviation to 1 before applying PCA\n  # this is really useful when sd is very different between columns\n  # but we don't nee\n  scale = TRUE,\n  \n  # Set mean to 0 before applying PCA\n  # it's better to keep it TRUE\n  center = TRUE\n)\n\nAfter performing a PCA we can see the next elements:\n\nnames(stats_pca)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\n\ncenter: the column means used to center to the data, or FALSE if the data weren’t centered.\n\nscale: the column standard deviations used to scale the data, or FALSE if the data weren’t scaled.\n\nrotation: the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components.\n\nx: the value of each observation in the original dataset projected to the principal components.\n\n\n\nJava is a prerequisite for H2O.\n\n# turn off progress bars for brevity\nh2o.no_progress()  \n\n# connect to H2O instance with 5 gigabytes\nh2o.init(max_mem_size = \"5g\")  \n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\angel\\AppData\\Local\\Temp\\RtmpYX5P4u\\file1e2c2f878b6/h2o_angel_started_from_r.out\n    C:\\Users\\angel\\AppData\\Local\\Temp\\RtmpYX5P4u\\file1e2c31fc2435/h2o_angel_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         4 seconds 860 milliseconds \n    H2O cluster timezone:       America/La_Paz \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.40.0.4 \n    H2O cluster version age:    4 months and 8 days \n    H2O cluster name:           H2O_started_from_R_angel_yif231 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   4.43 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.3 (2023-03-15 ucrt) \n\n# convert data to h2o object\nmy_basket.h2o &lt;- as.h2o(my_basket)\n\n\n\npca_method: When your data contains mostly numeric data use “GramSVD”, but if the data contain many categorical variables (or just a few categorical variables with high cardinality) we recommend to use “GLRM”.\n\nk: Integer specifying how many PCs to compute.Use ncol(data).\n\ntransform: Character string specifying how (if at all) your data should be standardized.\n\nimpute_missing: Logical specifying whether or not to impute missing values with the corresponding column mean.\n\nmax_runtime_secs: Number specifying the max run time (in seconds) to limit the runtime for model training.\n\n\nh2o_pca &lt;- h2o.prcomp(\n  training_frame = my_basket.h2o,\n  pca_method = \"GramSVD\",\n  k = ncol(my_basket.h2o), \n  transform = \"STANDARDIZE\", \n  impute_missing = TRUE,\n  max_runtime_secs = 1000\n)\n\nh2o.shutdown(prompt = FALSE)\n\n\n\n\npca_rec &lt;- recipe(~., data = my_basket) |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric(), id = \"pca\") |&gt;\n  prep()\n\n\n\n\n\n10.2.5 Extract the variance explained by each component\n\n\nstats\nh2o\nrecipes\n\n\n\n\nstats_pca_variance &lt;-\n  tidy(stats_pca,\n       matrix = \"eigenvalues\")\n\nsetDT(stats_pca_variance)\n\nstats_pca_variance[1:5]\n\n      PC  std.dev percent cumulative\n   &lt;num&gt;    &lt;num&gt;   &lt;num&gt;      &lt;num&gt;\n1:     1 1.513919 0.05457    0.05457\n2:     2 1.473768 0.05171    0.10628\n3:     3 1.459114 0.05069    0.15698\n4:     4 1.440635 0.04941    0.20639\n5:     5 1.435279 0.04905    0.25544\n\n\n\n\n\nh2o_pca@model$importance |&gt;\n  t() |&gt;\n  as.data.table(keep.rownames = \"component\") |&gt;\n  head(5L)\n\n   component Standard deviation Proportion of Variance Cumulative Proportion\n      &lt;char&gt;              &lt;num&gt;                  &lt;num&gt;                 &lt;num&gt;\n1:       pc1           1.513919             0.05457025            0.05457025\n2:       pc2           1.473768             0.05171412            0.10628436\n3:       pc3           1.459114             0.05069078            0.15697514\n4:       pc4           1.440635             0.04941497            0.20639012\n5:       pc5           1.435279             0.04904821            0.25543833\n\n\n\n\n\ntidy(pca_rec,\n     id = \"pca\",\n     type = \"variance\") |&gt;\n  filter(component &lt;= 5) |&gt;\n  pivot_wider(id_cols = component,\n              names_from = terms,\n              values_from = value)\n\n# A tibble: 5 × 5\n  component variance `cumulative variance` `percent variance`\n      &lt;int&gt;    &lt;dbl&gt;                 &lt;dbl&gt;              &lt;dbl&gt;\n1         1     2.29                  2.29               5.46\n2         2     2.17                  4.46               5.17\n3         3     2.13                  6.59               5.07\n4         4     2.08                  8.67               4.94\n5         5     2.06                 10.7                4.90\n# ℹ 1 more variable: `cumulative percent variance` &lt;dbl&gt;\n\n\n\n\n\n\n10.2.6 Select the components to explore\nVariance criterion\nAs the sum of the variance (eigenvalues) of all the components is equal to the number of variables entered into the PCA.\n\nstats_pca_variance[, .(total_variance = sum(std.dev^2))]\n\n   total_variance\n            &lt;num&gt;\n1:             42\n\n\nA variance of 1 means that the principal component would explain about one variable’s worth of the variability. In that sense we would just be interesting in selecting components with variance 1 or greater.\n\nstats_pca_variance[std.dev &gt;= 1]\n\n       PC  std.dev percent cumulative\n    &lt;num&gt;    &lt;num&gt;   &lt;num&gt;      &lt;num&gt;\n 1:     1 1.513919 0.05457    0.05457\n 2:     2 1.473768 0.05171    0.10628\n 3:     3 1.459114 0.05069    0.15698\n 4:     4 1.440635 0.04941    0.20639\n 5:     5 1.435279 0.04905    0.25544\n 6:     6 1.411544 0.04744    0.30288\n 7:     7 1.253307 0.03740    0.34028\n 8:     8 1.026387 0.02508    0.36536\n 9:     9 1.010238 0.02430    0.38966\n10:    10 1.007253 0.02416    0.41382\n\n\nScree plot criterion\nThe scree plot criterion looks for the “elbow” in the curve and selects all components just before the line flattens out, which looks like 8 in our example.\n\n\nggplot2\nfactoextra\n\n\n\n\nstats_pca_variance |&gt;\n  ggplot(aes(PC, percent, group = 1, label = PC)) +\n  geom_point() +\n  geom_line() +\n  geom_text(nudge_y = -.002,\n            check_overlap = TRUE)\n\n\n\n\n\n\n\nfviz_eig(stats_pca,\n         geom = \"line\",\n         ncp = 30,\n         ggtheme = theme_light())+\n  geom_text(aes(label = dim),\n            nudge_y = -.2,\n            check_overlap = TRUE)\n\n\n\n\n\n\n\nProportion of variance explained criterion\nDepending of the use case the investigator might want to explain a particular proportion of variability. For example, to explain at least 75% of total variability we need to select the first 27 components.\n\nstats_pca_variance[cumulative &gt;= 0.75][1L]\n\n      PC   std.dev percent cumulative\n   &lt;num&gt;     &lt;num&gt;   &lt;num&gt;      &lt;num&gt;\n1:    27 0.8718093  0.0181    0.76444\n\n\nConclusion\nThe frank answer is that there is no one best method for determining how many components to use. If we were merely trying to profile customers we would probably use 8 or 10, if we were performing dimension reduction to feed into a downstream predictive model we would likely retain 26 or more based on cross-validation.\n\n10.2.7 Interpret the results\n\nPC1 can be interpreted as the Unhealthy Lifestyle component, as the higher weights are associated with less healthy behaviors, such as alcohol (bulmers, red.wine, fosters, kronenbourg), sweets (mars, twix, kitkat), tobacco (cigarettes), and potentially gambling (lottery).\n\n\n\nggplot2\nfactoextra\n\n\n\n\nstats_pca_loadings &lt;-\n  tidy(stats_pca, matrix = \"loadings\")\n\nsetDT(stats_pca_loadings)\n\nstats_pca_loadings[PC == 1\n  ][order(-value)\n  ][, rbind(.SD[1:10],\n            .SD[(.N-9L):.N])] |&gt;\n  ggplot(aes(value, reorder(column, value))) +\n  geom_point()+\n  geom_vline(xintercept = 0,\n             linetype = 2,\n             linewidth = 1)+\n  labs(y = \"Original Columns\",\n       x = \"Unhealthy / Entertainment Lifestyle\")\n\n\n\n\n\n\n\nfviz_contrib(stats_pca, \n             choice = \"var\", \n             axes = 1,\n             sort.val = \"asc\",\n             top = 25)+\n  coord_flip()\n\n\n\n\n\n\n\n\nPC2 can be interpreted as the a Dine-in Food Choices component, as associated items are typically part of a main meal that one might consume for lunch or dinner.\n\n\n\nggplot2\nfactoextra\n\n\n\n\nstats_pca_loadings[PC == 2\n  ][order(-value)\n  ][, rbind(.SD[1:10],\n            .SD[(.N-9L):.N])] |&gt;\n  ggplot(aes(value, reorder(column, value))) +\n  geom_point()+\n  geom_vline(xintercept = 0,\n             linetype = 2,\n             linewidth = 1)+\n  labs(y = \"Original Columns\",\n       x = \"Dine-in Food Choices\")\n\n\n\n\n\n\n\nfviz_contrib(stats_pca, \n             choice = \"var\", \n             axes = 2,\n             sort.val = \"asc\",\n             top = 25)+\n  coord_flip()\n\n\n\n\n\n\n\n\nFind correlated features. As this can be hard we have many features let’s use the iris data set.\n\n\niris_pca &lt;- prcomp(iris[-5])\n\n\n\nggplot2\nfactoextra\n\n\n\n\niris_pca$rotation |&gt;\n  as.data.table(keep.rownames = \"column\") |&gt;\n  ggplot(aes(`PC1`, `PC2`, label = column)) +\n  geom_vline(xintercept = 0,\n             linetype = 2,\n             linewidth = 0.3)+\n  geom_hline(yintercept = 0,\n             linetype = 2,\n             linewidth = 0.3)+\n  geom_text()+\n  geom_segment(aes(xend = `PC1`,\n                   yend = `PC2`),\n               x = 0,\n               y = 0,\n               arrow = arrow(length = unit(0.15, \"inches\")))+\n  labs(title = \"Component Loadings\",\n       x = \"PC 1\",\n       y = \"PC 2\")\n\n\n\n\n\n\n\nfviz_pca_var(iris_pca,\n             check_overlap = TRUE)+\n  labs(title = \"Component Loadings\",\n       x = \"PC 1\",\n       y = \"PC 2\")\n\n\n\n\n\n\n\n\nFound out how much each feature contribute to new components.\n\n\n\nggplot2\nfactoextra\n\n\n\n\nstats_pca_loadings[PC &lt;= 2L\n  ][, contrib := sqrt(sum(value^2)),\n    by = \"column\"] |&gt;\n  dcast(column + contrib ~ PC, \n        value.var = \"value\") |&gt;\n  ggplot(aes(`1`, `2`, label = column)) +\n  geom_text(aes(alpha = contrib),\n            color = \"navyblue\",\n            check_overlap = TRUE)+\n  geom_vline(xintercept = 0,\n             linetype = 2,\n             linewidth = 0.3)+\n  geom_hline(yintercept = 0,\n             linetype = 2,\n             linewidth = 0.3)+\n  labs(title = \"Component Loadings\",\n       x = paste0(\"Unhealthy Lifestyle (\",\n                  scales::percent(stats_pca_variance$percent[1L],\n                                  accuracy = 0.01),\")\"),\n       y = paste0(\"Dine-in Food Choices (\",\n                  scales::percent(stats_pca_variance$percent[2L],\n                                  accuracy = 0.01),\")\"))+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nfviz_pca_var(stats_pca,\n             geom = \"text\",\n             col.var = \"contrib\",\n             check_overlap = TRUE)+\n  scale_color_gradient(low = NA, high = \"navyblue\")+\n  coord_cartesian(xlim = c(-0.5,0.5),\n                  ylim = c(-0.5,0.5))+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "unsupervised-learning.html#clustering",
    "href": "unsupervised-learning.html#clustering",
    "title": "\n10  Unsupervised Learning\n",
    "section": "\n10.3 Clustering",
    "text": "10.3 Clustering\nIt refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. Some applications could be to:\n\nFind few different unknown subtypes of breast cancer.\nPerform market segmentation by identify subgroups of people who might be more likely to purchase a particular product.\n\n\n10.3.1 K-means clustering\nIn K-means clustering, we seek to partition the observations into a pre-specified number of non-overlapping clusters \\(K\\).\n\n\n\n\nFor this method, the main goal is to classify observations within clusters with high intra-class similarity (low within-cluster variation), but with low inter-class similarity.\nMathematical Description\nLet \\(C_1, \\dots, C_K\\) denote sets containing the indices of the observations in each cluster, where:\n\nEach observation belongs to at least one of the \\(K\\) clusters. \\(C_1 \\cup C_2 \\cup \\dots \\cup C_K = \\{1, \\dots,n\\}\\)\nNo observation belongs to more than one cluster. \\(C_k \\cap C_{k'} = \\emptyset\\) for all \\(k \\neq k'\\).\n\n\\[\n\\underset{C_1. \\dots, C_K}{\\text{minimize}} =\n\\left\\{ \\sum_{k=1}^k W(C_k) \\right\\}\n\\]\n\\(W(C_k)\\) represent the amount by which the observations within a cluster differ from each other. There are many possible ways to define this concept, but the most common choice involves squared euclidean distance, which is sensitive to outliers and works better with gaussian distributed features.\n\\[\nW(C_k) = \\frac{1}{| C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2\n\\] Where:\n\n\n\\(|C_k|\\): Denotes the number of observations in the \\(k\\)th cluster\n\nDistance alternatives\nSome alternatives to the euclidean distance more robust to outliers and Non-normal distributions are:\n\nManhattan distance\nMinkowski distance\nGower distance\n\nIf we want to calculate the similarity for binary variables or categorical variables after applying one-hot encoding, we can use the Jaccard Index to calculate the distance.\n\\[\n\\begin{split}\nJ(A,B) & =\n  \\frac{\\overbrace{A \\cap B}^\\text{# same value columns}}\n       {\\underbrace{A \\cup B}_\\text{# columns}} \\\\ \\\\\n\\text{Distance} & = 1 - J(A,B)\n\\end{split}\n\\]\nWe can find the distance related to the Jaccard Index by typing the next function in R.\ndist(x, method = \"binary\")\nIf you are analyzing unscaled data where observations may have large differences in magnitude but similar behavior then a correlation-based distance is preferred like:\n\n\\(1 - \\text{Pearson correlation}\\)\n\\(1 - \\text{Spearman correlation}\\)\n\\(1 - \\text{Kendall Tau correlation}\\)\n\n\n\n\n\nAproximation algorithm\nAs solving this problem would be very difficult, since there are almost \\(K^n\\) ways to partition n observations into \\(K\\) clusters, but we can use a very simple algorithm to find local optimum.\n\n\n\n\n\n\n\n\nSince the results obtained will depend on the initial (random) cluster assignment of each observation it is important to run the algorithm multiple times (10-20) from different random initial configurations (random starts). Then one selects the solution with the smallest objective.\n\n\n\n\n\n\n\n\n\n\nValidation tip\n\n\n\nTo validate that we are modeling signal rather than noise the algorithm should produce similar clusters in each iteration.\n\n\nCoding example\nTo perform k-means clustering on mixed data we need to:\n\nConvert any ordinal categorical variables to numeric\nConvert nominal categorical variables to one-hot encode\nScale all variables\n\n\names_scale &lt;- AmesHousing::make_ames() |&gt;\n  # select numeric columns\n  select_if(is.numeric) |&gt;\n  \n  # remove target column\n  select(-Sale_Price) |&gt;\n  \n  # coerce to double type\n  mutate_all(as.double) |&gt;\n  \n  # center & scale the resulting columns\n  scale()                  \n\n\nCompute the distances between the rows of a data matrix\n\n\n# Dissimilarity matrix\names_dist &lt;- dist(\n  ames_scale,\n  method = \"euclidean\"\n)\n\n\nPerform k-means clustering after setting a seed\n\n\n# For reproducibility\nset.seed(123)\n\names_kmeans &lt;- kmeans(\n  ames_dist,\n  # Number of groups\n  centers = 10,\n  \n  # Number of models to create\n  # the it selects the best one\n  nstart = 10,\n  \n  # Max number of iterations for each model\n  iter.max = 10\n)\n\n\nMeasure model’s quality (Total within-cluster sum of squares). Which represent the sum all squared distances from each observation to its cluster center in the model.\n\n\names_kmeans$tot.withinss |&gt; comma()\n\n[1] \"7,099,192\"\n\n\n\n10.3.2 Hierarchical clustering\n\nIt doesn’t require to define the number of clusters.\nIt returns an attractive tree-based representation (dendrogram).\n\n\nIt assumes that clusters are nested, but that isn’t true k-means clustering coud yield better.\n\nUnderstanding dendrograms\nIn general we can say that:\n\nEach leaf represent an observation\n\nSimilar the groups of observations are lower in the tree\n\nDifferent the groups of observations are near the top of the tree\nThe height of the cut controls the number of clusters obtained.\n\n\n\n\n\nIn the next example:\n\n{1,6} and {5,7} are close observations\n\n\n\n\n\n\nObservation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7, as it was fused at higher height of the cut.\n\n\n\n\n\nHierarchical Clustering Types\n\n\nAGNES (bottom-up) versus DIANA (top-down) clustering\n\nAgglomerative Clustering (AGNES, Bottom-up)\n\nDefining a dissimilarity measure between each pair of observations, like Euclidean distance and correlation-based distance.\nDefining each of the \\(n\\) observations as a cluster.\n\nFusing the most similar 2 clusters and repeating the process until all the observations belong to one single cluster.\n\n\nStep 1\n\n\n\n\n\n\nStep 2\n\n\n\n\n\n\nStep 3\n\n\n\n\n\n\nIt is good at identifying small clusters.\n\nDivisive Clustering (DIANA, top-down)\n\nDefining a dissimilarity measure between each pair of observations, like Euclidean distance and correlation-based distance.\nDefining the root, in which all observations are included in a single cluster.\nThe current cluster is split into two clusters that are considered most heterogeneous. The process is iterated until all observations are in their own cluster.\n\n\nIt is good at identifying large clusters.\n\nLinkage\nIt measures the dissimilarity between two clusters and defines if we will have a balanced tree where each cluster is assigned to an even number of observations, or an unbalanced tree to find outliers.\n\n\nAGNES clustering\n\n\nComplete (maximal intercluster dissimilarity): Record the largest dissimilarity between cluster \\(A\\) and \\(B\\). It tends to produce more compact clusters and balanced trees.\n\nWard’s minimum variance: Minimizes the total within-cluster variance. At each step the pair of clusters with the smallest between-cluster distance are merged. Tends to produce more compact clusters.\n\n\n\nDIANA clustering\n\n\nAverage (mean intercluster dissimilarity): Record the average dissimilarity between cluster \\(A\\) and \\(B\\). It can vary in the compactness of the clusters it creates, but must of the time produces balanced trees.\n\n\nSingle (minimal intercluster dissimilarity): Record the smallest dissimilarity between cluster \\(A\\) and \\(B\\). It tends to produce more extended clusters and unbalanced trees.\nCentroid: Computes the dissimilarity between the centroid for cluster \\(A\\) (a mean vector of length \\(p\\), one element for each variable) and the centroid for cluster \\(B\\). It is often used in genomics, but inversions can lead to difficulties in visualizing and interpreting of the dendrogram.\n\n\n\n\n\nCoding example\nOnce we have our distance matrix we just need to define a linkage method. The default is the complete one.\n\names_hclust &lt;- hclust(\n  ames_dist, \n  method = \"ward.D\"\n)\n\nDefining linkage method\nTo define the best method we can use the coef.hclust function from the cluster package to extract the agglomeration coefficients from the result of a hierarchical cluster analysis, which indicate the cost of merging different clusters at each stage of the clustering process. As result, the higher this value is, the more dissimilar the clusters being merged are.\n\ncluster_methods &lt;- c(\n  \"average\",\n  \"single\",\n  \"complete\",\n  \"ward.D\"\n)\n\nsetattr(cluster_methods,\n        \"names\",\n        cluster_methods)\n\nsapply(cluster_methods,\n       \\(x) fastcluster::hclust(ames_dist, method = x) |&gt;\n        coef.hclust() ) |&gt; \n  sort(decreasing = TRUE)\n\n   ward.D  complete   average    single \n0.9972137 0.9267750 0.9139303 0.8712890 \n\n\nAs we were expecting the the best result was using the “ward.D” linkage as the default R hclust performs a bottom-up algorithm.\nPlotting Dendrogram\n\n\nstats\ndendextend\nfactoextra\n\n\n\n\nplot(ames_hclust)\nabline(h = 800, col = \"red\")\n\n\n\n\n\n\n\nas.dendrogram(ames_hclust) |&gt;\n  color_branches(h = 800) |&gt;\n  plot()\n\n\n\n\n\n\n\nThis function creates great plots but gets really slow if we have many observations.\n\n\nfviz_dend(ames_hclust, k = 2)\n\n\n\n\n\n\n\n\nGetting clusters\nTo get the clusters we have 2 alternatives:\n\nDefining the number of clusters to export.\n\n\ncutree(ames_hclust, k = 2) |&gt; table() |&gt; prop.table()\n\n\n        1         2 \n0.3959044 0.6040956 \n\n\n\nDefining the height where the tree should be cut, which returns groups where the members of the created clusters have an euclidean distance amongst each other no greater than our cut height if where are using the complete linkage.\n\n\ncutree(ames_hclust, h = 400) |&gt; table() |&gt; prop.table()\n\n\n        1         2         3         4 \n0.1853242 0.1795222 0.2105802 0.4245734 \n\n\n\n10.3.3 Partitioning around medians (PAM)\nIt has the same algorithmic steps as k-means but uses the median rather than the mean to determine the centroid; making it more robust to outliers.\nAs your data becomes more sparse the performance of k-means and hierarchical clustering become slow and ineffective. An alternative is to use the Gower distance, which applies a particular distance calculation that works well for each data type.\n\n\nquantitative (interval): range-normalized Manhattan distance.\n\nordinal: variable is first ranked, then Manhattan distance is used with a special adjustment for ties.\n\nnominal: variables with \\(k\\) categories are first converted into \\(k\\) binary columns (i.e., one-hot encoded) and then the Dice coefficient is used. To compute the dice metric for two observations \\((X,Y)\\) the algorithm looks across all one-hot encoded categorical variables and scores them as:\n\n\na — number of dummies 1 for both observations\n\nb — number of dummies 1 for \\(X\\) and 0 for \\(Y\\)\n\n\nc - number of dummies 0 for \\(X\\) and 1 for \\(Y\\)\n\n\nd — number of dummies 0 for both and then uses the following formula:\n\n\n\n\\[\nD = \\frac{2a}{2a + b +c}\n\\]\n\n# Original data minus Sale_Price\names_full &lt;- \n  AmesHousing::make_ames() |&gt; \n  subset(select = -Sale_Price)\n\n# Compute Gower distance for original data\ngower_dst &lt;- daisy(ames_full, metric = \"gower\")\n\n# You can supply the Gower distance matrix to several clustering algos\npam_gower &lt;- pam(x = gower_dst, k = 8, diss = TRUE)\n\n\n10.3.4 Clustering large applications (CLARA)\nIt uses the same algorithmic process as PAM; however, instead of finding the medoids for the entire data set it considers a small sample size and applies k-means or PAM.\n\nsystem.time(kmeans(my_basket, centers = 10))\n\n   user  system elapsed \n   0.00    0.00    0.01 \n\nsystem.time(clara(my_basket, k = 10))\n\n   user  system elapsed \n   0.02    0.00    0.02 \n\n\n\n10.3.5 Selecting number of clusters\nAs \\(k\\) increases the homogeneity between observations in each cluster, but it also increases the risk to overfit the model, but it is important to know that there is not a definitive answer to this question. Some possibilities are:\n\nDefining \\(k\\) based on domain knowledge or resources limitations. For example, you want to divide customers in 4 groups as you only 4 employees to execute the plan.\n\nOptimizing a criterion\n\nThe elbow method tries to minimize the total intra-cluster variation (within-cluster sum of square, WSS) by selecting the number of clusters so that adding another cluster doesn’t improve much better the total WSS.\n\n\n\nhttps://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/\n\n\nHC\nK-means\n\n\n\n\nfviz_nbclust(ames_scale,\n             FUNcluster = hcut,\n             method = \"wss\",\n             hc_method = \"ward.D\") +\n  labs(subtitle = \"Elbow method\")\n\n\n\n\n\n\n\nfviz_nbclust(ames_scale,\n             FUNcluster = kmeans,\n             method = \"wss\") +\n  labs(subtitle = \"Elbow method\")\n\n\n\n\n\n\n\n\nThe silhouette method allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters.. The optimal number of clusters \\(k\\) is the one that maximize the average silhouette over a range of possible values for \\(k\\).\n\nValues close to 1 suggest that the observation is well matched to the assigned cluster.\nValues close to 0 suggest that the observation is borderline matched between two clusters.\nValues close to -1 suggest that the observations may be assigned to the wrong cluster\n\n\n\n\n\nHC\nK-means\n\n\n\n\nfviz_nbclust(ames_scale,\n             FUNcluster = hcut,\n             method = \"silhouette\",\n             hc_method = \"ward.D\") +\n  labs(subtitle = \"Silhouette method\")\n\n\n\n\n\n\n\nfviz_nbclust(ames_scale,\n             FUNcluster = kmeans,\n             method = \"silhouette\") +\n  labs(subtitle = \"Silhouette method\")\n\n\n\n\n\n\n\n\n\nGap statistic: Compares the total within intra-cluster variation for different values of \\(k\\) with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic, so the clustering structure would be far away from the random uniform distribution of points.\n\n\\[\n\\text{Gap}(k) = \\frac{1}{B} \\sum_{b=1}^B \\log(W^*_{kb}) - \\log(W_k)\n\\]\n\n\nHC\nK-means\n\n\n\n\nset.seed(123)\nfviz_nbclust(ames_scale,\n             FUNcluster = hcut,\n             method = \"gap_stat\",\n             hc_method = \"ward.D\",\n             nboot = 50,\n             verbose = FALSE)+\n  labs(subtitle = \"Gap statistic method\")\n\n\n\n\n\n\n\nset.seed(123)\nfviz_nbclust(ames_scale,\n             FUNcluster = kmeans,\n             method = \"gap_stat\",\n             nboot = 50,\n             nstart = 25,\n             verbose = FALSE)+\n  labs(subtitle = \"Gap statistic method\")\n\n\n\n\n\n\n\n\nAs there there are more than 30 indices and methods to we can compute all them in order to decide the best number of clusters using the majority rule\n\n\n10.3.6 Finding clusters patterns\nPara find cluster’s patterns we have 3 alternatives:\n\nUsing summary statistics by feature to compare groups with groups or one cluster the rest of the data.\nUsing PCA components to visualize clusters.\nPlotting 2 dimensional plots each time with some dinamyc color or alpha."
  },
  {
    "objectID": "02-execises.html#conceptual",
    "href": "02-execises.html#conceptual",
    "title": "02 - Statistical Learning",
    "section": "Conceptual",
    "text": "Conceptual\n\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a ﬂexible statistical learning method to be better or worse than an inﬂexible method. Justify your answer.\n\n\nThe sample size n is extremely large, and the number of predictors p is small.\n\nBetter, flexible models reduce bias and quit the variance low when having a large n.\n\nThe number of predictors p is extremely large, and the number of observations n is small.\n\nWorse, a flexible model could increase its variance very high when having small n.\n\nThe relationship between the predictors and response is highly non-linear.\n\nBetter, a lineal model would have a very high bias in this case.\n\nThe variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high.\n\nWorse, a flexible model would over-fit trying to follow the irreducible error.\n\nExplain whether each scenario is a classiﬁcation or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.\n\n\nWe collect a set of data on the top 500 ﬁrms in the US. For each ﬁrm we record proﬁt, number of employees, industry and the CEO salary. We are interested in understanding which factors aﬀect CEO salary.\n\nRegression, inference, 500, 4\n\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n\nClassification, prediction, 20, 14\n\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n\nRegression, prediction, 52, 4\n\nWe now revisit the bias-variance decomposition.\n\n\nProvide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less ﬂexible statistical learning methods towards more ﬂexible approaches. The x-axis should represent the amount of ﬂexibility in the method, and the y-axis should represent the values for each curve. There should be ﬁve curves. Make sure to label each one.\n\n\n\nExplain why each of the ﬁve curves has the shape displayed in part (a).\n\nIn the example f isn’t lineal, so the the test error lower as we add flexibility until the point the models starts to overfit. The training error always goes down as we increase the flexibility. As we make the model more flexible variance always increase as the model is more likely to change as we change the training data and the bias always goes down as a more flexible model has fewer assumptions. The Bayes error is the irreducible error we can not change it.\n\nYou will now think of some real-life applications for statistical learning.\n\n\nDescribe three real-life applications in which classiﬁcation might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n\n\n\n\n\n\nGoal\nResponse\nPredictors\n\n\n\nInference\nCustomer Tower Churn (0 or 1)\nAnnual Rent, Tower Lat, Tower Log, Tower Type, Number of sites around 10 km, Population around 10 km, Average Annual Salary in the city, contract Rent increases, customer technology\n\n\nInference\nEmployee Churn (0 or 1)\nMonths in company, Salary, Number of positions, Major, Sex, Total Salary Change, Bono, Wellness Expend, Number of depends, Home location\n\n\nInference\nAbsent (0 or 1)\nSalary, Rain?, Holiday?, Number of uniforms, distance from home to work place, Months in company, Neighborhood median Salary, number of depends, number of marriage, Work start, Work End, Free day\n\n\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n\n\n\n\n\n\nGoal\nResponse\nPredictors\n\n\n\nInference\nNumber of likes\nWords, Has a video?, Has a picture?, Post time, hashtag used\n\n\nInference\nVelocidad de picheo\nEdad, Altura, Peso, Horas de corrida, Cantidad de sentadillas, cantidad de practicas por semana, Años practicando el deporte\n\n\nInference\nFood Satisfaction level (0 to 10)\nCountry, City, Height, Weight, Salary (US $), Salt, Spacy Level, Sugar (gr), Meat Type, Cheese (gr), Cheese Type\n\n\n\n\nDescribe three real-life applications in which cluster analysis might be useful.\n\n\n\n\n\n\n\nGoal\nPredictors\n\n\n\nClassify costumer to improve advertising\nWords searched, products clicked, Explored image, Seconds spent on each product, start time, end time, customer location\n\n\nClassify company towers to see patterns in customers\nTower Lat, Tower Log, Tower Type, Number of sites around 10 km, Population around 10 km, Average Annual Salary in the city, BTS?, start date, Height\n\n\nClassify football players check which players have similar results\nNumber of passes on each game, Number of meters run on each game, Position Played, Number of goals, Number of stolen balls, total time played\n\n\n\n\nWhat are the advantages and disadvantages of a very ﬂexible (versus a less ﬂexible) approach for regression or classiﬁcation? Under what circumstances might a more ﬂexible approach be preferred to a less ﬂexible approach? When might a less ﬂexible approach be preferred?\n\n\nFlexible model advantages\n\nThey have the potential to accurately ﬁt a wider range of possible shapes for f\n\n\nFlexible model disadvantages\n\nThey do not reduce the problem of estimating f to a small number of parameters.\nA very large number of observations is required in order to obtain an accurate estimate for f.\nThey are harder to interpret\n\n\n\nIt is preferred when we have a lot of data to train the model and the goal is to get accurate predictions rather than good interpretations.\nA less flexible approach is preferred when we don’t have a lot data to train the model or when the main goal is to make inferences to understand business rules.\n\nDescribe the diﬀerences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classiﬁcation (as opposed to a nonparametric approach)? What are its disadvantages?\n\n\n\n\n\n\n\nParametric\nNon-parametric\n\n\n\nMake an assumption about the functional form\nDon’t make an assumption about the functional form, to accurately ﬁt a wider range of possible shapes for \\(f\\)\n\n\n\nEstimates a small number parameters based on training data\nEstimates a large number parameters based on training data\n\n\nCan be trained with few examples\nNeeds many examples to be trained\n\n\nSmoothness level is fixed\nData analyst needs define a level of smoothness\n\n\n\n\nParametric model advantages\n\nReduce the problem of estimating f to a small number of parameters.\nCan be trained with few examples.\nThey are easy to interpret.\n\n\nParametric model disadvantages\n\nIn many times \\(f\\) doesn’t have the assumed shape adding a lot of bias to the model.\n\n\n\n\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable.\n\n\nDF_07 &lt;-\n  data.frame(X1 = c(0,2,0,0,-1,1),\n             X2 = c(3,0,1,1,0,0),\n             X3 = c(0,0,3,2,1,1),\n             Y = c(\"Red\",\"Red\",\"Red\",\"Green\",\"Green\",\"Red\"))\n\nDF_07\n\n  X1 X2 X3     Y\n1  0  3  0   Red\n2  2  0  0   Red\n3  0  1  3   Red\n4  0  1  2 Green\n5 -1  0  1 Green\n6  1  0  1   Red\n\n\nSuppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.\n\nCompute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.\n\n\n# As (C - 0)^2 = C^2\n\nDF_07 &lt;- transform(DF_07, dist = sqrt(X1^2+X2^2+X3^2))\n\n\nWhat is our prediction with K = 1? Why?\n\n\nDF_07[order(DF_07$dist),]\n\n  X1 X2 X3     Y     dist\n5 -1  0  1 Green 1.414214\n6  1  0  1   Red 1.414214\n2  2  0  0   Red 2.000000\n4  0  1  2 Green 2.236068\n1  0  3  0   Red 3.000000\n3  0  1  3   Red 3.162278\n\n\nIn the is case, the point would be in the Bayes decision boundary as there are two points of different colors at the same distance.\n\nWhat is our prediction with K = 3? Why?\n\nIn this case, the point would be a Red one as 2 of 3 of them are from that color.\n\nIf the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?\n\nAs flexibility decrease as K gets bigger, for highly nonlinear Bayes decision boundary the best K value should be a small one."
  },
  {
    "objectID": "02-execises.html#applied",
    "href": "02-execises.html#applied",
    "title": "02 - Statistical Learning",
    "section": "Applied",
    "text": "Applied\n\nThis exercise relates to the College data set, which can be found in the ﬁle College.csv on the book website. It contains a number of variables for 777 diﬀerent universities and colleges in the US\n\nBefore reading the data into R, it can be viewed in Excel or a text editor.\n\nUse the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. You should notice that the ﬁrst column is just the name of each university. We don’t really want R to treat this as data.\n\n\ncollege &lt;- \n  here::here(\"data/College.csv\") |&gt;\n  read.csv(row.names = 1, stringsAsFactors = TRUE)\n\n\nUse the summary() function to produce a numerical summary of the variables in the data set.\n\n\nsummary(college)\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\nUse the pairs() function to produce a scatterplot matrix of the ﬁrst ten columns or variables of the data. \n\n\npairs(college[,1:10])\n\n\n\n\n\nUse the plot() function to produce side-by-side boxplots of Outstate versus Private.\n\n\nplot(college$Private, college$Outstate)\n\n\n\n\n\nCreate a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10 % of their high school classes exceeds 50 %.\n\n\ncollege$Elite &lt;- ifelse(college$Top10perc &gt; 50, \"Yes\", \"No\") |&gt; as.factor()\n\n\nThen Use the summary() function to see how many elite universities there are.\n\n\nsummary(college$Elite)\n\n No Yes \n699  78 \n\n\n\nNow use the plot() function to produce side-by-side boxplots of Outstate versus Elite.\n\n\nplot(college$Elite, college$Outstate)\n\n\n\n\n\nUse the hist() function to produce some histograms with diﬀering numbers of bins for a few of the quantitative variables. You may ﬁnd the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.\n\n\npar(mfrow = c(3, 1))\n\nhist(college$Apps)\nhist(college$Accept)\nhist(college$Enroll)\n\n\n\n\n\nContinue exploring the data, and provide a brief summary of what you discover.\n\n\npar(mfrow = c(2, 2))\n\nplot(college$S.F.Ratio, college$Expend)\nplot(college$S.F.Ratio, college$Outstate)\nplot(college$Top10perc, college$Terminal)\nplot(college$Top10perc, college$Room.Board)\n\n\n\npar(mfrow = c(1, 1))\n\nAs students have more resources like teaching, supervision, curriculum development, and pastoral support institutions tend to expend less on each student and quest less money from out state students.\nWe also can see that students from top 10 % of high school class tend to go to universities where most the professors have the highest academic level available for each field or the highest room and board costs\n\nThis exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n\n\nWhich of the predictors are quantitative, and which are qualitative?\n\n\nlibrary(ISLR2)\n\nquantitative_vars &lt;-\n  sapply(Auto, is.numeric) |&gt;\n  (\\(x) names(x)[x])()\n\nqualitative_vars &lt;- setdiff(names(Auto), quantitative_vars)\n\n\nWhat is the range of each quantitative predictor? You can answer this using the range() function.\n\n\nsapply(quantitative_vars, \\(var) range(Auto[[var]]))\n\n      mpg cylinders displacement horsepower weight acceleration year origin\n[1,]  9.0         3           68         46   1613          8.0   70      1\n[2,] 46.6         8          455        230   5140         24.8   82      3\n\n\n\nWhat is the mean and standard deviation of each quantitative predictor?\n\n\nsapply(quantitative_vars, \\(var) c(mean(Auto[[var]]), sd(Auto[[var]])))\n\n           mpg cylinders displacement horsepower    weight acceleration\n[1,] 23.445918  5.471939      194.412  104.46939 2977.5842    15.541327\n[2,]  7.805007  1.705783      104.644   38.49116  849.4026     2.758864\n          year    origin\n[1,] 75.979592 1.5765306\n[2,]  3.683737 0.8055182\n\n\n\nNow remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\n\nAutoFiltered &lt;-Auto[-c(10:85),]\n\nsapply(quantitative_vars, \\(var) c(mean(AutoFiltered[[var]]), sd(AutoFiltered[[var]])))\n\n           mpg cylinders displacement horsepower    weight acceleration\n[1,] 24.404430  5.373418    187.24051  100.72152 2935.9715    15.726899\n[2,]  7.867283  1.654179     99.67837   35.70885  811.3002     2.693721\n          year   origin\n[1,] 77.145570 1.601266\n[2,]  3.106217 0.819910\n\n\n\nUsing the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your ﬁndings.\n\nCars with 4 or 5 cylinders are more efficient than others.\n\nplot(factor(Auto$cylinders) ,Auto$mpg,\n     xlab = \"cylinders\", ylab = \"mpg\", col = 2)\n\n\n\n\nCars have improved their efficiency each year.\n\nplot(factor(Auto$year) ,Auto$mpg,\n     xlab = \"year\", ylab = \"mpg\", col = \"blue\")\n\n\n\n\n\nSuppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\n\n\npairs(Auto[, quantitative_vars])\n\n\n\n\ncylinders, horsepower and year are good candidates as they have correlations with the mpg variable.\n\nThis exercise involves the Boston housing data set.\n\n\nHow many rows are in this data set? How many columns? What do the rows and columns represent?\n\nA data frame with 506 rows and 13 variables. Suburbs are represented as rows and columns represent different indicators.\n\nMake some pairwise scatterplots of the predictors (columns) in this data set. Describe your ﬁndings.\n\nAs all the data is numeric, let’s get the highest correlations.\n\nBostonRelations &lt;-\n  cor(Boston) |&gt;\n  (\\(m) data.frame(row = rownames(m)[row(m)[upper.tri(m)]], \n                   col = colnames(m)[col(m)[upper.tri(m)]], \n                   cor = m[upper.tri(m)]))() |&gt;\n  (\\(DF) DF[order(-abs(DF$cor)),])() \n\nhead(BostonRelations,3)\n\n     row col        cor\n45   rad tax  0.9102282\n26   nox dis -0.7692301\n9  indus nox  0.7636514\n\n\nAs we can see, if a suburb has high accessibility to radial highways the house value also increase.\n\nwith(Boston, plot(rad, tax, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(dis, nox, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(indus, nox, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nAre any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\nwith(BostonRelations, BostonRelations[row == \"crim\", ])\n\n    row     col         cor\n29 crim     rad  0.62550515\n37 crim     tax  0.58276431\n56 crim   lstat  0.45562148\n7  crim     nox  0.42097171\n2  crim   indus  0.40658341\n67 crim    medv -0.38830461\n22 crim     dis -0.37967009\n16 crim     age  0.35273425\n46 crim ptratio  0.28994558\n11 crim      rm -0.21924670\n1  crim      zn -0.20046922\n4  crim    chas -0.05589158\n\n\n\nwith(Boston, plot(rad, crim, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nwith(Boston, plot(tax, crim, \n                  col = rgb(0 , 0, 1, alpha = 0.1),\n                  pch = 16, cex = 1.5))\n\n\n\n\n\nDo any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\n\npar(mfrow=c(3,1))\n\nboxplot(Boston$crim, horizontal = TRUE)\nhist(Boston$tax)\nhist(Boston$ptratio)\n\n\n\npar(mfrow=c(1,1))\n\n\nHow many of the census tracts in this data set bound the Charles river?\n\n\nsum(Boston$chas)\n\n[1] 35\n\n\n\nWhat is the median pupil-teacher ratio among the towns in this data set?\n\n\nmedian(Boston$ptratio)\n\n[1] 19.05\n\n\n\nWhich census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your ﬁndings.\n\n\nwhich.min(Boston$age)\n\n[1] 42\n\nMinOwnerOccupiedHomes &lt;- Boston[which.min(Boston$age),]\n\nMinOwnerOccupiedHomes\n\n      crim zn indus chas   nox   rm age    dis rad tax ptratio lstat medv\n42 0.12744  0  6.91    0 0.448 6.77 2.9 5.7209   3 233    17.9  4.84 26.6\n\nVarsToPlot &lt;-\n  names(Boston) |&gt;\n  setdiff(\"crim\")\n\nfor(variable in VarsToPlot){\n  \n  hist(Boston[[variable]],\n       main = paste(\"Histogram of\" , variable), xlab = variable)\n  abline(v=MinOwnerOccupiedHomes[[variable]],col=\"blue\",lwd=2)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.\n\n\nsum(Boston$rm &gt; 7)\n\n[1] 64\n\nsum(Boston$rm &gt; 8)\n\n[1] 13\n\nBostonRelations[BostonRelations$row == \"rm\" | BostonRelations$col == \"rm\",]\n\n     row     col         cor\n72    rm    medv  0.69535995\n61    rm   lstat -0.61380827\n13 indus      rm -0.39167585\n51    rm ptratio -0.35550149\n12    zn      rm  0.31199059\n15   nox      rm -0.30218819\n42    rm     tax -0.29204783\n21    rm     age -0.24026493\n11  crim      rm -0.21924670\n34    rm     rad -0.20984667\n27    rm     dis  0.20524621\n14  chas      rm  0.09125123\n\npar(mfrow=c(1,2))\nwith(Boston, plot(rm, medv, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\nabline(v=8,col=\"red\",lwd=2)\n \n\nwith(Boston, plot(rm, lstat, \n                  col = rgb(0 , 0, 1, alpha = 0.2),\n                  pch = 16, cex = 1.5))\nabline(v=8,col=\"red\",lwd=2)\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "03-execises.html#conceptual",
    "href": "03-execises.html#conceptual",
    "title": "03 - Linear Regression",
    "section": "Conceptual",
    "text": "Conceptual\n1\n1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\n\nNull hypotheses for each predictor each coefficient is 0. We can see in the table that we can reject the null hypotheses for TV and radio but there isn’t enough evidence to reject the null hypotheses for newspaper.\n\n2. Carefully explain the differences between the KNN classifier and KNN regression methods.\n\nThe classifier assigns classes based on the most often class of the closest \\(K\\) elements, on the other hand the regression estimate each value taking the mean of the closest \\(K\\) elements.\n\n3. Suppose we have a data set with five predictors to predict the starting salary after graduation (in thousands of dollars) and after using least squares we fitted the next model:\n\n\nVariable\nCoefficient\n\n\n\nLevel (High School)\n\\(\\hat{\\beta}_{0} = 50\\)\n\n\n\n\\(X_{1}\\) = GPA\n\\(\\hat{\\beta}_{1} = 20\\)\n\n\n\n\\(X_{2}\\) = IQ\n\\(\\hat{\\beta}_{2} = 0.07\\)\n\n\n\n\\(X_{3}\\) = Level (College)\n\\(\\hat{\\beta}_{3} = 35\\)\n\n\n\n\\(X_{4}\\) = Interaction between GPA and IQ\n\\(\\hat{\\beta}_{4} = 0.01\\)\n\n\n\n\\(X_{5}\\) = Interaction between GPA and Level\n\\(\\hat{\\beta}_{5} = −10\\)\n\n\n\nWhich answer is correct, and why?\n\nBased on this information we can say that:\n\n\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduate.\n\n\nAs High School students earn on average \\(\\hat{\\beta}_{0} = 50\\) College students earn |\\(\\hat{\\beta}_{0} + \\hat{\\beta}_{3} = 85\\)\n\n\n(A) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n\\[\n\\begin{split}\n\\hat{Y} & = 35 + 20 (4) + 0.07 (110) + 35 + 0.01(4)(110) - 10 (4) \\\\\n        & = 122.1\n\\end{split}\n\\]\n\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n\nFALSE, we can not make conclusions about the significance of any tern about checking the the standard error of each term. The coefficient might small because the IQ has very high values if we contrast the GPA ones.\n\nI collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_{0} + \\beta_{1}x + \\beta_{2}x^2 + \\beta_{3}x^3 + \\epsilon\\).\n\n\nSuppose that the true relationship between X and Y is linear, i.e. \\(Y = \\beta_{0} + \\beta_{1}x + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\nAs the training RSS always gets lower as we increase the flexibility the cubic regression would have a lower RSS.\n\nAnswer (a) using test rather than training RSS.\n\nThe linear regression would have a lower test RSS, as it reduces de scare bias of the model.\n\nSuppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\nAs the training RSS always gets lower as we increase the flexibility the cubic regression would have a lower RSS.\n\nAnswer (c) using test rather than training RSS.\n\nThe cubic regression would have a lower test RSS, as it reduces de scare bias of the model.\n\nConsider the fitted values that result from performing linear regression without an intercept. In this setting, the \\(i\\)th fitted value takes the form.\n\n\\[\n\\hat{y}_{i} = x_{i}\\hat{\\beta}\n\\]\nWhere\n\\[\n\\hat{\\beta}= \\left( \\sum_{i=1}^{n}{x_{i}y_{i}}  \\right) /\n             \\left( \\sum_{i'=1}^{n}{x_{i'}^2}  \\right)\n\\]\n\nShow that we can write\n\n\\[\n\\hat{y}_{i} = \\sum_{i'=1}^{n}{a_{i'}y_{i'}}\n\\] I am not sure about this execise as I don’t understand the difference between \\(i\\) and \\(i'\\).\n\\[\n\\begin{split}\n\\sum_{i'=1}^{n}{a_{i'}y_{i'}} & = x_{i}\\hat{\\beta} \\\\\n\\sum_{i'=1}^{n}{a_{i'}y_{i'}} & = x_{i}\\frac{\\sum_{i=1}^{n}{x_{i}y_{i}}}\n                                            {\\sum_{i'=1}^{n}{x_{i'}^2} } \\\\\n\\sum_{i'=1}^{n}{a_{i'}} \\sum_{i'=1}^{n}{y_{i'}} & = \\frac{x_{i}\\sum_{i=1}^{n}{x_{i}}}\n                                                         {\\sum_{i'=1}^{n}{x_{i'}^2} }\n                                                    \\sum_{i=1}^{n} {y_{i}} \\\\\n\\sum_{i'=1}^{n}{a_{i'}} & = \\frac{x_{i}\\sum_{i=1}^{n}{x_{i}}}\n                                                         {\\sum_{i'=1}^{n}{x_{i'}^2} }\n\\end{split}\n\\]\n\nUsing (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \\((\\overline{x},\\overline{x})\\).\n\nAs you can see bellow the intercept it’s the responsible for that property.\n\\[\n\\begin{split}\n\\hat{y} & = \\left( \\hat{\\beta}_{0} \\right) + \\hat{\\beta}_{1} \\overline{x} \\\\\n\\hat{y} & = \\overline{y} - \\hat{\\beta}_{1}\\overline{x} + \\hat{\\beta}_{1} \\overline{x} \\\\\n\\hat{y} & = \\overline{y}\n\\end{split}\n\\]"
  },
  {
    "objectID": "03-execises.html#applied",
    "href": "03-execises.html#applied",
    "title": "03 - Linear Regression",
    "section": "Applied",
    "text": "Applied\n\nThis question involves the use of simple linear regression on the Auto data set.\n\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.\n\n\nlibrary(ISLR2)\n\nAutoSimpleModel &lt;- lm(mpg ~ horsepower, data = Auto)\n\nsummary(AutoSimpleModel)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n\nAs we see the regression p-value is much lower than 0.05 and we can reject the null hypotheses to conclude that there is a strong relationship between the response en the predictor. The coefficient of horsepower is negative, so we know that as the predictor increase the response decrease.\n\nWhat is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals.\n\n\npredict(AutoSimpleModel, newdata = data.frame(horsepower = 98), interval = \"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\npredict(AutoSimpleModel, newdata = data.frame(horsepower = 98), interval = \"prediction\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\n\n\nPlot the response and the predictor. Use the abline() function to display the least squares regression line.\n\n\nplot(Auto$horsepower,Auto$mpg)\nabline(AutoSimpleModel)\n\n\n\n\n\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.\n\n\npar(mfrow = c(2, 2))\nplot(AutoSimpleModel)\n\n\n\n\nThe Residuals vs Fitted shows that the relation is not linear and variance isn’t constant.\n\nThis question involves the use of multiple linear regression on the Auto data set.\n\n\nProduce a scatterplot matrix which includes all of the variables in the data set.\n\n\npairs(Auto)\n\n\n\n\n\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.\n\n\nAuto |&gt;\n  subset(select = -name)|&gt;\n  cor()\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\n\n\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance\n\n\nAutoModelNoInteraction &lt;- \n  lm(mpg ~ . -name, data = Auto)\n\nAutoModelNoInteractionummary &lt;- \n  summary(AutoModelNoInteraction)\n\nAutoModelNoInteractionummary\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  &lt; 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  &lt; 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16\n\n\n\nIs there a relationship between the predictors and the response?\n\nAs the regression p-value is bellow 0.05 we can reject the null hypothesis and conclude that at least one of the predictors have a relation with the response.\n\nWhich predictors appear to have a statistically significant relationship to the response?\n\n\nAutoModelNoInteractionummary |&gt;\n  coefficients() |&gt;\n  as.data.frame() |&gt;\n  subset(`Pr(&gt;|t|)` &lt; 0.05)\n\n                  Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  -17.218434622 4.6442941494 -3.707438 2.401841e-04\ndisplacement   0.019895644 0.0075150792  2.647430 8.444649e-03\nweight        -0.006474043 0.0006520478 -9.928787 7.874953e-21\nyear           0.750772678 0.0509731223 14.728795 3.055983e-39\norigin         1.426140495 0.2781360924  5.127492 4.665681e-07\n\n\n\nWhat does the coefficient for the year variable suggest?\n\nIt suggests that cars in average cars can drive 0.75 more miles per gallon every year.\n\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n\nNon-linearity of the response-predictor relationships\nNon-constant variance\nHigh-leverage points\n\n\n\n\npar(mfrow = c(2, 2))\nplot(AutoModelNoInteraction)\n\n\n\n\n\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n\n\nremove_rownames &lt;- function(DF){\n  \n  DF &lt;- cbind(name = row.names(DF), DF)\n  rownames(DF) &lt;- NULL\n  return(DF)\n  \n}\n\n\nnames(Auto) |&gt;\n  setdiff(c(\"mpg\",\"name\")) |&gt;\n  (\\(x) c(x,\n          combn(x, m = 2, \n                FUN = \\(y) paste0(y,collapse =\":\"))))() |&gt;\n  paste0(collapse = \" + \") |&gt;\n  paste0(\"mpg ~ \", predictors = _) |&gt;\n  lm(data = Auto) |&gt;\n  summary() |&gt;\n  coef() |&gt;\n  as.data.frame() |&gt;\n  remove_rownames() |&gt;\n  subset(`Pr(&gt;|t|)` &lt; 0.05 | name == \"year\")\n\n                  name      Estimate  Std. Error   t value    Pr(&gt;|t|)\n3         displacement  -0.478538689 0.189353429 -2.527225 0.011920695\n6         acceleration  -5.859173212 2.173621188 -2.695582 0.007353578\n7                 year   0.697430284 0.609670317  1.143947 0.253399572\n8               origin -20.895570401 7.097090511 -2.944245 0.003445892\n18   displacement:year   0.005933802 0.002390716  2.482019 0.013515633\n27   acceleration:year   0.055621508 0.025581747  2.174265 0.030330641\n28 acceleration:origin   0.458316099 0.156659694  2.925552 0.003654670\n\n\n\nTry a few different transformations of the variables, such as \\(\\log{x}\\), \\(\\sqrt{x}\\), \\(x^2\\). Comment on your findings.\n\nAs we can see bellow we can explain 3% more of the variability by applying log to some variables.\n\nlibrary(data.table)\n\napply_fun_lm &lt;- function(FUN,DF, trans_vars, remove_vars){\n  \n    as.data.table(DF\n    )[, (trans_vars) := lapply(.SD, FUN), .SDcols = trans_vars\n    ][, !remove_vars, with = FALSE\n    ][, lm(mpg ~ . , data = .SD)] |&gt;\n    summary() |&gt;\n    (\\(x) data.table(adj.r.squared = x$adj.r.squared,\n                     sigma = x$sigma,\n                     p.value = pf(x$fstatistic[\"value\"], \n                                  x$fstatistic[\"numdf\"], \n                                  x$fstatistic[\"dendf\"], \n                                  lower.tail = FALSE)))()\n  \n}\n\n\ndata.table(function_name = c(\"original\",\"log\", \"sqrt\",\"x^2\"),\n           function_list = list(\\(x) x,log, sqrt, \\(x) x^2)\n)[, data :=  \n    lapply(function_list, \n           FUN = apply_fun_lm,\n           DF = Auto, \n           trans_vars = c(\"displacement\", \"horsepower\", \n                          \"weight\", \"acceleration\"),\n           remove_vars = \"name\")\n][, rbindlist(data) |&gt; cbind(function_name, end = _)]\n\n   function_name end.adj.r.squared end.sigma   end.p.value\n          &lt;char&gt;             &lt;num&gt;     &lt;num&gt;         &lt;num&gt;\n1:      original         0.8182238  3.327682 2.037106e-139\n2:           log         0.8474528  3.048425 5.352738e-154\n3:          sqrt         0.8312704  3.206041 1.304165e-145\n4:           x^2         0.7986663  3.502124 6.372862e-131\n\n\n\nThis question should be answered using the Carseats data set.\n\n\nFit a multiple regression model to predict Sales using Price, Urban, and US.\n\n\nCarseatsModel &lt;-\n  lm(Sales~Price+Urban+US, data = Carseats)\n\nCarseatsModelSummary &lt;-  \n  summary(CarseatsModel)\n\nCarseatsModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price + Urban + US, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9206 -1.6220 -0.0564  1.5786  7.0581 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13.043469   0.651012  20.036  &lt; 2e-16 ***\nPrice       -0.054459   0.005242 -10.389  &lt; 2e-16 ***\nUrbanYes    -0.021916   0.271650  -0.081    0.936    \nUSYes        1.200573   0.259042   4.635 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.472 on 396 degrees of freedom\nMultiple R-squared:  0.2393,    Adjusted R-squared:  0.2335 \nF-statistic: 41.52 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\nProvide an interpretation of each coeﬃcient in the model. Be careful—some of the variables in the model are qualitative!\n\n\nCarseatsInterationModel &lt;-\n  lm(Sales~Price*Urban*US, data = Carseats)\n\nCarseatsInterationModelSummary &lt;-  \n  summary(CarseatsInterationModel)\n\nCarseatsInterationModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price * Urban * US, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7952 -1.6659 -0.0984  1.6119  7.2433 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          13.456350   1.727210   7.791 6.03e-14 ***\nPrice                -0.061657   0.014875  -4.145 4.17e-05 ***\nUrbanYes             -0.651545   2.071401  -0.315    0.753    \nUSYes                 2.049051   2.322591   0.882    0.378    \nPrice:UrbanYes        0.010793   0.017796   0.606    0.545    \nPrice:USYes          -0.001567   0.019972  -0.078    0.937    \nUrbanYes:USYes       -1.122034   2.759662  -0.407    0.685    \nPrice:UrbanYes:USYes  0.001288   0.023619   0.055    0.957    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.473 on 392 degrees of freedom\nMultiple R-squared:  0.2467,    Adjusted R-squared:  0.2333 \nF-statistic: 18.34 on 7 and 392 DF,  p-value: &lt; 2.2e-16\n\n\n\nWrite out the model in equation form, being careful to handle the qualitative variables properly.\n\n\ncoef(CarseatsInterationModel) |&gt;\n  round(3) |&gt;\n  (\\(x) paste0(ifelse(x &lt; 0, \" - \",\" + \"), abs(x),\" \\text{ \", names(x),\"}\"))() |&gt;\n  sub(pattern = \" \\text{ (Intercept)}\",replacement = \"\", fixed = TRUE) |&gt;\n  paste0(collapse = \"\") |&gt;\n  sub(pattern = \"^ \\\\+ \", replacement = \"\") |&gt;\n  sub(pattern = \"^ - \", replacement = \"\") |&gt;\n  paste0(\"hat{Y} = \", FUN = _)\n\n[1] \"hat{Y} = 13.456 - 0.062 \\text{ Price} - 0.652 \\text{ UrbanYes} + 2.049 \\text{ USYes} + 0.011 \\text{ Price:UrbanYes} - 0.002 \\text{ Price:USYes} - 1.122 \\text{ UrbanYes:USYes} + 0.001 \\text{ Price:UrbanYes:USYes}\"\n\n\n\\[\n\\begin{split}\n\\hat{Sales} & = 13.456 - 0.062 \\text{ Price} - 0.652 \\text{ UrbanYes} \\\\\n            & \\quad + 2.049 \\text{ USYes} + 0.011 \\text{ Price:UrbanYes} \\\\\n            & \\quad - 0.002 \\text{ Price:USYes} - 1.122 \\text{ UrbanYes:USYes} \\\\\n            & \\quad + 0.001 \\text{ Price:UrbanYes:USYes}\n\\end{split}\n\\]\n\nFor which of the predictors can you reject the null hypothesis H0 : βj = 0?\n\n\ncoef(CarseatsInterationModelSummary) |&gt;\n  as.data.frame() |&gt;\n  (\\(DF) DF[DF$`Pr(&gt;|t|)` &lt; 0.05,])()\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 13.45634952 1.72720976  7.790802 6.030364e-14\nPrice       -0.06165717 0.01487479 -4.145079 4.165536e-05\n\n\n\nOn the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n\n\nCarseatsPriceModel &lt;-\n  lm(Sales~Price, data = Carseats)\n\nCarseatsPriceModelSummary &lt;-\n  summary(CarseatsPriceModel)\n\nCarseatsPriceModelSummary\n\n\nCall:\nlm(formula = Sales ~ Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13.641915   0.632812  21.558   &lt;2e-16 ***\nPrice       -0.053073   0.005354  -9.912   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: &lt; 2.2e-16\n\n\n\nHow well do the models in (a) and (e) ﬁt the data?\n\nModel a fits better to the data with 0.23 against 0.2 of model e.\n\nUsing the model from (e), obtain 95 % conﬁdence intervals for the coeﬃcient(s).\n\n\nconfint(CarseatsPriceModel, level = 0.95)\n\n                 2.5 %      97.5 %\n(Intercept) 12.3978438 14.88598655\nPrice       -0.0635995 -0.04254653\n\n\n\nIs there evidence of outliers or high leverage observations in the model from (e)?\n\n\npar(mfrow = c(2,2))\nplot(CarseatsPriceModel)\n\n\n\npar(mfrow = c(1,1))\n\nThere is a leverage point.\n\nIn this problem we will investigate the t-statistic for the null hypothesis H0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.\n\n\nset.seed(1)\n\nx &lt;- rnorm(100)\ny &lt;- 2*x+rnorm(100)\n\nSimulatedData &lt;- data.frame(x, y) \n\n\nPerform a simple linear regression of y onto x, without an intercept. Report the coeﬃcient estimate ˆβ, the standard error of this coeﬃcient estimate, and the t-statistic and p-value associated with the null hypothesis H0 : β = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).)\n\nAs we can see below we can reject the null hypothesis and conclude that y increases 1.99 for each unit of x explaining 78% of the variability.\n\nlm(y~ x+0, data = SimulatedData) |&gt;\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9154 -0.6472 -0.1771  0.5056  2.3109 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nx   1.9939     0.1065   18.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9586 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\nNow perform a simple linear regression of x onto y without an intercept, and report the coeﬃcient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis H0 : β = 0. Comment on these results.\n\nAs we can see below we can reject the null hypothesis and conclude that x increases 0.39 for each unit of y explaining 78% of the variability.\n\nlm(x~ y+0, data = SimulatedData) |&gt;\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8699 -0.2368  0.1030  0.2858  0.8938 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \ny  0.39111    0.02089   18.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4246 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat is the relationship between the results obtained in (a) and (b)?\n\ny can explain x as well a x explains y.\n\nIn R, show that when regression is performed with an intercept, the t-statistic for H0 : β1 = 0 is the same for the regression of y onto x as it is for the regression of x onto y.\n\nAs you can see below the t-statistic for \\(\\beta_{1}\\) is t-statistic for both regressions is 18.56.\n\nlm(y~ x, data = SimulatedData) |&gt;\n  summary()\n\n\nCall:\nlm(formula = y ~ x, data = SimulatedData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8768 -0.6138 -0.1395  0.5394  2.3462 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.03769    0.09699  -0.389    0.698    \nx            1.99894    0.10773  18.556   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9628 on 98 degrees of freedom\nMultiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 \nF-statistic: 344.3 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\nlm(x~ y, data = SimulatedData) |&gt;\n  summary()\n\n\nCall:\nlm(formula = x ~ y, data = SimulatedData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90848 -0.28101  0.06274  0.24570  0.85736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03880    0.04266    0.91    0.365    \ny            0.38942    0.02099   18.56   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4249 on 98 degrees of freedom\nMultiple R-squared:  0.7784,    Adjusted R-squared:  0.7762 \nF-statistic: 344.3 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\nThis problem involves simple linear regression without an intercept.\n\n\nRecall that the coeﬃcient estimate β for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coeﬃcient estimate for the regression of X onto Y the same as the coeﬃcient estimate for the regression of Y onto X?\n\nThe coefficient would be different between y~x and x~y.\n\nGenerate an example in R with n = 100 observations in which the coeﬃcient estimate for the regression of X onto Y is diﬀerent from the coeﬃcient estimate for the regression of Y onto X.\n\n\nset.seed(5)\nSimulatedData2 &lt;- \n  data.frame(x = rnorm(100, mean = 8, sd = 4))\n\nset.seed(8)\nSimulatedData2$y &lt;- \n  10*SimulatedData2$x + rnorm(100, sd = 10) \n\nplot(SimulatedData2$x, SimulatedData2$y)\n\n\n\nlm(y~ x+0, data = SimulatedData2) |&gt;\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.7686  -6.8107  -0.3744   6.5070  24.3187 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nx   9.9363     0.1207    82.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.81 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\nlm(x~ y+0, data = SimulatedData2) |&gt;\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2861 -0.5429  0.1264  0.7279  3.0421 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \ny 0.099191   0.001205    82.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.08 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\nGenerate an example in R with n = 100 observations in which the coeﬃcient estimate for the regression of X onto Y is the same as the coeﬃcient estimate for the regression of Y onto X.\n\n\nset.seed(5)\nSimulatedData3 &lt;- \n  data.frame(x = rnorm(100, mean = 8, sd = 4))\n\nset.seed(8)\nSimulatedData3$y &lt;- \n  SimulatedData3$x + rnorm(100, sd = 1) \n\nplot(SimulatedData3$x, SimulatedData3$y)\n\n\n\nlm(y~ x+0, data = SimulatedData3) |&gt;\n  summary()\n\n\nCall:\nlm(formula = y ~ x + 0, data = SimulatedData3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.97686 -0.68107 -0.03744  0.65070  2.43187 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nx  0.99363    0.01207    82.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.081 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\nlm(x~ y+0, data = SimulatedData3) |&gt;\n  summary()\n\n\nCall:\nlm(formula = x ~ y + 0, data = SimulatedData3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2861 -0.5429  0.1264  0.7279  3.0421 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \ny  0.99191    0.01205    82.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.08 on 99 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9854 \nF-statistic:  6774 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\nIn this exercise you will create some simulated data and will ﬁt simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.\n\n\nUsing the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0, 1) distribution. This represents a feature, X.\n\n\nset.seed(1)\n\nx &lt;- rnorm(100)\n\n\nUsing the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution—a normal distribution with mean zero and variance 0.25.\n\n\neps &lt;- rnorm(100, sd = sqrt(0.25))\n\n\nUsing x and eps, generate a vector y according to the model.\n\n\\[\nY = -1 + 0.5X + \\epsilon\n\\]\n\ny &lt;- -1 + 0.5*x +eps\n\n\n- **What is the length of the vector y?**\n\nIt has the same length of x.\n\n\nWhat are the values of β0 and β1 in this linear model?\n\n\n\\(\\beta_{0} = -1\\) and \\(\\beta_{1} = 0.5\\).\n\nCreate a scatterplot displaying the relationship between x and y. Comment on what you observe.\n\n\nplot(x,y)\n\n\n\n\n\nFit a least squares linear model to predict y using x. Comment on the model obtained. How do ˆβ0 and ˆ β1 compare to β0 and β1?\n\nAfter rounding the value to one decimal the coefficients are the same.\n\nSimilatedModel &lt;-lm(y~x)\n\nSimilatedModel |&gt;\n  coef() |&gt;\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\n\nDisplay the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a diﬀerent color. Use the legend() command to create an appropriate legend.\n\n\nplot(x,y)\nabline(SimilatedModel, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nNow ﬁt a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model ﬁt? Explain your answer.\n\nThere is no evidence that the polynomial model fits better to the data.\n\nSimilatedPolyModel &lt;-lm(y~x+I(x^2))\n\nanova(SimilatedModel,SimilatedPolyModel)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ x + I(x^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     98 22.709                           \n2     97 22.257  1   0.45163 1.9682 0.1638\n\n\n\nRepeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term ϵ in (b). Describe your results.\n\n\nset.seed(1)\n\nx &lt;- rnorm(100)\neps &lt;- rnorm(100, sd = sqrt(0.10))\ny &lt;- -1 + 0.5*x +eps\n\nplot(x,y)\n\n\n\n\nThe coefficients remind the same.\n\nSimilatedModel2 &lt;-lm(y~x)\n\nSimilatedModel2 |&gt;\n  coef() |&gt;\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\nAnd the Lease Square Line and the Population Line are closer.\n\nplot(x,y)\nabline(SimilatedModel2, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nRepeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term ϵ in (b). Describe your results.\n\n\nset.seed(1)\n\nx &lt;- rnorm(100)\neps &lt;- rnorm(100, sd = sqrt(1.5))\ny &lt;- -1 + 0.5*x +eps\n\nplot(x,y)\n\n\n\n\nThe coefficients remind the same.\n\nSimilatedModel3 &lt;-lm(y~x)\n\nSimilatedModel3 |&gt;\n  coef() |&gt;\n  round(1)\n\n(Intercept)           x \n       -1.0         0.5 \n\n\nDespite, y has a wider range of values are almost the same.\n\nplot(x,y)\nabline(SimilatedModel3, col = \"red\")\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2.35, 0.40 , \n       legend = c(\"Lease Square Line\", \"Population Line\"), \n       col = c(\"red\",\"blue\"), lty=1, cex=0.8)\n\n\n\n\n\nWhat are the conﬁdence intervals for β0 and β1 based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.\n\n\nlibrary(ggplot2)\n\n\nadd_source &lt;- function(list.DT, source.name = \"source\"){\n  \n  table_names &lt;- names(list.DT)\n  \n  for(tb_i in seq_along(list.DT)){\n    list.DT[[tb_i]][, (source.name) := names(list.DT)[tb_i] ] \n  }\n  \n  return(list.DT)\n}\n\n\nlist(original = SimilatedModel,\n     less_noisy = SimilatedModel2,\n     noisier = SimilatedModel3) |&gt;\n lapply(\\(model) cbind(center = coef(model), confint(model)) |&gt; \n                 as.data.table(keep.rownames = \"coef\")) |&gt;\n add_source(source.name = \"model\") |&gt;\n rbindlist() |&gt;\n (\\(DT) DT[, model := factor(model, \n                        levels = c(\"less_noisy\", \"original\",\"noisier\"))] )() |&gt;\n ggplot(aes(model, center, color = model))+\n  geom_hline(yintercept = 0, linetype = 2, size = 1)+\n  geom_point()+\n  geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`), width = 0.5)+\n  scale_color_brewer(palette = \"Blues\")+\n  facet_wrap(~coef, ncol = 2, scales = \"free_y\")+\n  labs(title = \"Coefficient Confident Intervals get wider\",\n       subtitle = \"as the error increase but it isn't enough to change conclusions\")+\n  theme_classic()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n\n\n\n\nThis problem focuses on the collinearity problem.\n\n\nPerform the following commands in R:\n\n\nset.seed(1)\nx1 &lt;- runif(100)\nx2 &lt;- 0.5*x1+rnorm(100) / 10\ny &lt;- 2+2*x1+0.3*x2+rnorm(100)\n\n\nThe last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coeﬃcients?\n\n\\[\nY = 2 + 2 x_{1} + 0.3 x_{2} + \\epsilon\n\\]\n\nWhat is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.\n\n\nplot(x1,x2, \n     main = paste0(\"x1 and x2 correlation :\",round(cor(x1,x2), 2)))\n\n\n\n\n\nUsing this data, ﬁt a least squares regression to predict y using x1 and x2. Describe the results obtained. What are ˆβ0, ˆ β1, and ˆβ2? How do these relate to the true β0, β1, and β2? Can you reject the null hypothesis H0 : β1 = 0? How about the null hypothesis H0 : β2 = 0?\n\n\nSimulatedModelExc14 &lt;- lm(y~x1+x2)\n\nSimulatedModelExc14Summary &lt;- summary(SimulatedModelExc14)\n\nSimulatedModelExc14Summary\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8311 -0.7273 -0.0537  0.6338  2.3359 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1305     0.2319   9.188 7.61e-15 ***\nx1            1.4396     0.7212   1.996   0.0487 *  \nx2            1.0097     1.1337   0.891   0.3754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.056 on 97 degrees of freedom\nMultiple R-squared:  0.2088,    Adjusted R-squared:  0.1925 \nF-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05\n\n\nThe \\(\\hat{\\beta}_{0} = 2.13\\) which is really close to the true value of \\(\\beta_{0} = 2\\), but \\(\\hat{\\beta}_{1}\\) and \\(\\hat{\\beta}_{2}\\) are very different to their real values. We almost can not reject the null hypothesis for \\(\\beta_{1}\\) and can not reject the null hypothesis for \\(\\beta_{1}\\) where both should be significant to explain \\(\\hat{Y}\\).\n\nNow ﬁt a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?\n\n\nlm(y~x1) |&gt;\n  summary()\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89495 -0.66874 -0.07785  0.59221  2.45560 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1124     0.2307   9.155 8.27e-15 ***\nx1            1.9759     0.3963   4.986 2.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.055 on 98 degrees of freedom\nMultiple R-squared:  0.2024,    Adjusted R-squared:  0.1942 \nF-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06\n\n\nNow \\(\\beta_{1}\\) we can surely reject the null t-value is now 2.5 times higher that it used to be.\n-Now ﬁt a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?\n\nlm(y~x2) |&gt;\n  summary()\n\n\nCall:\nlm(formula = y ~ x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62687 -0.75156 -0.03598  0.72383  2.44890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.3899     0.1949   12.26  &lt; 2e-16 ***\nx2            2.8996     0.6330    4.58 1.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.072 on 98 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.1679 \nF-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05\n\n\nNow \\(\\beta_{2}\\) we can surely reject the null t-value is now 5.14 times higher that it used to be.\n\nDo the results obtained in (c)–(e) contradict each other? Explain your answer.\n\nYes, they do. In c, we couldn’t reject the null hypothesis for x2 but that change in the e question.\n\nNow suppose we obtain one additional observation, which was unfortunately mismeasured.\n\n\nx1_c&lt;-c(x1, 0.1)\nx2_c&lt;-c(x2, 0.8)\ny_c&lt;-c(y, 6)\n\n\nRe-ﬁt the linear models from (c) to (e) using this new data. What eﬀect does this new observation have on the each of the models?\n\nThanks the additional row x2 seems to be significant rather than x1.\n\nModelC &lt;- lm(y_c~x1_c+x2_c) \nsummary(ModelC)\n\n\nCall:\nlm(formula = y_c ~ x1_c + x2_c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.73348 -0.69318 -0.05263  0.66385  2.30619 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.2267     0.2314   9.624 7.91e-16 ***\nx1_c          0.5394     0.5922   0.911  0.36458    \nx2_c          2.5146     0.8977   2.801  0.00614 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.075 on 98 degrees of freedom\nMultiple R-squared:  0.2188,    Adjusted R-squared:  0.2029 \nF-statistic: 13.72 on 2 and 98 DF,  p-value: 5.564e-06\n\n\nIn the next model, we can see that the previous model was fitting better to y based on x1. The \\(R^2\\) went down from 0.20 to 0.16.\n\nModelD &lt;- lm(y_c~x1_c) \nsummary(ModelD)\n\n\nCall:\nlm(formula = y_c ~ x1_c)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8897 -0.6556 -0.0909  0.5682  3.5665 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.2569     0.2390   9.445 1.78e-15 ***\nx1_c          1.7657     0.4124   4.282 4.29e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.111 on 99 degrees of freedom\nMultiple R-squared:  0.1562,    Adjusted R-squared:  0.1477 \nF-statistic: 18.33 on 1 and 99 DF,  p-value: 4.295e-05\n\n\nIn the next model, we can see that the previous model was fitting worse to y based on x2. The \\(R^2\\) went up from 0.18 to 0.21.\n\nModelE &lt;- lm(y_c~x2_c) \nsummary(ModelE)\n\n\nCall:\nlm(formula = y_c ~ x2_c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.64729 -0.71021 -0.06899  0.72699  2.38074 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.3451     0.1912  12.264  &lt; 2e-16 ***\nx2_c          3.1190     0.6040   5.164 1.25e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.074 on 99 degrees of freedom\nMultiple R-squared:  0.2122,    Adjusted R-squared:  0.2042 \nF-statistic: 26.66 on 1 and 99 DF,  p-value: 1.253e-06\n\n\n\nIn each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.\n\nIn the c model the last observation is a high-leverage point.\n\npar(mfrow = c(2,2))\nplot(ModelC)\n\n\n\n\nIn the d model the last observation is an outlier point as it’s studentized residuals is greater than 3.\n\npar(mfrow = c(2,2))\nplot(ModelD)\n\n\n\n\nIn the e model the last observation is a high-leverage point.\n\npar(mfrow = c(2,2))\nplot(ModelE)\n\n\n\n\n\nThis problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n\n\nFor each predictor, ﬁt a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically signiﬁcant association between the predictor and the response? Create some plots to back up your assertions.\n\nAs we can see below the only predictor that wasn’t found significant was the chas one.\n\nBostonModelSummary &lt;-\n  data.table(predictor = colnames(Boston) |&gt; setdiff(\"crim\")\n  )[, model := lapply(predictor, \\(x) paste0(\"crim~\",x) |&gt; \n                           lm(data = Boston) |&gt;\n                           summary() |&gt;\n                           coef() |&gt;\n                           as.data.table(keep.rownames = \"coef\")) \n  ][, model[[1]], \n    by = \"predictor\"\n  ][predictor == coef, !c(\"coef\")\n  ][, is_significant :=  `Pr(&gt;|t|)` &lt; 0.05\n  ][order(`Pr(&gt;|t|)`)]\n\nBostonModelSummary\n\n    predictor    Estimate  Std. Error   t value     Pr(&gt;|t|) is_significant\n       &lt;char&gt;       &lt;num&gt;       &lt;num&gt;     &lt;num&gt;        &lt;num&gt;         &lt;lgcl&gt;\n 1:       rad  0.61791093 0.034331820 17.998199 2.693844e-56           TRUE\n 2:       tax  0.02974225 0.001847415 16.099388 2.357127e-47           TRUE\n 3:     lstat  0.54880478 0.047760971 11.490654 2.654277e-27           TRUE\n 4:       nox 31.24853120 2.999190381 10.418989 3.751739e-23           TRUE\n 5:     indus  0.50977633 0.051024332  9.990848 1.450349e-21           TRUE\n 6:      medv -0.36315992 0.038390175 -9.459710 1.173987e-19           TRUE\n 7:       dis -1.55090168 0.168330031 -9.213458 8.519949e-19           TRUE\n 8:       age  0.10778623 0.012736436  8.462825 2.854869e-16           TRUE\n 9:   ptratio  1.15198279 0.169373609  6.801430 2.942922e-11           TRUE\n10:        rm -2.68405122 0.532041083 -5.044819 6.346703e-07           TRUE\n11:        zn -0.07393498 0.016094596 -4.593776 5.506472e-06           TRUE\n12:      chas -1.89277655 1.506115484 -1.256727 2.094345e-01          FALSE\n\n\nAny relation seems to be really linear and the chas predictor has been wrongly classify as numeric when it should have been a qualitative variable.\n\nfor(predictor in BostonModelSummary$predictor){\n \n  cor(Boston[[predictor]],Boston$crim) |&gt;\n  round(2) |&gt;\n  paste0(\"Crim vs \", predictor,\"\\nCorrelation :\", correlation = _) |&gt;\n  plot(Boston[[predictor]],Boston$crim, \n       xlab = predictor, ylab = \"crim\", \n       main = _)\n   \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut after changing chas to a factor we keep the same conclusion and the coefficient it’s the same.\n\nlm(crim~as.factor(chas), data = Boston) |&gt;\n  summary()\n\n\nCall:\nlm(formula = crim ~ as.factor(chas), data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.738 -3.661 -3.435  0.018 85.232 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.7444     0.3961   9.453   &lt;2e-16 ***\nas.factor(chas)1  -1.8928     1.5061  -1.257    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.597 on 504 degrees of freedom\nMultiple R-squared:  0.003124,  Adjusted R-squared:  0.001146 \nF-statistic: 1.579 on 1 and 504 DF,  p-value: 0.2094\n\n\n\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0\n\nNow we just can reject the null hypothesis for the following predictos:\n\nBostonModel2 &lt;-\n  lm(formula = crim ~ . , data = Boston)\n\nBostonModelSummary2 &lt;-\n  summary(BostonModel2) |&gt;\n  coef() |&gt;\n  as.data.table(keep.rownames = \"predictor\")\n\nBostonModelSummary2[`Pr(&gt;|t|)` &lt; 0.05]\n\n   predictor    Estimate Std. Error   t value     Pr(&gt;|t|)\n      &lt;char&gt;       &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:        zn  0.04571004 0.01879032  2.432637 1.534403e-02\n2:       dis -1.01224674 0.28246757 -3.583586 3.725942e-04\n3:       rad  0.61246531 0.08753576  6.996744 8.588123e-12\n4:      medv -0.22005636 0.05982396 -3.678399 2.605302e-04\n\n\n\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coeﬃcients from (a) on the x-axis, and the multiple regression coeﬃcients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coeﬃcient in a simple linear regression model is shown on the x-axis, and its coeﬃcient estimate in the multiple linear regression model is shown on the y-axis.\n\n\nmerge(BostonModelSummary, BostonModelSummary2,\n      by = \"predictor\", suffixes = c(\"_uni\",\"_multi\")\n  )[, .(predictor,\n        Estimate_uni = round(Estimate_uni, 2),\n        Estimate_multi = round(Estimate_multi, 2), \n        coef_change = abs(Estimate_uni / Estimate_multi),\n        vif = car::vif(BostonModel2)[predictor],\n        kept_significant = is_significant & `Pr(&gt;|t|)_multi` &lt; 0.05)\n  ][, predictor := reorder(predictor, coef_change)] |&gt;\n  ggplot(aes(coef_change,vif))+\n  geom_point(aes(color = kept_significant))+\n  geom_text(aes(label = predictor), vjust = 1.2)+\n  scale_color_manual(values = c(\"TRUE\" = \"dodgerblue4\", \"FALSE\" = \"gray80\"))+\n  scale_x_log10()+\n  scale_y_log10()+\n  labs(title = \"Significan Predictors Change Less\",\n       subtitle = \"Predictos Coeﬃcient Change Between Simple and Multiple Lineal Models\")+\n  theme_classic()+\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n\n\n\n\nIs there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, ﬁt a model of the form.\n\n\\[\nY = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^2 + \\beta_{3} X^2 + \\epsilon\n\\]\n\nSimpleVsPolySummary &lt;-\n  data.table(predictor = colnames(Boston) |&gt; setdiff(c(\"crim\",\"chas\"))\n  )[,`:=`(r2_simple = sapply(predictor, \\(x) paste0(\"crim~\",x) |&gt; \n                               lm(data = Boston) |&gt;\n                               summary() |&gt;\n                               (\\(x) x[[\"r.squared\"]])() ),\n          r2_poly = sapply(predictor, \\(x) gsub(\"x\",x,\"crim~x+I(x^2)+I(x^3)\") |&gt; \n                             lm(data = Boston) |&gt;\n                             summary() |&gt;\n                             (\\(x) x[[\"r.squared\"]])() )), \n  ][, change := r2_poly - r2_simple\n  ][order(-change)]\n\n\nSimpleVsPolySummary[, lapply(.SD, \\(x) \n                             if(is.numeric(x)) scales::percent(x, accuracy = 0.01)\n                             else x)]\n\n    predictor r2_simple r2_poly change\n       &lt;char&gt;    &lt;char&gt;  &lt;char&gt; &lt;char&gt;\n 1:      medv    15.08%  42.02% 26.94%\n 2:       dis    14.41%  27.78% 13.37%\n 3:       nox    17.72%  29.70% 11.98%\n 4:     indus    16.53%  25.97%  9.43%\n 5:       age    12.44%  17.42%  4.98%\n 6:   ptratio     8.41%  11.38%  2.97%\n 7:       tax    33.96%  36.89%  2.93%\n 8:        rm     4.81%   6.78%  1.97%\n 9:        zn     4.02%   5.82%  1.81%\n10:     lstat    20.76%  21.79%  1.03%\n11:       rad    39.13%  40.00%  0.88%\n\nfor(predictor in SimpleVsPolySummary[change &gt;= 0.1 ,predictor]){\n \n  cor(Boston[[predictor]],Boston$crim) |&gt;\n  round(2) |&gt;\n  paste0(\"Crim vs \", predictor,\"\\nCorrelation :\", correlation = _) |&gt;\n  plot(Boston[[predictor]],Boston$crim, \n       xlab = predictor, ylab = \"crim\", \n       main = _)\n   \n}"
  },
  {
    "objectID": "04-execises.html#libraries",
    "href": "04-execises.html#libraries",
    "title": "04 - Classification",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(data.table)\nlibrary(tidymodels)\nlibrary(discrim)\nlibrary(klaR)\nlibrary(poissonreg)"
  },
  {
    "objectID": "04-execises.html#custom-functions",
    "href": "04-execises.html#custom-functions",
    "title": "04 - Classification",
    "section": "Custom functions",
    "text": "Custom functions\nIn this section I place the functions that were used in many code chunks across the chapter.\n\nrbind_list_name &lt;- function(list.DT, new_col_name, ...){\n  \n  stopifnot(is.list(list.DT))\n  stopifnot(all(sapply(list.DT, is.data.table)))\n  \n  lapply(seq_along(list.DT), \n         function(tb_i){\n           list.DT[[tb_i]][, (new_col_name) := names(list.DT)[tb_i]] }) |&gt;\n  rbindlist(...)\n}\n\nget_class_metrics &lt;- metric_set(sens, spec)\n\nget_class_matrix_metrics &lt;- function(model,\n                                     split,\n                                     fit_formula,\n                                     metric_function = metric_set(sens, spec),\n                                     ...){\n  \n testing_predictions &lt;-\n  model |&gt;\n  last_fit(as.formula(fit_formula), split = split) |&gt;\n  collect_predictions() \n \n truth_var &lt;- sub(pattern = \" *~.+$\", replacement = \"\", x = fit_formula)\n \n list(conf_mat = conf_mat(testing_predictions, \n                          truth = !!truth_var, \n                          estimate = .pred_class),\n      metrics = metric_function(testing_predictions,\n                                truth = !!truth_var,\n                                estimate = .pred_class,\n                                ...) )\n}\n\n\nmodel_knn_k &lt;- function(k){\n  \n  model &lt;-\n    nearest_neighbor(neighbors = k) |&gt;\n    set_mode(\"classification\") |&gt;\n    set_engine(\"kknn\")\n  \n  return(model)\n}\n\n\nModelNaiveBayes &lt;- \n  naive_Bayes() |&gt;\n  set_mode(\"classification\") |&gt; \n  set_engine(\"klaR\") |&gt;\n  set_args(usekernel = FALSE)\n\n\nevaluate_model &lt;- function(model, recipe_list, split){\n  \n  lapply(recipe_list, function(recipe, split){\n    workflow() |&gt;\n      add_recipe(recipe) |&gt;\n      add_model(model) |&gt;\n      last_fit(split = split) |&gt;\n      collect_metrics() |&gt;\n      as.data.table() }, \n  split = split) |&gt;\n  rbind_list_name(new_col_name = \"recipe\")\n  \n}"
  },
  {
    "objectID": "04-execises.html#conceptual",
    "href": "04-execises.html#conceptual",
    "title": "04 - Classification",
    "section": "Conceptual",
    "text": "Conceptual\n1\n1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\\[\n\\begin{split}\np(X) &= \\frac{e^{\\beta_{0}+\\beta_{1}X}}\n            {1+e^{\\beta_{0}+\\beta_{1}X}} \\\\\np(X) (1+e^{\\beta_{0}+\\beta_{1}X}) & = e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X)+p(X) e^{\\beta_{0}+\\beta_{1}X} & = e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X) & = e^{\\beta_{0}+\\beta_{1}X} - p(X) e^{\\beta_{0}+\\beta_{1}X} \\\\\np(X) & = (1 - p(X))e^{\\beta_{0}+\\beta_{1}X} \\\\\n\\frac{p(X)}{1 - p(X)} & = e^{\\beta_{0}+\\beta_{1}X}\n\\end{split}\n\\]\n2\n2. It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a \\(N(\\mu_k,\\sigma^2)\\) distribution, the Bayes classiﬁer assigns an observation to the class for which the discriminant function is maximized.\n\nTo prove that both functions would provide the same class we need to check that both functions change from one \\(Y\\) class to other in the same \\(x\\) value.\nIf \\(K = 2\\) and \\(\\pi_1 = \\pi_2 = \\pi_c\\) we can show that:\n\n\\[\n\\begin{split}\np_{1}(x) & = p_{2}(x) \\\\\n\\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{l})^2}}\n& =\n\\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{l})^2}} \\\\\ne^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}\n& =\ne^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2} \\\\\n\\frac{e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{1})^2}}{e^{-\\frac{1}{2\\sigma^2} (x - \\mu_{2})^2}}\n& = 1 \\\\\n(x - \\mu_{2})^2 - (x - \\mu_{1})^2\n& = 0 \\\\\nx^2 - 2x\\mu_{2} + \\mu_{2}^2 - (x^2 - 2x\\mu_{1} + \\mu_{1}^2)\n& = 0 \\\\\n2x (\\mu_{1}- \\mu_{2}) & = \\mu_{1}^2 - \\mu_{2}^2  \\\\\nx & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2 (\\mu_{1}- \\mu_{2})} \\\\\nx & = \\frac{\\mu_1 + \\mu_2}{2}\n\\end{split}\n\\]\n\nAnd also:\n\n\\[\n\\begin{split}\n\\delta_{1}(x) & = \\delta_{2}(x) \\\\\n\\log{(\\pi_{c})}\n- \\frac{\\mu_{1}^2}{2\\sigma^2}\n+ x \\cdot \\frac{\\mu_{1}}{\\sigma^2}\n& =\n\\log{(\\pi_{c})}\n- \\frac{\\mu_{2}^2}{2\\sigma^2}\n+ x \\cdot \\frac{\\mu_{2}}{\\sigma^2} \\\\\nx (\\mu_{1} - \\mu_{2}) & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2} \\\\\nx & = \\frac{\\mu_{1}^2 - \\mu_{2}^2}{2(\\mu_{1} - \\mu_{2})} \\\\\nx & = \\frac{\\mu_1 + \\mu_2}{2}\n\\end{split}\n\\]\n\nLet’s see an example visually by setting as example the next values for each \\(Y\\) class:\n\n\n\n\\(k\\)\n\\(\\sigma\\)\n\\(\\pi\\)\n\\(\\mu\\)\n\n\n\n1\n0.5\n0.5\n2\n\n\n2\n0.5\n0.5\n4\n\n\n\n\nldm_k_prop &lt;- function(x,\n                       k,\n                       sigma = c(0.5,0.5),\n                       pi_k = c(0.5,0.5),\n                       mu = c(2, 4),\n                       logit = FALSE){\n  \n  if(logit){\n    \n    return(x * mu[k]/sigma[k]^2 - mu[k]^2/(2*sigma[k]^2) + log(pi_k[k]))\n    \n  }\n  \n  denominator &lt;-\n    sapply(x, \\(y) sum(pi_k * (1/(sqrt(2*pi)*sigma)) * exp(-1/(2*sigma^2) * (y - mu)^2) ) )\n  \n  k_numerador &lt;-\n   (pi_k[k]* (sqrt(2*pi)*sigma[k])^-1 * exp(- (2*sigma[k]^2)^-1 * (x - mu[k])^2))\n  \n  return(k_numerador / denominator)\n  \n}\n\n\nBasePlot &lt;-\n  data.frame(x = 1:5) |&gt;\n  ggplot(aes(x))+\n  scale_x_continuous(breaks = scales::breaks_width(1))+\n  theme_light()+\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n  \np1 &lt;-\n  BasePlot +\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 1), color = \"blue\")+\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 2), color = \"red\")\n\np2 &lt;-\n  BasePlot +\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 1, logit = TRUE), color = \"blue\")+\n  geom_function(fun = \\(y) ldm_k_prop(x = y, k = 2, logit = TRUE), color = \"red\")\n\n\n(p1 / p2) +\n  plot_annotation(title = 'Comparing Proportion Function vs Logit Function of LDA',\n                  theme = theme(plot.title = element_text(face = \"bold\")) )\n\n\n\n\n3\n3. This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where \\(p = 1\\); i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, \\(X ∼ N(\\mu_k, \\sigma^2_k)\\). Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\n\\[\n\\begin{split}\np_k(x) & =\n\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi} \\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K}\n      \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n& =\n\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi} \\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\frac{1}{\\sqrt{2\\pi}}\n     \\sum_{l=1}^{K}\\pi_l \\frac{1}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n& =\n\\frac{\\frac{\\pi_k}{\\sigma_k} e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{1})^2}}\n     {\\sum_{l=1}^{K} \\frac{\\pi_l}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}} \\\\\n\\end{split}\n\\]\n\nAs the denominator is a constant for any \\(k\\), we can define: \\(g(x) = \\frac{1}{\\sum_{l=1}^{K} \\frac{\\pi_l}{\\sigma_l} e^{-\\frac{1}{2\\sigma_l^2} (x - \\mu_{l})^2}}\\)\n\n\n\\[\n\\begin{split}\np_k(x) & = g(x) \\frac{\\pi_k}{\\sigma_k}\n           e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2}\\\\\n\\log{(p_k(x))}& =\n\\log{\\left( g(x) \\frac{\\pi_k}{\\sigma_k}\n            e^{-\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2} \\right)}\\\\\n& =\n\\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} -\\frac{1}{2\\sigma_k^2} (x - \\mu_{k})^2 \\\\\n& =\n\\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} -\\frac{\\mu_k^2 - 2\\mu_kx + x^2}{2\\sigma_k^2} \\\\\n& =\n\\left( \\log{(g(x))} + \\log{(\\pi_k)} - \\log{(\\sigma_k)} - \\frac{\\mu_k^2}{2\\sigma_k^2} \\right)\n+ \\frac{\\mu_k}{\\sigma_k^2} \\cdot x - \\frac{1}{2\\sigma_k^2} \\cdot x^2 \\\\\n\\end{split}\n\\]\n4\n4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.\nA\n(A) Suppose that we have a set of observations, each with measurements on \\(p = 1\\) feature, X. We assume that X is uniformly (evenly) distributed on \\([0, 1]\\). Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of the range of \\(X\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X = 0.6\\), we will use observations in the range \\([0.55, 0.65]\\). On average, what fraction of the available observations will we use to make the prediction?\n\nset.seed(123)\n\nUnifDistVar1 &lt;-\n  data.table(sample = 1:1000\n  )[, .(x1 = runif(1000)),\n    by = \"sample\"\n  ][, .(prop = mean(x1 %between% c(0.6 - 0.1/2, 0.6 + 0.1/2))),\n    by = \"sample\"]\n  \nmean(UnifDistVar1$prop)\n\n[1] 0.100297\n\n\nB\n(B) Now suppose that we have a set of observations, each with measurements on \\(p = 2\\) features, \\(X_1\\) and \\(X_2\\). We assume that \\((X_1, X_2)\\) are uniformly distributed on \\([0, 1] \\times [0, 1]\\). We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of \\(X_2\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X_1 = 0.6\\) and \\(X_2 = 0.35\\), we will use observations in the range \\([0.55, 0.65]\\) for \\(X_1\\) and in the range \\([0.3, 0.4]\\) for \\(X_2\\). On average, what fraction of the available observations will we use to make the prediction?\n\nset.seed(123)\n\nUnifDistVar2 &lt;-\n  UnifDistVar1[, .(x1 = runif(1000),\n                   x2 = runif(1000)),\n               by = \"sample\"\n  ][ , .(prop = mean(x1 %between% c(0.6 - 0.1/2, 0.6 + 0.1/2) &\n                     x2 %between% c(0.35 - 0.1/2, 0.35 + 0.1/2))),\n     by = \"sample\"]\n\nmean(UnifDistVar2$prop)\n\n[1] 0.009943\n\n\nC\n(C) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n\n0.1^100\n\n[1] 1e-100\n\n\nD\n(D) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation.\n\nAs p gets lager every point has more specifications to meet and the number of point which meet them decrease.\nE (missing)\n(E) Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For \\(p = 1\\), \\(2\\), and \\(100\\), what is the length of each side of the hypercube? Comment on your answer.\nNote: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When \\(p = 1\\), a hypercube is simply a line segment, when \\(p = 2\\) it is a square, and when \\(p = 100\\) it is a 100-dimensional cube.\n5\n5. We now examine the differences between LDA and QDA.\nA\n(A) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nType of set to test\nPerform better\n\n\n\ntraining set\n\nQDA, as it will model some noise\n\n\ntest set\n\nLDA, as decision boundary is lineal\n\n\nB\n(B) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nType of set to test\nPerform better\n\n\n\ntraining set\n\nQDA, as it will model some noise\n\n\ntest set\n\nQDA, as decision boundary is non-linear\n\n\nC\n(C) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n\nIf the Bayes decision boundary is non-linear, QDA could improve its test prediction accuracy as the sample size n increases. When the model has more examples it has less changes to overfit the data.\nD\n(D) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n\n\nFALSE, as QDA would model some noise that could affect the prediction accuracy.\n6\n6. Suppose we collect data for a group of students in a statistics class with variables \\(X_1\\) = hours studied, \\(X_2\\) = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0 = −6\\), \\(\\hat{\\beta}_1 = 0.05\\) and \\(\\hat{\\beta}_2 = 1\\).\nA\n(A) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.\n\nLogisticCoef &lt;- -6 + 0.05 * 40 + 1* 3.5\n\nexp(LogisticCoef)/(1+exp(LogisticCoef))\n\n[1] 0.3775407\n\n\nB\n(B) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\n\\[\n\\begin{split}\n\\log{ \\left( \\frac{p(X)}{1 - p(X)} \\right)} & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\n\\log{ \\left( \\frac{0.5}{1 - 0.5} \\right)} & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\n0 & = \\beta_{0}+\\beta_{1}X_1+\\beta_{2}X_2 \\\\\nX_2 & = \\frac{-\\beta_0-\\beta_{2}X_2}{\\beta_{1}} \\\\\nX_2 & = \\frac{-(-6)-(1 \\times 3.5)}{0.05} \\\\\nX_2 & = 50 \\text{ horas}\n\\end{split}\n\\]\n7\n7. Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was \\(\\overline{X}_y = 10\\), while the mean for those that didn’t was \\(\\overline{X}_n = 0\\). In addition, the variance of X for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(x = 4\\) last year.\n\\[\n\\begin{split}\np_{k}(x) & = \\frac{\\pi_{k} f_{k}(x)}\n                  {\\sum_{l=1}^{K} \\pi_{l} f_{l}(x)} \\\\\np_{y}(4) & =\n\\frac{0.8 \\times\n      \\left(\n      \\frac{e^{\\frac{-(4-10)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n      \\right)}\n{0.2 \\times\n\\left(\n\\frac{e^{\\frac{-(4-0)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n\\right)\n+\n0.8 \\times\n\\left(\n\\frac{e^{\\frac{-(4-10)^2}{2 \\times 36} }}{\\sqrt{2 \\times \\pi \\times 36}}\n\\right)\n} \\\\\np_{k}(4) & = 0.75\n\\end{split}\n\\]\n\nldm_k_prop(4,\n           k = 1,\n           sigma = c(6, 6),\n           pi_k = c(0.8, 0.2),\n           mu = c(10, 0)) |&gt;\n  scales::percent(accuracy = 0.01)\n\n[1] \"75.19%\"\n\n\n8\n8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures.\n\n\nModel\nTraining Error Rate\nTest Error Rate\n\n\n\nLogistic regression\n20%\n30%\n\n\n1-nearestbors\n0%\n36%\n\n\n\nBased on these results, which method should we prefer to use for classification of new observations? Why?\n\nWe should use the Logistic Regression as it has a lower test error rate than the logistic regression.\n9\n9. This problem has to do with odds.\nA\n(A) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\\[\n\\begin{split}\n\\frac{p(X)}{1 - p(X)} & = Y \\\\\np(X) & = Y - Yp(X) \\\\\np(X) + Yp(X) & = Y \\\\\np(X) & = \\frac{Y}{1+Y} = \\frac{0.37}{1.37} = 0.27\n\\end{split}\n\\]\nB\n(B) Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?\n\nscales::percent(0.16/(1-0.16), accuracy = 0.01) \n\n[1] \"19.05%\"\n\n\n10 (missing)\n10. Equation 4.32 derived an expression for \\(\\log{ \\left( \\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=K|X=x)} \\right)}\\) in the setting where \\(p &gt; 1\\), so that the mean for the \\(k\\)th class, \\(\\mu_k\\), is p-dimensional vector, and the shared covariance \\(\\Sigma\\) is a \\(p \\times p\\) matrix. However, in the setting with \\(p=1\\), (4.32) takes a simpler form, since the means \\(\\mu_1,\\dots,\\mu_K\\) and the variance \\(\\sigma^2\\) are scalars In this simpler setting, repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(\\pi_k\\), \\(\\pi_k\\), \\(\\mu_k\\), \\(\\mu_K\\), and \\(\\sigma^2\\).\n\\[\n\\begin{split}\n\\log{ \\left( \\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=K|X=x)} \\right)}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                & =\n\\end{split}\n\\]\n11 (missing)\n11. Work out the detailed forms of \\(a_k\\), \\(b_{kj}\\) , and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(\\pi_k\\), \\(\\pi_k\\), \\(\\mu_k\\), \\(\\mu_K\\), \\(\\Sigma_k\\), and \\(\\Sigma_K\\).\n12\n12. Suppose that you wish to classify an observation X ∈ R into apples and oranges. You fit a logistic regression model and find that\n\\[\n\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x) =\n\\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1x)}\n{1 + \\exp(\\hat{\\beta}_0+\\hat{\\beta}_1x)}\n\\]\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\\[\n\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x) =\n\\frac{\\exp(\\hat{\\alpha}_{\\text{orange}0}+\n           \\hat{\\alpha}_{\\text{orange}1}x)}\n{\\exp(\\hat{\\alpha}_{\\text{orange}0}+\n      \\hat{\\alpha}_{\\text{orange}1}x)+\n\\exp(\\hat{\\alpha}_{\\text{apple}0}+\n      \\hat{\\alpha}_{\\text{apple}1}x)}\n\\]\nA\n(A) What is the log odds of orange versus apple in your model?\n\\[\n\\log{\n  \\left(\n    \\frac{\\widehat{\\text{Pr}}(Y = \\text{orange}|X=x)}\n         {1 - \\widehat{\\text{Pr}}(Y = \\text{orange}|X=x)}\n  \\right)} =\n\\hat{\\beta}_0+\\hat{\\beta}_1x\n\\]\nB\n(B) What is the log odds of orange versus apple in your friend’s model?\n\\[\n\\log{\n  \\left(\n    \\frac{\\exp(\\hat{\\alpha}_{\\text{orange}0} + \\hat{\\alpha}_{\\text{orange}1}x)}\n         {\\exp(\\hat{\\alpha}_{\\text{apple}0} + \\hat{\\alpha}_{\\text{apple}1}x)}\n  \\right)} =\n(\\hat{\\alpha}_{\\text{orange}0} - \\hat{\\alpha}_{\\text{apple}0}) +\n(\\hat{\\alpha}_{\\text{orange}1} - \\hat{\\alpha}_{\\text{apple}1}) x\n\\]\nC\n(C) Suppose that in your model, \\(\\hat{\\beta}_0 = 2\\) and \\(\\hat{\\beta}_1 = -1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\n\n\\(\\hat{\\beta}_0 = 2\\) and \\(\\hat{\\beta}_1 = -1\\)\n\nD\n(D) Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat{\\alpha}_{\\text{orange}0} = 1.2\\), \\(\\hat{\\alpha}_{\\text{orange}1} = −2\\), \\(\\hat{\\alpha}_{\\text{apple}0} = 3\\), \\(\\hat{\\alpha}_{\\text{apple}1} = 0.6\\). What are the coefficient estimates in your model?\n\n\n\\(\\hat{\\beta}_0 = 1.8\\) and \\(\\hat{\\beta}_1 = -2.6\\)\n\nE\n(E) Finally, suppose you apply both models from (D) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\n\nBoth models will predict the same class for every test observation."
  },
  {
    "objectID": "04-execises.html#applied",
    "href": "04-execises.html#applied",
    "title": "04 - Classification",
    "section": "Applied",
    "text": "Applied\n13\n13. This question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\nA\n(A) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n\nWeeklyDT &lt;- \n  as.data.table(ISLR2::Weekly\n  # We want to measure the prob of going Up rather than Down\n  )[, Direction := factor(Direction, levels = c(\"Up\",\"Down\"))\n  ][,`:=`(n_weeks = .N,\n          year_week = seq_len(.N)),\n    by = \"Year\"\n  # As 1990 has less week that the rest of year we assume that\n  # we are missing some weeks the beginning of the year rather than \n  # the end if the year\n  ][n_weeks &lt; 52L, \n    year_week := year_week + (52L - n_weeks)]\n\n\nggplot(WeeklyDT,aes(Year, Volume, color = Direction))+\n  geom_point(alpha = 0.3)+\n  geom_smooth(se = FALSE)+\n  scale_y_log10()+\n  labs(title = \"Year and Volume are highly correlated\",\n       subtitle = \"So we can not use both in the same model\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\nggplot(WeeklyDT,aes(year_week, Volume, color = Direction))+\n  geom_point(alpha = 0.3)+\n  geom_smooth(se = FALSE)+\n  scale_y_log10()+\n  scale_x_continuous(breaks = scales::breaks_width(10))+\n  labs(title = \"The volume was higher for Down Direction\",\n       subtitle = \"In the first 35 weeks\",\n       y = \"Log10(Volume)\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\nmelt(WeeklyDT,\n     measure.vars = patterns(\"^Lag\\\\d$\"),\n     variable.name = \"Lag_name\",\n     value.name = \"Lag_value\") |&gt;\n  ggplot(aes(Volume, Lag_value, color = Direction))+\n  geom_point(alpha = 0.3)+\n  geom_smooth(se = FALSE)+\n  scale_x_log10()+\n  facet_wrap(vars(Lag_name), scales = \"free_y\")+\n  labs(title = \"There the lags can help to differenciate between Up and Down\",\n       x = \"Log10(Volume)\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\n\nB\n(B) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\nLogisticLagModelFit &lt;-\n  logistic_reg() |&gt;\n  fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = WeeklyDT)\n\nLogisticLagTrainPred &lt;-\n  cbind(WeeklyDT, augment(LogisticLagModelFit, new_data = NULL))\n\nsummary(LogisticLagModelFit$fit)\n\n\nCall:\nstats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + \n    Lag5 + Volume, family = stats::binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4579  -1.0849  -0.9913   1.2565   1.6949  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -0.26686    0.08593  -3.106   0.0019 **\nLag1         0.04127    0.02641   1.563   0.1181   \nLag2        -0.05844    0.02686  -2.175   0.0296 * \nLag3         0.01606    0.02666   0.602   0.5469   \nLag4         0.02779    0.02646   1.050   0.2937   \nLag5         0.01447    0.02638   0.549   0.5833   \nVolume       0.02274    0.03690   0.616   0.5377   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1496.2  on 1088  degrees of freedom\nResidual deviance: 1486.4  on 1082  degrees of freedom\nAIC: 1500.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nLag2 it’s significant.\nC\n(C) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n\nconf_mat(LogisticLagTrainPred,\n         truth = \"Direction\", \n         estimate = .pred_class)\n\n          Truth\nPrediction  Up Down\n      Up   557  430\n      Down  48   54\n\nget_class_metrics(LogisticLagTrainPred,\n                  truth = Direction,\n                  estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.921\n2 spec    binary         0.112\n\n\n\nBy checking the specificity rate we can see that the model is making a bad job identifying when the number is going to get Down.\nD\n(D) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n\nWeeklyYearSplit &lt;-\n  WeeklyDT[, list(analysis = .I[Year &lt;= 2008],\n                  assessment = .I[Year &gt; 2008]) |&gt;\n             make_splits(data = .SD)]\n\nWeeklyYearTraining &lt;- training(WeeklyYearSplit)\n\nWeeklyYearLogisticResults &lt;-\n  logistic_reg() |&gt;\n  get_class_matrix_metrics(split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearLogisticResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   56   34\n      Down  5    9\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.918\n2 spec    binary         0.209\n\n\nE\n(E) Repeat (d) using LDA.\n\nWeeklyYearLdaResults &lt;-\n  discrim_linear() |&gt;\n  get_class_matrix_metrics(split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearLdaResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   56   34\n      Down  5    9\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.918\n2 spec    binary         0.209\n\n\nF\n(F) Repeat (d) using QDA.\n\nWeeklyYearQdaResults &lt;-\n  discrim_quad() |&gt;\n  get_class_matrix_metrics(split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearQdaResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   61   43\n      Down  0    0\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary             1\n2 spec    binary             0\n\n\nG\n(G) Repeat (d) using KNN with K = 1.\n\nWeeklyYearKnnResults &lt;-\n  get_class_matrix_metrics(model_knn_k(1),\n                           split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearKnnResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   30   21\n      Down 31   22\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.492\n2 spec    binary         0.512\n\n\nH\n(H) Repeat (d) using naive Bayes.\n\nWeeklyYearNbResults &lt;-\n  get_class_matrix_metrics(ModelNaiveBayes,\n                           split = WeeklyYearSplit,\n                           fit_formula = \"Direction ~ Lag2\")\n\nWeeklyYearNbResults\n\n$conf_mat\n          Truth\nPrediction Up Down\n      Up   61   43\n      Down  0    0\n\n$metrics\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary             1\n2 spec    binary             0\n\n\nI\n(I) Which of these methods appears to provide the best results on this data?\n\nlist(cbind(WeeklyYearLogisticResults$metrics, model = \"logistic\"),\n     cbind(WeeklyYearLdaResults$metrics, model = \"lda\"),\n     cbind(WeeklyYearQdaResults$metrics, model = \"qda\"),\n     cbind(WeeklyYearNbResults$metrics, model = \"nb\")) |&gt;\n  rbindlist() |&gt;\n  (\\(DT) DT[.metric == \"spec\"][order(-.estimate)])()\n\n   .metric .estimator .estimate    model\n    &lt;char&gt;     &lt;char&gt;     &lt;num&gt;   &lt;char&gt;\n1:    spec     binary 0.2093023 logistic\n2:    spec     binary 0.2093023      lda\n3:    spec     binary 0.0000000      qda\n4:    spec     binary 0.0000000       nb\n\n\n\nThe best models are Logistic Regression and the Linear Discriminant Analysis.\nJ\n(J) Experiment with diﬀerent combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classiﬁer.\n\nLet’s define different recipes to evaluate\n\n\nWeeklyRecipeBaseFormula &lt;-\n  recipe(Direction ~  Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume + year_week,\n         data = WeeklyYearTraining)\n\nWeeklyRecipeLogVolume &lt;-\n  WeeklyRecipeBaseFormula |&gt;\n  step_log(Volume, base = 10)\n\nWeeklyRecipeInteractions &lt;-\n  WeeklyRecipeLogVolume |&gt;\n  step_interact(~ Volume:year_week + Volume:starts_with(\"Lag\")) \n\n\nWeeklyRecipeList &lt;-\n  list(\"BaseFormula\" = WeeklyRecipeBaseFormula,\n       \"LogVolume\" = WeeklyRecipeLogVolume,\n       \"Interactions\" = WeeklyRecipeInteractions)\n\n\nThe models to fit\n\n\nWeeklyModelList &lt;-\n  c(1L, seq.int(10L, 100L, 10L)) |&gt;\n  (\\(x) structure(x, names = paste(\"knn\",x)) )() |&gt;\n  lapply(model_knn_k) |&gt;\n  append(list(\"logistic\" = logistic_reg(),\n              \"lda\" = discrim_linear(),\n              \"qda\" = discrim_quad(),\n              bayes = ModelNaiveBayes)) \n\n\nPerforming the evaluation\n\n\nWeeklyEvaluationResults &lt;-\n  lapply(WeeklyModelList,\n         FUN = evaluate_model,\n         recipe_list = WeeklyRecipeList,\n         split = WeeklyYearSplit) |&gt;\n  rbind_list_name(new_col_name = \"model\", fill = TRUE)\n  \n \nWeeklyEvaluationResults[, dcast(.SD, recipe+model ~.metric,\n                                value.var = \".estimate\")\n][order(-roc_auc)\n][1:5]\n\n\n\n         recipe    model  accuracy   roc_auc\n         &lt;char&gt;   &lt;char&gt;     &lt;num&gt;     &lt;num&gt;\n1: Interactions logistic 0.5961538 0.6164697\n2: Interactions      lda 0.6057692 0.6157072\n3:    LogVolume   knn 30 0.5865385 0.6023637\n4:    LogVolume   knn 40 0.5769231 0.5981700\n5:    LogVolume   knn 50 0.5769231 0.5932139\n\n\n\nIn this case the Logistic Regressions has the best general results after adding interactions to the model.\n\n\nWeeklyFittedWorkFlow &lt;-\n  workflow() |&gt;\n  add_recipe(WeeklyRecipeInteractions) |&gt;\n  add_model(logistic_reg()) |&gt;\n  fit(data = WeeklyYearTraining) \n\nWeeklyFittedWorkFlow |&gt;\n  augment(new_data = testing(WeeklyYearSplit)) |&gt;\n  get_class_metrics(truth = Direction,\n                    estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.639\n2 spec    binary         0.535\n\n\n\nThe model can predict if the Direction would go “Up” 64 of 100 times correctly and predict “Down” correctly 54 of 100 times, that is little bit better than guessing.\n\n\nWeeklyFittedWorkFlow$fit$fit$fit |&gt;\n  tidy() |&gt;\n  as.data.table() |&gt;\n  (\\(DT) DT[order(-abs(estimate))])() |&gt;\n  head(5) |&gt;\n  ggplot(aes(estimate, reorder(term,estimate)))+\n  geom_col()+\n  theme_classic()\n\n\n\n\n14\n14. In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\nA\n(A) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may ﬁnd it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\n\nAutoDT &lt;- \n  as.data.table(ISLR2::Auto\n  )[,`:=`(mpg01 = factor(mpg &gt; median(mpg), levels = c(\"TRUE\",\"FALSE\")),\n          origin = c(\"American\",\"European\",\"Japanese\")[origin],\n          cylinders = factor(cylinders))]\n\nB\n(B) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your ﬁndings.\n\nExploring categorical variables\n\n\nAutoDT[, .(prob = mean(mpg01 == \"TRUE\")),  \n       by = \"cylinders\"] |&gt;\n  ggplot(aes(cylinders, prob)) +\n  geom_col(aes(fill = prob &gt; 0.6), alpha = 0.8) +\n  labs(title = \"Cars with 4 and 5 cyclinders are more efficient\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nAutoDT[, .(prob = mean(mpg01 == \"TRUE\")),\n       by = \"origin\"] |&gt;\n  ggplot(aes(origin, prob)) +\n  geom_col(aes(fill = prob &lt; 0.4), alpha = 0.8) +\n  labs(title = \"American Cars are less efficient\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"firebrick3\", \"FALSE\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nExploring numerical variables\n\n\nAutoDT[, .(prob = mean(mpg01 == \"TRUE\")),\n       by = \"year\"] |&gt;\n  ggplot(aes(year, prob)) +\n  geom_point(color = \"#AAAAAA\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"dodgerblue3\")+\n  labs(title = \"New cars are more efficient on average\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"firebrick3\", \"FALSE\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(mpg01, displacement))+\n  geom_boxplot(aes(fill = mpg01), alpha = 0.6)+\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The Engine displacement is lower for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(mpg01, horsepower))+\n  geom_boxplot(aes(fill = mpg01), alpha = 0.6)+\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The Engine horsepower is lower for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(mpg01, weight))+\n  geom_boxplot(aes(fill = mpg01), alpha = 0.6)+\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The weight is lower for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(AutoDT, aes(acceleration, fill = factor(mpg01,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, position = \"identity\", bins = 15)+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The acceleration is a little bit higher for efficient cars\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\nC\n(C) Split the data into a training set and a test set.\n\nset.seed(1224)\n\nAutoSplit &lt;- initial_split(AutoDT, strata = mpg01)\n\nAutoTraining &lt;- training(AutoSplit)\n\nD\n(D) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nAutoRecipe &lt;- \n  recipe(mpg01 ~ cylinders + origin + year + displacement + horsepower + weight + acceleration,\n         data = AutoTraining) |&gt;\n  step_dummy(cylinders, origin) |&gt;\n  # I couldn't perform QDA with a higher threshold\n  step_corr(all_numeric_predictors(), threshold = 0.59)\n\nworkflow() |&gt;\n  add_recipe(AutoRecipe) |&gt;\n  add_model(discrim_linear()) |&gt;\n  last_fit(split = AutoSplit) |&gt;\n  collect_predictions() |&gt;\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1122449\n\n\nE\n(E) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nworkflow() |&gt;\n  add_recipe(AutoRecipe) |&gt;\n  add_model(discrim_quad()) |&gt;\n  last_fit(split = AutoSplit) |&gt;\n  collect_predictions() |&gt;\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1428571\n\n\nF\n(F) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nworkflow() |&gt;\n  add_recipe(AutoRecipe) |&gt;\n  add_model(logistic_reg()) |&gt;\n  last_fit(split = AutoSplit) |&gt;\n  collect_predictions() |&gt;\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1122449\n\n\nG\n(G) Perform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n\nworkflow() |&gt;\n  add_recipe(AutoRecipe) |&gt;\n  add_model(ModelNaiveBayes) |&gt;\n  last_fit(split = AutoSplit) |&gt;\n  collect_predictions() |&gt;\n  (\\(DF) mean(DF$.pred_class != DF$mpg01) )()\n\n[1] 0.1020408\n\n\nH\n(H) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?\n\nAutoKnnTestError &lt;-\n  lapply(1:15, function(k){\n    \n    workflow() |&gt;\n      add_recipe(AutoRecipe) |&gt;\n      add_model(model_knn_k(k)) |&gt;\n      last_fit(split = AutoSplit) |&gt;\n      collect_predictions() |&gt;\n      summarise(k = k,\n                test_error = mean(.pred_class != mpg01))\n    \n  }) |&gt;\n  rbindlist() |&gt;\n  arrange(test_error)\n\nggplot(AutoKnnTestError, aes(k, test_error))+\n  geom_line(color = \"#AAAAAA\")+\n  geom_point(aes(color = test_error == min(test_error)))+\n  expand_limits(y = 0)+\n  scale_color_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The lowest error starts with k = 12 and keep constant\")+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\n\n\nhead(AutoKnnTestError, 1L)\n\n       k test_error\n   &lt;int&gt;      &lt;num&gt;\n1:    12  0.1020408\n\n\n15\n15. This problem involves writing functions.\nA\n(A) Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute \\(2^3\\) and print out the results.\n\nPower &lt;- \\() 2^3\n\nPower()\n\n[1] 8\n\n\nB\n(B) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a.\n\nPower2&lt;-function(x, a) { print(x^a)}\n\nPower2(3,8)\n\n[1] 6561\n\n\nC\n(C) Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^{3}\\).\n\nPower2(c(10,8,131),c(3, 17, 3))\n\n[1] 1.000000e+03 2.251800e+15 2.248091e+06\n\n\nD\n(D) Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line:\n\nPower3 &lt;- function(x, a) {\n  result &lt;- x^a\n  return(result)\n}\n\nE\n(E) Now using the Power3() function, create a plot of \\(f(x) =x^2\\). The x-axis should display a range of integers from 1 to 10, and the y-axis should display \\(x^2\\). Label the axes appropriately, and use an appropriate title for the ﬁgure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log = \"x\", log = \"y\", or log = \"xy\" as arguments to the plot() function.\n\nggplot(data.frame(x = 1:10), aes(x))+\n  geom_function(fun  = \\(x) Power3(x,2), color= \"blue\")+\n  scale_y_log10()+\n  scale_x_continuous(breaks = breaks_width(1))+\n  labs(title = \"Plotting a function with ggplot2\", y = \"log10(x^2)\")+\n  theme_classic()\n\n\n\n\nF\n(F) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a ﬁxed a and for a range of values of x. For instance, if you call\n\nPlotPower(1:10,3)\n\nthen a plot should be created with an x-axis taking on values 1, 2,…,10, and a y-axis taking on values \\(1^3, 2^3, \\dots, 10^3\\).\n\nPlotPower &lt;- function(x, a){\n  \n  ggplot(data.frame(x_values = x), aes(x_values))+\n  geom_function(fun  = \\(x) Power3(x, a), color= \"blue\")+\n  scale_y_log10()+\n  labs(title = \"Plotting a function with ggplot2\", \n       y =paste0(\"log10(x^\",a,\")\") )+\n  theme_classic()\n  \n}\n\nPlotPower(1:100,2)\n\n\n\n\n16\n16. Using the Boston data set, ﬁt classiﬁcation models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes, and KNN models using various subsets of the predictors. Describe your ﬁndings.\nHint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set.\n\nLet’s transform the data.\n\n\nBostonDT &lt;- \n  as.data.table(Boston\n  )[,`:=`(high_crim = factor(crim &gt; median(crim),levels = c(\"TRUE\",\"FALSE\")),\n          rad = factor(rad),\n          crim = NULL)]\n\nset.seed(2014)\n\nBostonSplit &lt;- initial_split(BostonDT, strata = high_crim)\n\nBostonTraining &lt;- training(BostonSplit)\n\nBostonRecipeAllVars &lt;-\n  recipe(high_crim ~ . , data = BostonTraining) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_corr(all_numeric_predictors(), threshold = 0.8)\n\nBostonTrainingPrep &lt;-\n  prep(BostonRecipeAllVars, training = BostonTraining) |&gt;\n  bake(new_data = NULL) |&gt;\n  as.data.table()\n\n\nVariable with differences\n\n\nggplot(BostonTrainingPrep, aes(zn, fill =  factor(high_crim,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, bins = 8, position = \"identity\")+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The crime is higher in zones with less residential land\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim, indus))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher in zones with\\na greater proportion of non-retail stores\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim, nox))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher in zones with higher concentration of nitrogen oxides\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  age))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher in old dwelling\",\n       y = \"Proportion of dwelling built prior to 1940\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  dis))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher as get closer to the employment centres\",\n       y = \"Proportion of dwelling built prior to 1940\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nBostonTraining[, .(prob = mean(high_crim == \"TRUE\"),\n                   x = sum(high_crim == \"TRUE\"),\n                   n = .N),\n               by = \"rad\"\n  # Using Jeffeys CI base on Beta distribution\n  ][, `:=`(lower = qbeta(0.025, 0.5+x, 0.5 +n-x),\n           higher = qbeta(0.975, 0.5+x, 0.5 +n-x),\n           rad = reorder(rad, prob))] |&gt;\n  ggplot(aes(rad, prob, group = \"1\")) +\n  geom_ribbon(aes(ymin = lower, ymax = higher),\n                 fill = \"gray90\")+\n  geom_line(color = \"dodgerblue3\") +\n  geom_point(size = 2, color = \"gray40\")+\n  labs(title = \"The crime has a positive correlation with the index of accessibility\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  lstat))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher as lower status increases\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim,  medv))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime is higher as the dwelling is cheaper\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nVariable without differences\n\n\nBostonTrainingPrep[, .(prob = mean(high_crim == \"TRUE\"),\n                       x = sum(high_crim == \"TRUE\"),\n                       n = .N),\n                   by = .(chas = factor(chas))\n  # Using Jeffeys CI base on Beta distribution\n  ][, `:=`(lower = qbeta(0.025, 0.5+x, 0.5 +n-x),\n           higher = qbeta(0.975, 0.5+x, 0.5 +n-x))] |&gt;\n  ggplot(aes(prob, chas)) +\n  geom_errorbarh(aes(xmin = lower, xmax = higher),\n                 color = \"#AAAAAA\",\n                 height = .3)+\n  geom_point(aes(color = chas), size = 3)+\n  labs(title = \"We don't have enough evidence to confirm a significan\\ndifference between tracts with or without river\") +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  scale_color_manual(values = c(\"1\" = \"dodgerblue3\", \"0\" = \"#AAAAAA\")) +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(high_crim, rm))+\n  geom_violin(fill = NA, color = \"#AAAAAA\") +\n  geom_boxplot(aes(fill = high_crim), alpha = 0.8) +\n  scale_fill_manual(values = c(\"TRUE\" = \"dodgerblue3\", \"FALSE\" = \"#AAAAAA\")) +\n  labs(title = \"The crime level is the same no matter the number of rooms\",\n       y = \"Number of Rooms\",\n       x = \"Crime over the median\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(ptratio, fill =  factor(high_crim,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, bins = 6, position = \"identity\")+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The crime level is the same no matter the pupil-teacher ratio\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\nggplot(BostonTrainingPrep, aes(black, fill =  factor(high_crim,levels = c(\"FALSE\",\"TRUE\"))))+\n  geom_histogram(alpha = 0.5, bins = 12, position = \"identity\")+\n  scale_fill_manual(values = c(\"FALSE\" = \"#AAAAAA\", \"TRUE\" = \"dodgerblue3\"))+\n  labs(title = \"The crime level is the same the proportion of blacks by town\") +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\n\nLet’s define different recipes to evaluate\n\n\nBostonRecipeSignificant &lt;-\n  recipe(high_crim ~ zn + indus + nox + age + dis + rad + lstat + medv,\n         data = BostonTraining) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_corr(all_numeric_predictors(), threshold = 0.8)\n\n\nBostonRecipeList &lt;-\n  list(\"AllVars\" = BostonRecipeAllVars,\n       \"Significant\" = BostonRecipeSignificant)\n\n\nThe models to fit\n\n\nBostonModelList &lt;-\n  c(1L, seq.int(10L, 100L, 10L)) |&gt;\n  (\\(x) structure(x, names = paste(\"knn\",x)) )() |&gt;\n  lapply(model_knn_k) |&gt;\n  # We couldn't fit data with QDA or a Naive Bayes as the variables\n  # has the variable has zero variance in some cases\n  append(list(\"logistic\" = logistic_reg(),\n              \"lda\" = discrim_linear())) \n\n\nPerforming the evaluation\n\n\nBostonEvaluationResults &lt;-\n  lapply(BostonModelList,\n         FUN = evaluate_model,\n         recipe_list = BostonRecipeList,\n         split = BostonSplit) |&gt;\n  rbind_list_name(new_col_name = \"model\", fill = TRUE)\n  \n \nBostonEvaluationResults[, dcast(.SD,recipe+model ~.metric,\n                                value.var = \".estimate\")\n][order(-roc_auc)\n][1:5]\n\n\n\n        recipe    model  accuracy   roc_auc\n        &lt;char&gt;   &lt;char&gt;     &lt;num&gt;     &lt;num&gt;\n1:     AllVars   knn 10 0.9375000 0.9655762\n2:     AllVars logistic 0.9375000 0.9641113\n3: Significant logistic 0.9296875 0.9635010\n4:     AllVars   knn 40 0.9375000 0.9619141\n5: Significant   knn 20 0.9296875 0.9619141\n\n\n\nAs we can see the the Logistic Regression and the KNN can predict really well this variable no matter if we take all the predictor or the ones highlighted during the exploration process. In that case would use the Logistic Regression with the Significant recipe.\n\n\nBostonFittedWorkFlow &lt;-\n  workflow() |&gt;\n  add_recipe(BostonRecipeSignificant) |&gt;\n  add_model(logistic_reg()) |&gt;\n  fit(data = BostonTraining) \n\nBostonFittedWorkFlow$fit$fit$fit |&gt;\n  tidy() |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  ggplot(aes(abs(estimate), reorder(term,abs(estimate))))+\n  geom_col(aes(fill = estimate &gt;= 0), alpha = 0.8)+\n  scale_fill_manual(values = c(\"FALSE\" = \"firebrick3\", \n                               \"TRUE\" = \"dodgerblue3\"))+\n  scale_x_log10(labels = comma_format(accuracy = 0.01))+\n  theme_classic()+\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "05-execises.html#libraries",
    "href": "05-execises.html#libraries",
    "title": "05 - Resampling Methods",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(scales)\nlibrary(tidymodels)\nlibrary(data.table)"
  },
  {
    "objectID": "05-execises.html#conceptual",
    "href": "05-execises.html#conceptual",
    "title": "05 - Resampling Methods",
    "section": "Conceptual",
    "text": "Conceptual\n1\n1. Using basic statistical properties of the variance, as well as single-variable calculus, derive (5.6). In other words, prove that \\(\\alpha\\) given by (5.6) does indeed minimize \\(\\text{Var}(\\alpha X + (1-\\alpha)Y)\\).\nBy taking as a reference the Propagation section of the Variance Wikipedia post.\n\\[\n\\begin{split}\n\\text{Var}(\\alpha X + (1-\\alpha)Y) & =  \n        \\alpha^2 \\text{Var}(X)+\n        (1-\\alpha)^2  \\text{Var}(Y) +\n        2 \\alpha (1-\\alpha)   \\text{Cov}(X,Y) \\\\\n    & = \\alpha^2      \\text{Var}(X)+\n        (1 - 2 \\alpha + \\alpha^2) \\text{Var}(Y) +\n         (2 \\alpha-2\\alpha^2)   \\text{Cov}(X,Y) \\\\\n    & = \\alpha^2      \\text{Var}(X)+\n        \\text{Var}(Y) - 2 \\alpha \\text{Var}(Y) + \\alpha^2 \\text{Var}(Y)+\n         2 \\alpha \\text{Cov}(X,Y) - 2\\alpha^2 \\text{Cov}(X,Y)    \\\\   \n    & = [\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)] \\alpha^2 +\n        2[ \\text{Cov}(X,Y) - \\text{Var}(Y)] \\alpha + \\text{Var}(Y)\n\\end{split}\n\\]\nOnce we have the function, we can derivative using the derivative of power and solve the equation.\n\\[\n\\begin{split}\n2[\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)] \\alpha + 2[ \\text{Cov}(X,Y) - \\text{Var}(Y)] & =  0 \\\\\n2[\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)] \\alpha & = 2[\\text{Var}(Y) - \\text{Cov}(X,Y)] \\\\\n\\alpha  = \\frac{\\text{Var}(Y) - \\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y)}\n\\end{split}\n\\]\n2\n2. We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.\nA\n(A) What is the probability that the ﬁrst bootstrap observation is not the jth observation from the original sample? Justify your answer.\nThe probability of an observation to be in any position of the bootstrap sample is \\(1/n\\) and the opposite \\(1 - 1/n\\).\nB\n(B) What is the probability that the second bootstrap observation is not the jth observation from the original sample?\nThe probability it’s the same (\\(1 - 1/n\\)) as we are sampling with replacement.\nC\n(C) Argue that the probability that the jth observation is not in the bootstrap sample is \\((1 - 1/n)^n\\).\nAs the probability of the jth observation for avoiding each position in bootstrap sample is \\(1 - 1/n\\) to get the probability in that situation we should use \\((1 - 1/n)^n\\) as the probabilities are independent.\nD\n(D) When \\(n = 5\\), what is the probability that the jth observation is in the bootstrap sample?\nAs \\((1 - 1/5)^5\\) represent the probability that an observation won’t appear.\n\\[\n1 - (1 - 1/5)^5 = 0.6723\n\\]\nE\n(E) When \\(n = 100\\), what is the probability that the jth observation is in the bootstrap sample?\n\\[\n1 - (1 - 1/100)^{100} = 0.6340\n\\]\nF\n(F) When \\(n = 10,000\\), what is the probability that the jth observation is in the bootstrap sample?\n\\[\n1 - (1 - 1/10000)^{10000} = 0.6321\n\\]\nG\n(G) Create a plot that displays, for each integer value of n from \\(1\\) to \\(100,000\\), the probability that the jth observation is in the bootstrap sample. Comment on what you observe.\n\nggplot(data.frame(x = 1:1e4))+\n  geom_function(aes(x), fun = \\(x) 1 - (1 - 1/x)^x, \n                color = \"blue\", linewidth = 0.8)+\n  geom_hline(yintercept = 1 - 1/exp(1), linetype = 2,\n             color = as.character(round(1 - 1/exp(1), 4)))+\n  expand_limits(y = 0)+\n  labs(x = \"Number of observations\",\n       y = \"Probability\",\n       title = \"Probability an observation is in the bootstrap sample\")+\n  scale_y_continuous(labels = percent_format(accuracy = 1),\n                     breaks = breaks_width(0.1))+\n  scale_x_continuous(labels = comma_format(accuracy = 1))+\n  theme_light()+\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank(),\n        plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\nH\n(H) We will now investigate numerically the probability that a bootstrap sample of size \\(n = 100\\) contains the jth observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n\nset.seed(2018)\n\nvapply(1:1e4, \n       FUN = \\(x) 4L %in% sample.int(100L, 100L, replace = TRUE),\n       FUN.VALUE = TRUE) |&gt;\n  mean()\n\n[1] 0.6329\n\n\nAs we can see the probability es really close to \\(0.6340\\).\n3\n3. We now review k-fold cross-validation.\nA\n(A) Explain how k-fold cross-validation is implemented.\nIt involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The ﬁrst fold is treated as a validation set, and the method is ﬁt on the remaining \\(k - 1\\) folds.\nB\n(B) What are the advantages and disadvantages of k-fold cross validation relative to the validation set approach and the LOOCV?\n\n\nMain Characteristics\nValidation Set Approach\nLOOCV\n\n\n\nAccuracy in estimating the testing error\nLower\nLower\n\n\nTime efficiency\nHigher\nLower\n\n\nProportion of data used to train the models (bias mitigation)\nLower\nHigher\n\n\nEstimation variance\n-\nHigher\n\n\n4\n4. Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\nI would use to bootstrap method to re-sample the original data set many times, fit a statistical learning method on each re-sample, predict the value based on the predictors we want to study and calculate the standard deviation of the response, which is a good approximation of the standard error as we can on page 210."
  },
  {
    "objectID": "05-execises.html#applied",
    "href": "05-execises.html#applied",
    "title": "05 - Resampling Methods",
    "section": "Applied",
    "text": "Applied\n5\n5. In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n\nDefaultFormulaToFit &lt;- as.formula(\"default ~ balance + income\")\n\nA\n(A) Fit a logistic regression model that uses income and balance to predict default.\n\nDefaultFittedModel &lt;-\n  logistic_reg() |&gt;\n  fit(DefaultFormulaToFit, data = ISLR::Default)\n\nDefaultFittedModel\n\nparsnip model object\n\n\nCall:  stats::glm(formula = default ~ balance + income, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      balance       income  \n -1.154e+01    5.647e-03    2.081e-05  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n\nB\n(B) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n\nSplit the sample set into a training set and a validation set.\n\n\nset.seed(4)\n\nDefaultSplit &lt;- initial_split(ISLR::Default, prop = 0.5, strata = default)\n\nDefaultSplit\n\n&lt;Training/Testing/Total&gt;\n&lt;5000/5000/10000&gt;\n\n\n\nFit a multiple logistic regression model using only the training observations.\n\n\nDefaultTrainingModel &lt;-\n  logistic_reg() |&gt;\n  fit(DefaultFormulaToFit, data = training(DefaultSplit))\n\nDefaultTrainingModel\n\nparsnip model object\n\n\nCall:  stats::glm(formula = default ~ balance + income, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      balance       income  \n -1.153e+01    5.598e-03    2.384e-05  \n\nDegrees of Freedom: 4999 Total (i.e. Null);  4997 Residual\nNull Deviance:      1484 \nResidual Deviance: 813.8    AIC: 819.8\n\n\n\nObtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\n\n\nDefaultTestPredictions &lt;-\n  augment(DefaultTrainingModel, new_data = testing(DefaultSplit))\n\nDefaultTestPredictions\n\n# A tibble: 5,000 × 7\n   default student balance income .pred_class .pred_No .pred_Yes\n   &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 No      No         529. 35704. No             1.00  0.000446 \n 2 No      Yes        920.  7492. No             0.998 0.00202  \n 3 No      Yes        809. 17600. No             0.999 0.00138  \n 4 No      No           0  29275. No             1.00  0.0000198\n 5 No      No         237. 28252. No             1.00  0.0000728\n 6 No      No         607. 44995. No             0.999 0.000859 \n 7 No      No           0  50265. No             1.00  0.0000326\n 8 No      No         486. 61566. No             0.999 0.000649 \n 9 No      No        1095. 26465. No             0.992 0.00844  \n10 No      No         954. 32458. No             0.996 0.00444  \n# ℹ 4,990 more rows\n\n\n\nCompute the validation set error, which is the fraction of the observations in the validation set that are misclassiﬁed.\n\n\nDefaultTestPredictions |&gt;\n  summarize(`Test error rate` = mean(default != .pred_class))\n\n# A tibble: 1 × 1\n  `Test error rate`\n              &lt;dbl&gt;\n1            0.0242\n\n\nC\n(C) Repeat the process in (b) three times, using three diﬀerent splits of the observations into a training set and a validation set. Comment on the results obtained.\n\nDefaultBasedResults &lt;-\n  lapply(8:10, \n         model_recipe = recipe(DefaultFormulaToFit, data = ISLR::Default),\n         \n         FUN = \\(seed, model_recipe){\n           set.seed(seed)\n           \n           split &lt;- initial_split(ISLR::Default, prop = 0.5, strata = default)\n           \n           workflow() |&gt;\n             add_model(logistic_reg()) |&gt;\n             add_recipe(model_recipe) |&gt;\n             last_fit(split = split) |&gt;\n             collect_predictions() |&gt;\n             summarize(seed_used = seed,\n                       `test_error_rate` = mean(.pred_class != default)) }) |&gt;\n  rbindlist()\n\nDefaultBasedResults\n\n   seed_used test_error_rate\n       &lt;int&gt;           &lt;num&gt;\n1:         8          0.0262\n2:         9          0.0240\n3:        10          0.0282\n\n\nD\n(D) Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.\nAdding the student variable as a dummy one doesn’t make a big impact on the prediction accurency.\n\nDefaultDummyRecipe &lt;- \n  recipe(default ~ ., data = ISLR::Default) |&gt;\n  step_dummy(student)\n\n\nDefaultDummyResults &lt;- \n  lapply(8:10, \n         model_recipe = DefaultDummyRecipe,\n         \n         FUN = \\(seed, model_recipe){\n           set.seed(seed)\n           \n           split &lt;- initial_split(ISLR::Default, prop = 0.5, strata = default)\n           \n           workflow() |&gt;\n             add_model(logistic_reg()) |&gt;\n             add_recipe(model_recipe) |&gt;\n             last_fit(split = split) |&gt;\n             collect_predictions() |&gt;\n             summarize(seed_used = seed,\n                       `test_error_rate_dummy` = mean(.pred_class != default)) }) |&gt;\n  rbindlist()\n\n\nDefaultBasedResults[DefaultDummyResults, on = \"seed_used\"\n][, diff := comma(test_error_rate_dummy - test_error_rate, accuracy = 0.0001)][]\n\n   seed_used test_error_rate test_error_rate_dummy    diff\n       &lt;int&gt;           &lt;num&gt;                 &lt;num&gt;  &lt;char&gt;\n1:         8          0.0262                0.0260 -0.0002\n2:         9          0.0240                0.0248  0.0008\n3:        10          0.0282                0.0278 -0.0004\n\n\n6\n6. We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coeﬃcients in two diﬀerent ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\nA\n(A) Using the summary() and glm() functions, determine the estimated standard errors for the coeﬃcients associated with income and balance in a multiple logistic regression model that uses both predictors.\n\nDefaultGlmSummary &lt;- tidy(DefaultFittedModel)\n\nDefaultGlmSummary\n\n# A tibble: 3 × 5\n  term           estimate  std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -11.5       0.435         -26.5  2.96e-155\n2 balance       0.00565   0.000227       24.8  3.64e-136\n3 income        0.0000208 0.00000499      4.17 2.99e-  5\n\n\nB\n(B) Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coeﬃcient estimates for income and balance in the multiple logistic regression model.\nTo create the function it’s optimal\nC\n(C) Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coeﬃcients for income and balance.\n\nset.seed(15)\n\nDefaultBootstrapsSe &lt;- \n  as.data.table(bootstraps(ISLR::Default, times = 500)\n  )[, logistic_reg() |&gt; \n      fit(DefaultFormulaToFit, \n          data = analysis(splits[[1L]])) |&gt;\n      tidy(),\n    by = \"id\"\n  ][, .(SE = sd(estimate)),\n    by = \"term\"]\n\n\nDefaultBootstrapsSe\n\n          term           SE\n        &lt;char&gt;        &lt;num&gt;\n1: (Intercept) 4.230130e-01\n2:     balance 2.203893e-04\n3:      income 4.895823e-06\n\n\nD\n(D) Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\nAs you can see bellow the results are really close to each other.\n\nleft_join(DefaultBootstrapsSe,\n          DefaultGlmSummary[,c(\"term\",\"std.error\")],\n          by = \"term\") |&gt;\n  mutate(diff = SE - std.error)\n\n          term           SE    std.error          diff\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;\n1: (Intercept) 4.230130e-01 4.347564e-01 -1.174338e-02\n2:     balance 2.203893e-04 2.273731e-04 -6.983879e-06\n3:      income 4.895823e-06 4.985167e-06 -8.934457e-08\n\n\n7\n7. In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classiﬁcation problems, the LOOCV error is given in (5.4).\nA\n(A) Fit a logistic regression model that predicts Direction using Lag1 and Lag2.\n\nWeeklyModel &lt;-\n  logistic_reg() |&gt;\n  fit(Direction ~ Lag1 + Lag2, data = ISLR::Weekly)\n\nWeeklyModel\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Direction ~ Lag1 + Lag2, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)         Lag1         Lag2  \n    0.22122     -0.03872      0.06025  \n\nDegrees of Freedom: 1088 Total (i.e. Null);  1086 Residual\nNull Deviance:      1496 \nResidual Deviance: 1488     AIC: 1494\n\n\nB\n(B) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the ﬁrst observation.\n\nWeeklyModelNotFirst &lt;-\n  logistic_reg() |&gt;\n  fit(Direction ~ Lag1 + Lag2, data = ISLR::Weekly[-1L,])\n\nWeeklyModelNotFirst\n\nparsnip model object\n\n\nCall:  stats::glm(formula = Direction ~ Lag1 + Lag2, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)         Lag1         Lag2  \n    0.22324     -0.03843      0.06085  \n\nDegrees of Freedom: 1087 Total (i.e. Null);  1085 Residual\nNull Deviance:      1495 \nResidual Deviance: 1487     AIC: 1493\n\n\nC\n(C) Use the model from (b) to predict the direction of the ﬁrst observation. You can do this by predicting that the ﬁrst observation will go up if P(Direction = \"Up\" | Lag1, Lag2) &gt; 0.5. Was this observation correctly classiﬁed?\nNo, it wasn’t.\n\nWeeklyModelNotFirst |&gt;\n  augment(new_data = ISLR::Weekly[1L,] )\n\n# A tibble: 1 × 12\n   Year  Lag1  Lag2  Lag3   Lag4  Lag5 Volume Today Direction .pred_class\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;      \n1  1990 0.816  1.57 -3.94 -0.229 -3.48  0.155 -0.27 Down      Up         \n# ℹ 2 more variables: .pred_Down &lt;dbl&gt;, .pred_Up &lt;dbl&gt;\n\n\nD\n(D) Write a for loop from \\(i = 1\\) to \\(i = n\\), where n is the number of observations in the data set, that performs each of the following steps:\n\nFit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.\nCompute the posterior probability of the market moving up for the ith observation.\nUse the posterior probability for the ith observation in order to predict whether or not the market moves up.\nDetermine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.\n\n\nWeeklyLoocv &lt;- loo_cv(ISLR::Weekly)\n\nsetDT(WeeklyLoocv)\n\nWeeklyLoocvPredictions &lt;-\n  WeeklyLoocv[, training(splits[[1L]]), by = \"id\"\n  ][, .(model = .(logistic_reg() |&gt;\n                    fit(Direction ~ Lag1 + Lag2, data = .SD))),\n    by = \"id\"\n  ][WeeklyLoocv[, testing(splits[[1L]]), by = \"id\"],\n    on = \"id\"\n  ][, .pred_class := predict(model[[1L]], new_data = .SD, type = \"class\"),\n    by = \"id\"\n  ][, is_error := Direction != .pred_class]\n\nE\n(E) Take the average of the n numbers obtained in (4d) in order to obtain the LOOCV estimate for the test error. Comment on the results.\n\nmean(WeeklyLoocvPredictions$is_error)\n\n[1] 0.4499541\n\n\n8\nA\n(A) Generate a simulated data set as follows. In this data set, what is n and what is p? Write out the model used to generate the data in equation form.\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- x- 2*x^2 + rnorm(100)\n\nSimulatedDt &lt;- data.table(x, y)\n\nn: 100 and p: 1.\nB\n(B) Create a scatterplot of X against Y. Comment on what you ﬁnd.\nThe values follows a function of second degree.\n\nggplot(SimulatedDt, aes(x,  y))+\n  geom_point()+\n  geom_smooth(se = FALSE)+\n  theme_light()\n\n\n\n\nC\n(C) Set a random seed, and then compute the LOOCV errors that result from ﬁtting the following four models using least squares. Note you may ﬁnd it helpful to use the data.frame() function to create a single data set containing both X and Y .\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\epsilon\\)\n\nwe don’t need to set a seed when performing LOOCV, as we don’t do anything at random.\n\ncollect_loo_testing_error &lt;- function(formula,\n                                      loo_split,\n                                      metric_function = rmse,\n                                      ...){\n  # Validations\n  stopifnot(\"There is no espace between y and ~\" = formula %like% \"[A-Za-z]+ \")\n  stopifnot(\"loo_split must be a data.table object\" = is.data.table(loo_split))\n  \n  predictor &lt;- sub(pattern = \" .+\", replacement = \"\", formula)\n  formula_to_fit &lt;- as.formula(formula)\n  \n  Results &lt;-\n    loo_split[, training(splits[[1L]]), by = \"id\"\n    ][, .(model = .(lm(formula_to_fit, data = .SD))),\n      by = \"id\"\n    ][loo_split[, testing(splits[[1L]]), by = \"id\"],\n      on = \"id\"\n    ][, .pred := predict(model[[1L]], newdata = .SD),\n      by = \"id\"\n    ][,  metric_function(.SD, truth = !!predictor, estimate = .pred, ...) ]\n \n  setDT(Results)\n  \n  \n  if(formula %like% \"degree\"){\n    \n    degree &lt;- gsub(pattern = \"[ A-Za-z,=\\\\~()]\", replacement = \"\", formula)\n    \n    Results &lt;- \n      Results[,.(degree = degree, \n                .metric, \n                .estimator, \n                .estimate)]\n    \n  }\n  \n  return(Results)\n    \n}\n\n\n# Creating the rplit object\nSimulatedDtSplit &lt;- loo_cv(SimulatedDt)\n\n# Transforming to data.table\nsetDT(SimulatedDtSplit)\n\npaste0(\"y ~ poly(x, degree=\", 1:4, \")\") |&gt;\n  lapply(collect_loo_testing_error,\n         loo_split = SimulatedDtSplit) |&gt;\n  rbindlist()\n\n   degree .metric .estimator .estimate\n   &lt;char&gt;  &lt;char&gt;     &lt;char&gt;     &lt;num&gt;\n1:      1    rmse   standard 2.6996595\n2:      2    rmse   standard 0.9682064\n3:      3    rmse   standard 0.9780705\n4:      4    rmse   standard 0.9766805\n\n\nD\n(D) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?\nThe results will be the same as LOOCV don’t perform any random process.\nE\n(E) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.\nThe model with the lowest test error is the model using as a base the second grade evacuation. It’s what we were expecting, as we know the true form of the original function it’s a second degree one and adding more flexibility to the model just over-fit it.\nF\n(F) Comment on the statistical signiﬁcance of the coeﬃcient estimates that results from ﬁtting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\nFor the first model the predictor it’s significant.\n\nlm(y ~ poly(x, degree= 1), data = SimulatedDt) |&gt;\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 1), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5161 -0.6800  0.6812  1.5491  3.8183 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -1.550      0.260  -5.961 3.95e-08 ***\npoly(x, degree = 1)    6.189      2.600   2.380   0.0192 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.6 on 98 degrees of freedom\nMultiple R-squared:  0.05465,   Adjusted R-squared:  0.045 \nF-statistic: 5.665 on 1 and 98 DF,  p-value: 0.01924\n\n\nFor the second model the predictors are very significant.\n\nlm(y ~ poly(x, degree= 2), data = SimulatedDt) |&gt;\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 2), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9650 -0.6254 -0.1288  0.5803  2.2700 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -1.5500     0.0958  -16.18  &lt; 2e-16 ***\npoly(x, degree = 2)1   6.1888     0.9580    6.46 4.18e-09 ***\npoly(x, degree = 2)2 -23.9483     0.9580  -25.00  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.958 on 97 degrees of freedom\nMultiple R-squared:  0.873, Adjusted R-squared:  0.8704 \nF-statistic: 333.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe additional element of the function is not significant.\n\nlm(y ~ poly(x, degree= 3), data = SimulatedDt) |&gt;\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 3), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9765 -0.6302 -0.1227  0.5545  2.2843 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -1.55002    0.09626 -16.102  &lt; 2e-16 ***\npoly(x, degree = 3)1   6.18883    0.96263   6.429 4.97e-09 ***\npoly(x, degree = 3)2 -23.94830    0.96263 -24.878  &lt; 2e-16 ***\npoly(x, degree = 3)3   0.26411    0.96263   0.274    0.784    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9626 on 96 degrees of freedom\nMultiple R-squared:  0.8731,    Adjusted R-squared:  0.8691 \nF-statistic: 220.1 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nThe additional element of the function is not significant.\n\nlm(y ~ poly(x, degree= 4), data = SimulatedDt) |&gt;\n    summary()\n\n\nCall:\nlm(formula = y ~ poly(x, degree = 4), data = SimulatedDt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0550 -0.6212 -0.1567  0.5952  2.2267 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -1.55002    0.09591 -16.162  &lt; 2e-16 ***\npoly(x, degree = 4)1   6.18883    0.95905   6.453 4.59e-09 ***\npoly(x, degree = 4)2 -23.94830    0.95905 -24.971  &lt; 2e-16 ***\npoly(x, degree = 4)3   0.26411    0.95905   0.275    0.784    \npoly(x, degree = 4)4   1.25710    0.95905   1.311    0.193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9591 on 95 degrees of freedom\nMultiple R-squared:  0.8753,    Adjusted R-squared:  0.8701 \nF-statistic: 166.7 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n\n9\n9. We will now consider the Boston housing data set, from the ISLR2 library.\nA\n(A) Based on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat{\\mu}\\).\n\nBostonMedvMean &lt;- mean(ISLR2::Boston$medv)\n\nBostonMedvMean\n\n[1] 22.53281\n\n\nB\n(B) Provide an estimate of the standard error of \\(\\hat{\\mu}\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n\nBostonMedvSeEstimation &lt;- sd(ISLR2::Boston$medv)/sqrt(nrow(ISLR2::Boston))\n\nC\n(C) Now estimate the standard error of \\(\\hat{\\mu}\\). using the bootstrap. How does this compare to your answer from (b)?\nBoth estimations are really close.\n\n# Using the infer package as just need to estimate\n# a single number\n\nset.seed(123)\n\nBostonMedvBootstrap &lt;-\n  ISLR2::Boston |&gt;\n  specify(medv ~ NULL) |&gt;\n  generate(reps = 5000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\") \n\nBostonMedvBootstrap |&gt;\n  summarize(Se_bootstrap = sd(stat)) |&gt;\n  mutate(Se_estimation = BostonMedvSeEstimation,\n         diff = Se_bootstrap - Se_estimation)\n\n# A tibble: 1 × 3\n  Se_bootstrap Se_estimation    diff\n         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1        0.414         0.409 0.00481\n\n\nD\n(D) Based on your bootstrap estimate from (c), provide a 95 % conﬁdence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv). Hint: You can approximate a 95 % conﬁdence interval using the formula [\\(\\hat{\\mu} - 2\\text{SE}(\\hat{\\mu}), \\hat{\\mu} + 2\\text{SE}(\\hat{\\mu})\\) .\n\nget_ci(BostonMedvBootstrap, \n       point_estimate = BostonMedvMean,\n       level = 0.95,\n       type = \"se\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     21.7     23.3\n\n\nE\n(E) Based on this data set, provide an estimate, \\(\\hat{\\mu}_{med}\\), for the median value of medv in the population.\n\nmedian(ISLR2::Boston$medv)\n\n[1] 21.2\n\n\nF\n(F) We now would like to estimate the standard error of \\(\\hat{\\mu}_{med}\\). Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your ﬁndings.\nThe intervals for the median seams to be a little bit lower than the ones for the average. It seems the distributions of medv is right skewed.\n\nset.seed(77)\n\nISLR2::Boston |&gt;\n  specify(medv ~ NULL) |&gt;\n  generate(reps = 5000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"median\") |&gt;\n  get_ci(level = 0.95,\n         type = \"percentile\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     20.5     21.9\n\n\nRight skewed has been confirmed.\n\nggplot(ISLR2::Boston, aes(medv))+\n  geom_histogram(fill = \"dodgerblue3\", \n                 alpha = 0.9, bins = 15)+\n  theme_light()+\n  theme(panel.grid = element_blank())\n\n\n\n\nG\n(G) Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity \\(\\hat{\\mu}_{0.1}\\). (You can use the quantile() function.)\n\nquantile(ISLR2::Boston$medv, probs = 0.1)\n\n  10% \n12.75 \n\n\nH\n(H) Use the bootstrap to estimate the standard error of \\(\\hat{\\mu}_{0.1}\\). Comment on your ﬁndings.\nThe standard error is slightly larger relative to \\(\\hat{\\mu}_{0.1}\\), but it is still small.\n\nset.seed(77)\n\nISLR2::Boston |&gt;\n  specify(medv ~ NULL) |&gt;\n  generate(reps = 5000, type = \"bootstrap\") |&gt;\n  group_by(replicate) |&gt;\n  summarize(medv_tenth_percentile = quantile(medv, probs = 0.1)) |&gt;\n  summarize(se_medv_tenth_percentile = sd(medv_tenth_percentile))\n\n# A tibble: 1 × 1\n  se_medv_tenth_percentile\n                     &lt;dbl&gt;\n1                    0.507"
  },
  {
    "objectID": "11-execises.html#libraries",
    "href": "11-execises.html#libraries",
    "title": "11 - Survival Analysis and Censored Data",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(data.table)\n\nBrainCancer &lt;- ISLR2::BrainCancer\nBrainCancerDt &lt;- as.data.table(ISLR2::BrainCancer)"
  },
  {
    "objectID": "11-execises.html#conceptual",
    "href": "11-execises.html#conceptual",
    "title": "11 - Survival Analysis and Censored Data",
    "section": "Conceptual",
    "text": "Conceptual\n1\n1. For each example, state whether or not the censoring mechanism is independent. Justify your answer.\n\n\n\n\n\n\n\nCase\nIndependent\nJustification\n\n\n\n\n(A) In a study of disease relapse, due to a careless research scientist, all patients whose phone numbers begin with the number “2” are lost to follow up.\nTRUE\nPhone number don’t any relation with disease relapse\n\n\n\n(B) In a study of longevity, a formatting error causes all patient ages that exceed 99 years to be lost (i.e. we know that those patients are more than 99 years old, but we do not know their exact ages).\nFALSE\nAs older is a person has higher chances to die\n\n\n\n(C) Hospital A conducts a study of longevity. However, very sick patients tend to be transferred to Hospital B, and are lost to follow up.\nFALSE\nVery sick patients has higher chances to die\n\n\n\n(D) In a study of unemployment duration, the people who find work earlier are less motivated to stay in touch with study investigators, and therefore are more likely to be lost to follow up.\nFALSE\nCensoring time is correlated with the event time.\n\n\n\n(E) In a study of pregnancy duration, women who deliver their babies pre-term are more likely to do so away from their usual hospital, and thus are more likely to be censored, relative to women who deliver full-term babies.\nFALSE\nCensoring time is correlated with the event time.\n\n\n\n(F) A researcher wishes to model the number of years of education of the residents of a small town. Residents who enroll in college out of town are more likely to be lost to follow up, and are also more likely to attend graduate school, relative to those who attend college in town.\nTRUE\nIt’s unusual to have a censored student that don’t keep studying.\n\n\n\n(G) Researchers conduct a study of disease-free survival (i.e. time until disease relapse following treatment). Patients who have not relapsed within five years are considered to be cured, and thus their survival time is censored at five years.\nTRUE\nAfter 5 years without the disease is not probably to get it back\n\n\n\n(H) We wish to model the failure time for some electrical component. This component can be manufactured in Iowa or in Pittsburgh, with no difference in quality. The Iowa factory opened five years ago, and so components manufactured in Iowa are censored at five years. The Pittsburgh factory opened two years ago, so those components are censored at two years.\nTRUE\nThere is no reason to think that the components will failure just after been censored.\n\n\n\n(I) We wish to model the failure time of an electrical component made in two different factories, one of which opened before the other. We have reason to believe that the components manufactured in the factory that opened earlier are of higher quality.\n\nThere is no information related to the data censoring process\n\n\n2\n2. We conduct a study with \\(n = 4\\) participants who have just purchased cell phones, in order to model the time until phone replacement. The first participant replaces her phone after 1.2 years. The second participant still has not replaced her phone at the end of the two-year study period. The third participant changes her phone number and is lost to follow up (but has not yet replaced her phone) 1.5 years into the study. The fourth participant replaces her phone after 0.2 years. For each of the four participants (\\(i = 1, \\dots, 4\\)), answer the following questions using the notation introduced in Section 11.1:\n(A) Is the participant’s cell phone replacement time censored?\n\\(\\delta_1 = 1, \\; \\delta_2 = 0, \\; \\delta_3 = 0, \\; \\delta_4 = 1\\)\n(B) Is the value of \\(c_i\\) known, and if so, then what is it?\n\\(c_2 = 2, \\; c_3 = 1.5\\)\n(C) Is the value of \\(t_i\\) known, and if so, then what is it?\nThe \\(t_i\\) isn’t known.\n(D) Is the value of \\(y_i\\) known, and if so, then what is it?\n\\(y_1 = 1.2, \\; y_2 = 2, \\; y_3 = 1.5, \\; y_4 = 0.2\\)\n(E) Is the value of \\(\\delta_i\\) known, and if so, then what is it?\n\\(\\delta_1 = 1, \\; \\delta_2 = 0, \\; \\delta_3 = 0, \\; \\delta_4 = 1\\)\n3\n3. For the example in Exercise 2, report the values of \\(K\\), \\(d_1,\\dots ,d_K\\), \\(r_1, \\dots , r_K\\), and \\(q_1, \\dots, q_K\\), where this notation was defined in Section 11.3.\n\n\nK\nr\nq\n\n\n\n1\n0\n1\n\n\n2\n1\n0\n\n\n3\n1\n0\n\n\n4\n0\n1\n\n\n4\n4. This problem makes use of the Kaplan-Meier survival curve displayed in Figure 11.9. The raw data that went into plotting this survival curve is given in Table 11.4. The covariate column of that table is not needed for this problem.\n\n\n\n\n\ny_values &lt;- c(26.5, 37.2, 57.3, 90.8, 20.2, 89.8)\nnames(y_values) &lt;- c(rep(1,3),rep(0,3))\n\nsort(y_values)\n\n   0    1    1    1    0    0 \n20.2 26.5 37.2 57.3 89.8 90.8 \n\nround(4/5 * 3/4 * 2/3, 2)\n\n[1] 0.4\n\n\n(A) What is the estimated probability of survival past 50 days?\n\nround(4/5 * 3/4, 2)\n\n[1] 0.6\n\n\n(B) Write out an analytical expression for the estimated survival function.\n\\[\n\\hat{S}(t) =\n\\begin{cases}\n1.00 & \\; \\text{if } t &lt; 26.5\\\\\n0.80 & \\; \\text{if } 26.5 \\leq t &lt; 37.2\\\\\n0.60 & \\; \\text{if } 37.2 \\leq t &lt; 57.3\\\\\n0.40 & \\; \\text{if } 57.3 \\leq t\\\\\n\\end{cases}\n\\]\n5\n5. Sketch the survival function given by the equation.\n\\[\n\\hat{S}(t) =\n\\begin{cases}\n0.80 & \\; \\text{if } t &lt; 31\\\\\n0.50 & \\; \\text{if } 31 \\leq t &lt; 77\\\\\n0.22 & \\; \\text{if } 77 \\leq t \\\\\n\\end{cases}\n\\]\n\ndata.frame(\n  time = c(0:31,31:77, 77),\n  surv = c(rep(0.8, 32), rep(0.5, 47), 0.22)\n) |&gt;\nggplot(aes(time, surv))+\n  geom_line(color = \"blue\")+\n  expand_limits(y = 1)+\n  scale_x_continuous(breaks = seq(0, 80, by = 10))+\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1),\n                     labels = \\(x) paste0(x *100,\"%\"))+\n  theme_classic()"
  },
  {
    "objectID": "11-execises.html#applied",
    "href": "11-execises.html#applied",
    "title": "11 - Survival Analysis and Censored Data",
    "section": "Applied",
    "text": "Applied\n10\n10. This exercise focuses on the brain tumor data, which is included in the ISLR2 R library.\n(A) Plot the Kaplan-Meier survival curve with \\(\\pm1\\) standard error bands, using the survfit() function in the survival package.\n\nBrainCancerKmSurv &lt;- survfit(\n  Surv(time, status) ~ 1, \n  data = BrainCancerDt\n)\n\nggsurvplot(BrainCancerKmSurv, color = \"blue\")\n\n\n\n\n(B) Draw a bootstrap sample of size n = 88 from the pairs (yi, δi), and compute the resulting Kaplan-Meier survival curve. Repeat this process B = 200 times. Use the results to obtain an estimate of the standard error of the Kaplan-Meier survival curve at each timepoint. Compare this to the standard errors obtained in (a)\n\nBrainCancerKmSurvDt &lt;- surv_summary(BrainCancerKmSurv)\nsetDT(BrainCancerKmSurvDt)\n\nset.seed(120)\n\nBrainCancerBT &lt;-\n  data.table(resample = 1:200\n  )[, .(data = .(\n    survfit(Surv(time, status) ~ 1, \n            data = BrainCancerDt[sample.int(88, replace = TRUE)]) |&gt;\n    surv_summary()\n  )),\n    by = \"resample\"\n  ][, data[[1]], by = \"resample\"\n  ][, .(bootstrap_se = sd(surv)),\n    by = \"time\"\n  ][BrainCancerKmSurvDt, on = \"time\"\n  ][, melt(.SD, measure.vars = c(\"std.err\", \"bootstrap_se\"),\n           value.name = \"Standard Error\",\n           variable.name = \"Estimation Method\")]\n\nggplot(BrainCancerBT, aes(time, `Standard Error`, color = `Estimation Method`))+\n  geom_line(linewidth = 1)+\n  scale_y_continuous(breaks = breaks_width(0.02), labels = percent_format())+\n  scale_x_continuous(breaks = breaks_width(10), labels = comma_format())+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\n\n(C) Fit a Cox proportional hazards model that uses all of the predictors to predict survival. Summarize the main findings.\n\nBrainCancerCox &lt;- coxph(\n  Surv(time, status) ~ ., \n  data = BrainCancer\n)\n\nBrainCancerCox\n\nCall:\ncoxph(formula = Surv(time, status) ~ ., data = BrainCancer)\n\n                       coef exp(coef) se(coef)      z        p\nsexMale             0.18375   1.20171  0.36036  0.510  0.61012\ndiagnosisLG glioma  0.91502   2.49683  0.63816  1.434  0.15161\ndiagnosisHG glioma  2.15457   8.62414  0.45052  4.782 1.73e-06\ndiagnosisOther      0.88570   2.42467  0.65787  1.346  0.17821\nlocSupratentorial   0.44119   1.55456  0.70367  0.627  0.53066\nki                 -0.05496   0.94653  0.01831 -3.001  0.00269\ngtv                 0.03429   1.03489  0.02233  1.536  0.12466\nstereoSRT           0.17778   1.19456  0.60158  0.296  0.76760\n\nLikelihood ratio test=41.37  on 8 df, p=1.776e-06\nn= 87, number of events= 35 \n   (1 observation deleted due to missingness)\n\n\n(D) Stratify the data by the value of ki . (Since only one observation has ki=40 , you can group that observation together with the observations that have ki=60 .) Plot Kaplan-Meier survival curves for each of the five strata, adjusted for the other predictors.\n\nBrainCancerKiGrid &lt;-\n  BrainCancerDt[, lapply(.SD, \\(x) if(is.integer(x)) fifelse(x == 40L, 60L, x) else x[1L] )\n  ][, unique(.SD) , .SDcols = !c(\"time\",\"status\")]\n\nBrainCancerKiSv &lt;-\n  survfit(BrainCancerCox,\n          data = BrainCancer,\n          newdata = BrainCancerKiGrid,\n          conf.type = \"none\")\n\nplot(BrainCancerKiSv,\n     xlab = \"Months\",\n     ylab = \"Survival Probability\",\n     col = 2:6)\nlegend(\"bottomleft\",\n      factor(BrainCancerKiGrid$ki) |&gt;\n        as.character(), \n      col = 2:6, lty = 1)\n\n\n\n\n11\n11. This example makes use of the data in Table 11.4.\n(A) Create two groups of observations. In Group 1, X &lt;2, whereas in Group 2, X ≥ 2. Plot the Kaplan-Meier survival curves corresponding to the two groups. Be sure to label the curves so that it is clear which curve corresponds to which group. By eye, does there appear to be a difference between the two groups’ survival curves?\n\nExampleTable &lt;-\n  data.table(time = y_values,\n             status = names(y_values) |&gt; as.integer(),\n             X = c(0.1, 11, -0.3, 2.8, 1.8, 0.4)\n  )[, under_2 := factor(X &lt; 2)]\n\n\nExampleTableSv &lt;- survfit(\n  Surv(time, status) ~ under_2, \n  data = ExampleTable\n)\n\nplot(ExampleTableSv,\n     xlab = \"Months\",\n     ylab = \"Survival Probability\",\n     col = 2:6)\nlegend(\"bottomleft\",\n       levels(ExampleTable$under_2), \n       col = 2:6, lty = 1)\n\n\n\n\n(B) Fit Cox’s proportional hazards model, using the group indicator as a covariate. What is the estimated coefficient? Write a sentence providing the interpretation of this coefficient, in terms of the hazard or the instantaneous probability of the event. Is there evidence that the true coefficient value is non-zero?\n\ncoxph(\n  Surv(time, status) ~ under_2, \n  data = ExampleTable\n)\n\nCall:\ncoxph(formula = Surv(time, status) ~ under_2, data = ExampleTable)\n\n              coef exp(coef) se(coef)     z     p\nunder_2TRUE 0.3401    1.4051   1.2359 0.275 0.783\n\nLikelihood ratio test=0.08  on 1 df, p=0.7797\nn= 6, number of events= 3 \n\n\n(C) Recall from Section 11.5.2 that in the case of a single binary covariate, the log-rank test statistic should be identical to the score statistic for the Cox model. Conduct a log-rank test to determine whether there is a difference between the survival curves for the two groups. How does the p-value for the log-rank test statistic compare to the p-value for the score statistic for the Cox model from (b)?\n\nsurvdiff(\n  Surv(time, status) ~ under_2, \n  data = ExampleTable\n)\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ under_2, data = ExampleTable)\n\n              N Observed Expected (O-E)^2/E (O-E)^2/V\nunder_2=FALSE 2        1     1.23    0.0441    0.0764\nunder_2=TRUE  4        2     1.77    0.0308    0.0764\n\n Chisq= 0.1  on 1 degrees of freedom, p= 0.8"
  },
  {
    "objectID": "non-parametric-models.html#support-vector-machine",
    "href": "non-parametric-models.html#support-vector-machine",
    "title": "\n8  Non-parametric Methods\n",
    "section": "\n8.3 Support Vector Machine",
    "text": "8.3 Support Vector Machine\n\n8.3.1 Maximal Margin Classifier\nIn a p-dimensional space, a hyperplane is a flat affine subspace of hyperplane dimension \\(p − 1\\).\n\\[\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p = 0\n\\]\nBut if a point \\(X = (X_1, X_3, \\dots, X_p)^T\\) doesn’t satisfy that equation that equation the point would lies to one or other side the equation: - Over the hyperplane if \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p &gt; 0\\) - Under the hyperplane if \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p &lt; 0\\)\nBut as we can see below there are several possible hyperplane that could do the job.\n\n\n\n\nTo solve this problem, we need to find the maximal margin hyperplane by computing the perpendicular distance from each training observation to a given separating hyperplane to select the farthest hyperplane from the training observations.\n\n\n\n\nIn the last example the two blue points and the purple point that lie on the dashed lines are the support vectors as they “support” the maximal margin hyperplane.\nThe maximal margin hyperplane is the solution to the next optimization problem where the associated class labels \\(y_1, \\dots, y_n \\in \\{-1, 1\\}\\):\n\\[\n\\begin{split}\n\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, M}{\\text{maximize}} & \\; M \\\\\n\\text{subject to } \\sum_{j=1}^p \\beta_{j}^2  & = 1 , \\\\\ny_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}) & \\geq M \\; \\; \\forall \\; i = 1, \\dots, n\n\\end{split}\n\\]\nThis method has two disadvantages:\n\nSometimes there isn’t any possible separating hyperplane.\n\n\n\n\n\n\nClassifying correctly all of the training can lead to sensitivity to individual observations, returning as a result a overfitted model.\n\n\n\n\n\n\n8.3.2 Support Vector Classifiers (soft margin classifier)\nTo solve this problem we need to allow that:\n\nSome observations will be on the incorrect side of the margin like observations 1 and 8.\nSome observations will be on the incorrect side of the hyperplane like observations 11 and 12.\n\n\n\n\n\nTo get that result, we need to solve the next problem:\n\\[\n\\begin{split}\n\\underset{\\beta_0, \\beta_1, \\dots, \\beta_p, M}{\\text{maximize}} & \\; M \\\\\n\\text{subject to } \\sum_{j=1}^p \\beta_{j}^2  & = 1 , \\\\\ny_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}) & \\geq M (1 - \\epsilon_i), \\\\\n\\epsilon_i \\geq 0, \\; \\sum_{i=1}^n \\epsilon_i \\leq C,\n\\end{split}\n\\]\n\nWhere:\n\n\n\\(M\\) is the width of the margin.\n\n\\(\\epsilon_1, \\dots, \\epsilon_n\\) are slack variables that allow individual observations to be on the wrong side of the margin or the hyperplane.\n\nIf \\(\\epsilon_i = 0\\) then the ith observation is on the correct side of the margin.\nIf \\(\\epsilon_i &gt; 0\\) then the ith observation is on the wrong side of the margin.\nIf \\(\\epsilon_i &gt; 1\\) then the ith observation is on the wrong side of the hyperplane.\n\n\n\n\\(C\\) is a nonnegative tuning parameter that represents the budget for the amount that the margin can be violated by the n observations. For \\(C &gt; 0\\) no more than \\(C\\) observations can be on the wrong side of the hyperplane.\n\n\n\nIt’s important to point that only observations that either lie on the margin or violate the margin will affect the hyperplane.\n\n\n\n\n\n8.3.3 Non-linear boundaries\nTo extend the Support Vector Classifier to non-lineal settings we need to use functions that quantifies the similarity of two observations, known as kernels \\(K(x_i, x_{i'})\\) and implement it to the hyperplane function for the support vectors \\(\\mathcal{S}\\).\n\\[\nf(x) = \\beta_0 + \\sum_{i \\in \\mathcal{S}} \\alpha_i K(x_i, x_{i'})\n\\] And depending on the shape that we want to use there are 2 types of kernels to use.\nPolynomial\nAs the degree \\(d\\) increases the fit becomes more non-linear\n\\[\nK(x_i, x_{i'}) = (1 + \\sum_{j=1}^p x_{ij} x_{i'j})^d\n\\]\nRadial\nAs \\(\\gamma\\) increases the fit becomes more non-linear.\n\\[\nK(x_i, x_{i'}) = \\exp(-\\gamma \\sum_{j=1}^p(x_{ij}-x_{i'j})^2)\n\\]\n\n8.3.4 Extending SVMs to the K-class case\nTo extend this method we have 2 alternatives\n\nOne-Versus-One Classification: It constructs \\(\\left( \\begin{array}{c} K \\\\ 2 \\end{array} \\right)\\), tallys the number of times that the test observation is assigned to each of the \\(K\\). The final classification is performed by assigning the test observation to the class to which it was most frequently assigned.\nOne-Versus-All Classification: We fit \\(K\\) SVMs, each time comparing one of the \\(K\\) classes to the remaining \\(K − 1\\) classes. Let \\(\\beta = \\beta_{0k}, \\beta_{1k}, \\dots, \\beta_{pk}\\) denote the parameters that result from fitting an SVM comparing the \\(k\\)th class (coded as \\(+1\\)) to the others (coded as \\(−1\\)). We assign the observation to the class for which the lineal combination of coefficients and the test observation \\(\\beta x^*\\) is largest.\n\n8.3.5 Coding Example\n\nLoad libraries\n\n\nlibrary(tidymodels)\nlibrary(ISLR)\nlibrary(kernlab)\ntheme_set(theme_light())\n\nset.seed(1)\nsim_data &lt;- tibble(\n  x1 = rnorm(40),\n  x2 = rnorm(40),\n  y  = factor(rep(c(-1, 1), 20))\n) %&gt;%\n  mutate(x1 = ifelse(y == 1, x1 + 1.5, x1),\n         x2 = ifelse(y == 1, x2 + 1.5, x2))\n\nggplot(sim_data, aes(x1, x2, color = y)) +\n  geom_point()\n\n\n\n\n\nDefine model specification\n\n\nsvm_linear_spec &lt;- svm_poly(degree = 1) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  # We don't need scaling for now\n  set_engine(\"kernlab\", scaled = FALSE)\n\n\nFitting the model and checking results\n\n\nsvm_linear_fit &lt;- svm_linear_spec %&gt;% \n  set_args(cost = 10) %&gt;%\n  fit(y ~ ., data = sim_data)\n\nsvm_linear_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  plot()"
  }
]