<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Angel Feliz">
<title>An Introduction to Statistical Learning (Non Official Solution Book) - 8&nbsp; Non-parametric Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./survival-analysis.html" rel="next">
<link href="./flexible-regression.html" rel="prev">
<link href="./cover.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="summary-format.css">
</head>
<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg navbar-dark "><div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">An Introduction to Statistical Learning (Non Official Solution Book)</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto">
    <a href="https://github.com/AngelFelizR/ISL-Solutuon-Book" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./modeling-process.html">Teorical Summary</a></li><li class="breadcrumb-item"><a href="./non-parametric-models.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-parametric Methods</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Teorical Summary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modeling-process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Strategy to implement machine learning solutions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./missing-values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Dealing with missingness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Understanding model performance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resampling-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Resampling methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generalized-linear-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Generalized linear models (GLM)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generative-classification-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generative Models for Classiﬁcation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./flexible-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Flexible Regression Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./non-parametric-models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-parametric Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Time-event (Survival) Analysis and Censored Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Exercise Solutions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-execises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">02 - Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-execises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">03 - Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-execises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">04 - Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-execises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">05 - Resampling Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-execises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11 - Survival Analysis and Censored Data</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#k-nearest-neighbors-knn" id="toc-k-nearest-neighbors-knn" class="nav-link active" data-scroll-target="#k-nearest-neighbors-knn"><span class="header-section-number">8.1</span> K-nearest neighbors (KNN)</a>
  <ul>
<li><a href="#classi%EF%AC%81er" id="toc-classiﬁer" class="nav-link" data-scroll-target="#classi%EF%AC%81er"><span class="header-section-number">8.1.1</span> Classiﬁer</a></li>
  <li><a href="#regression" id="toc-regression" class="nav-link" data-scroll-target="#regression"><span class="header-section-number">8.1.2</span> Regression</a></li>
  <li><a href="#pre-processing" id="toc-pre-processing" class="nav-link" data-scroll-target="#pre-processing"><span class="header-section-number">8.1.3</span> Pre-processing</a></li>
  <li><a href="#coding-example" id="toc-coding-example" class="nav-link" data-scroll-target="#coding-example"><span class="header-section-number">8.1.4</span> Coding example</a></li>
  </ul>
</li>
  <li>
<a href="#tree-based-methods" id="toc-tree-based-methods" class="nav-link" data-scroll-target="#tree-based-methods"><span class="header-section-number">8.2</span> Tree-Based Methods</a>
  <ul>
<li>
<a href="#simple-trees" id="toc-simple-trees" class="nav-link" data-scroll-target="#simple-trees"><span class="header-section-number">8.2.1</span> Simple trees</a>
  <ul class="collapse">
<li><a href="#advantages-and-disadvantages" id="toc-advantages-and-disadvantages" class="nav-link" data-scroll-target="#advantages-and-disadvantages">Advantages and disadvantages</a></li>
  <li><a href="#regression-1" id="toc-regression-1" class="nav-link" data-scroll-target="#regression-1">Regression</a></li>
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification">Classification</a></li>
  <li><a href="#coding-example-1" id="toc-coding-example-1" class="nav-link" data-scroll-target="#coding-example-1">Coding example</a></li>
  </ul>
</li>
  <li>
<a href="#bagging-bootstrap-aggregation" id="toc-bagging-bootstrap-aggregation" class="nav-link" data-scroll-target="#bagging-bootstrap-aggregation"><span class="header-section-number">8.2.2</span> Bagging (bootstrap aggregation)</a>
  <ul class="collapse">
<li><a href="#out-of-bag-error-estimation" id="toc-out-of-bag-error-estimation" class="nav-link" data-scroll-target="#out-of-bag-error-estimation">Out-of-bag error estimation</a></li>
  <li><a href="#variable-importance-measures" id="toc-variable-importance-measures" class="nav-link" data-scroll-target="#variable-importance-measures">Variable importance measures</a></li>
  </ul>
</li>
  <li>
<a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">8.2.3</span> Random Forests</a>
  <ul class="collapse">
<li><a href="#parameters-to-tune" id="toc-parameters-to-tune" class="nav-link" data-scroll-target="#parameters-to-tune">Parameters to tune</a></li>
  </ul>
</li>
  <li>
<a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="header-section-number">8.2.4</span> Boosting</a>
  <ul class="collapse">
<li><a href="#parameters-to-tune-1" id="toc-parameters-to-tune-1" class="nav-link" data-scroll-target="#parameters-to-tune-1">Parameters to tune</a></li>
  </ul>
</li>
  <li><a href="#bayesian-additive-regression-trees-bart" id="toc-bayesian-additive-regression-trees-bart" class="nav-link" data-scroll-target="#bayesian-additive-regression-trees-bart"><span class="header-section-number">8.2.5</span> Bayesian additive regression trees (BART)</a></li>
  </ul>
</li>
  <li>
<a href="#support-vector-machine" id="toc-support-vector-machine" class="nav-link" data-scroll-target="#support-vector-machine"><span class="header-section-number">8.3</span> Support Vector Machine</a>
  <ul>
<li><a href="#maximal-margin-classifier" id="toc-maximal-margin-classifier" class="nav-link" data-scroll-target="#maximal-margin-classifier"><span class="header-section-number">8.3.1</span> Maximal Margin Classifier</a></li>
  <li><a href="#support-vector-classifiers-soft-margin-classifier" id="toc-support-vector-classifiers-soft-margin-classifier" class="nav-link" data-scroll-target="#support-vector-classifiers-soft-margin-classifier"><span class="header-section-number">8.3.2</span> Support Vector Classifiers (soft margin classifier)</a></li>
  <li>
<a href="#non-linear-boundaries" id="toc-non-linear-boundaries" class="nav-link" data-scroll-target="#non-linear-boundaries"><span class="header-section-number">8.3.3</span> Non-linear boundaries</a>
  <ul class="collapse">
<li><a href="#polynomial" id="toc-polynomial" class="nav-link" data-scroll-target="#polynomial">Polynomial</a></li>
  <li><a href="#radial" id="toc-radial" class="nav-link" data-scroll-target="#radial">Radial</a></li>
  </ul>
</li>
  <li><a href="#extending-svms-to-the-k-class-case" id="toc-extending-svms-to-the-k-class-case" class="nav-link" data-scroll-target="#extending-svms-to-the-k-class-case"><span class="header-section-number">8.3.4</span> Extending SVMs to the K-class case</a></li>
  <li><a href="#coding-example-2" id="toc-coding-example-2" class="nav-link" data-scroll-target="#coding-example-2"><span class="header-section-number">8.3.5</span> Coding Example</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-parametric Methods</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><section id="k-nearest-neighbors-knn" class="level2" data-number="8.1"><h2 data-number="8.1" class="anchored" data-anchor-id="k-nearest-neighbors-knn">
<span class="header-section-number">8.1</span> K-nearest neighbors (KNN)</h2>
<p>It uses the principle of nearest neighbors to classify unlabeled examples by using the <strong>Euclidean Distance</strong> to calculate distance between the point we want to predict and <span class="math inline">\(k\)</span> closest neighbors on the training data.</p>
<p><span class="math display">\[
d\left( a,b\right)   = \sqrt {\sum _{i=1}^{p}  \left( a_{i}-b_{i}\right)^2 }
\]</span></p>
<p>KNN unlike parametric models does not tell us which predictors are important, making it hard to make inferences using this model.</p>
<p>This method performs worst than a parametric as we starting adding <em>noise</em> predictors. In fact, we will get in the situation where for a given observation has no <em>nearby neighbors</em>, known as <strong>curse of dimensionality</strong> and leading to a very poor prediction of <span class="math inline">\(f(x_{0})\)</span>.</p>
<section id="classiﬁer" class="level3" data-number="8.1.1"><h3 data-number="8.1.1" class="anchored" data-anchor-id="classiﬁer">
<span class="header-section-number">8.1.1</span> Classiﬁer</h3>
<p>The next function estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of points in <span class="math inline">\(N_{0}\)</span> whose response values equal <span class="math inline">\(j\)</span>.</p>
<p><span class="math display">\[
\text{Pr}(Y = j|X = x_{0}) = \frac{1}{K}
                      \displaystyle\sum_{i \in N_{0}} I(y_{i} = j)
\]</span></p>
<ul>
<li>Where
<ul>
<li>
<span class="math inline">\(j\)</span> response value to test</li>
<li>
<span class="math inline">\(x_{0}\)</span> is the test observation</li>
<li>
<span class="math inline">\(K\)</span> the number of points in the training data that are closest to <span class="math inline">\(x_{0}\)</span> and reduce the model flexibility</li>
<li>
<span class="math inline">\(N_{0}\)</span> points in the training data that are closest to <span class="math inline">\(x_{0}\)</span>
</li>
</ul>
</li>
</ul>
<p>Then KNN classiﬁes the test observation <span class="math inline">\(x_{0}\)</span> to the class with the largest probability.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/08-knn-classifier.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="regression" class="level3" data-number="8.1.2"><h3 data-number="8.1.2" class="anchored" data-anchor-id="regression">
<span class="header-section-number">8.1.2</span> Regression</h3>
<p>KNN regression estimates <span class="math inline">\(f(x_{0})\)</span> using the average of all the training responses in <span class="math inline">\(N_{0}\)</span>.</p>
<p><span class="math display">\[
\hat{f}(x_{0}) = \frac{1}{K}
                      \displaystyle\sum_{i \in N_{0}} y_{i}
\]</span></p>
<ul>
<li>Where
<ul>
<li>
<span class="math inline">\(x_{0}\)</span> is the test observation</li>
<li>
<span class="math inline">\(K\)</span> the number of points in the training data that are closest to <span class="math inline">\(x_{0}\)</span> and reduce the model flexibility</li>
<li>
<span class="math inline">\(N_{0}\)</span> points in the training data that are closest to <span class="math inline">\(x_{0}\)</span>
</li>
</ul>
</li>
</ul></section><section id="pre-processing" class="level3" data-number="8.1.3"><h3 data-number="8.1.3" class="anchored" data-anchor-id="pre-processing">
<span class="header-section-number">8.1.3</span> Pre-processing</h3>
<p>To use this method we need to make sure that all our variables are numeric. If one our variables is a factor we need to perform a dummy transformation of that variable with the <code><a href="https://recipes.tidymodels.org/reference/step_dummy.html">recipes::step_dummy</a></code> function.</p>
<p>On the other hand, as this model uses distances to make predicts it’s important to check that each feature of the input data is measured with <strong>the same range of values</strong> with the <code><a href="https://recipes.tidymodels.org/reference/step_range.html">recipes::step_range</a></code> function which normalize from 0 to 1 as happens with the dummy function.</p>
<p><span class="math display">\[
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
\]</span></p>
<p>Another normalization alternative is centering the predictors in <span class="math inline">\(\overline{x} = 0\)</span> with <span class="math inline">\(S = 0\)</span> with the function <code><a href="https://recipes.tidymodels.org/reference/step_normalize.html">recipes::step_normalize</a></code> or the function <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> which apply the <a href="https://developers.google.com/machine-learning/data-prep/transform/normalization">z-score normalization</a>.</p>
<p><span class="math display">\[
x' = \frac{x - \mu}{\sigma}
\]</span></p>
</section><section id="coding-example" class="level3" data-number="8.1.4"><h3 data-number="8.1.4" class="anchored" data-anchor-id="coding-example">
<span class="header-section-number">8.1.4</span> Coding example</h3>
<p>To perform <strong>K-Nearest Neighbors</strong> we just need to create the model specification by using <strong>kknn</strong> engine.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">Smarket_train</span> <span class="op">&lt;-</span> </span>
<span>  <span class="va">Smarket</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">!=</span> <span class="fl">2005</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Smarket_test</span> <span class="op">&lt;-</span> </span>
<span>  <span class="va">Smarket</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">==</span> <span class="fl">2005</span><span class="op">)</span></span>
<span></span>
<span><span class="va">knn_spec</span> <span class="op">&lt;-</span> <span class="fu">nearest_neighbor</span><span class="op">(</span>neighbors <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"classification"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"kknn"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">SmarketKnnPredictions</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">knn_spec</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">fit</span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, data <span class="op">=</span> <span class="va">Smarket_train</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">augment</span><span class="op">(</span>new_data <span class="op">=</span> <span class="va">Smarket_test</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu">conf_mat</span><span class="op">(</span><span class="va">SmarketKnnPredictions</span>, truth <span class="op">=</span> <span class="va">Direction</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Truth
Prediction Down Up
      Down   43 58
      Up     68 83</code></pre>
</div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">accuracy</span><span class="op">(</span><span class="va">SmarketKnnPredictions</span>, truth <span class="op">=</span> <span class="va">Direction</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy binary           0.5</code></pre>
</div>
</div>
</section></section><section id="tree-based-methods" class="level2" data-number="8.2"><h2 data-number="8.2" class="anchored" data-anchor-id="tree-based-methods">
<span class="header-section-number">8.2</span> Tree-Based Methods</h2>
<p>These methods involve <strong>stratifying the predictor</strong> space into a number of <em>simple regions</em> and then use mean or the mode response value for the training observations in the region to which it belongs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/48-Hitters-salary-regression-tree-regions.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As these results can be summarized in a tree, these types of approaches are known as <strong>decision tree</strong> methods, we have some important parts:</p>
<ul>
<li>
<em>Terminal nodes</em> (<em>leaves</em>) are represented by <span class="math inline">\(R_1\)</span>, <span class="math inline">\(R_2\)</span> and <span class="math inline">\(R_3\)</span>.</li>
<li>
<em>Internal nodes</em> refers to the points along the tree where the predictor space is split.</li>
<li>
<em>Branches</em> refer to the segments of the trees that connect the nodes.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/47-Hitters-salary-regression-tree.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>It’s important to take in consideration that the order in which is presented each predictors also explain the level of importance of each variable. For example, the number of <code>Years</code> has a higher effect over the player’s salary than the number of <code>Hits</code>.</p>
<section id="simple-trees" class="level3" data-number="8.2.1"><h3 data-number="8.2.1" class="anchored" data-anchor-id="simple-trees">
<span class="header-section-number">8.2.1</span> Simple trees</h3>
<section id="advantages-and-disadvantages" class="level4"><h4 class="anchored" data-anchor-id="advantages-and-disadvantages">Advantages and disadvantages</h4>
<table class="table">
<colgroup>
<col style="width: 43%">
<col style="width: 56%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Advantages</th>
<th style="text-align: left;">Disadvantages</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Simpler to explain than linear regression thanks to its graphical representation</td>
<td style="text-align: left;">Small change in the data can cause a large change in the final estimated tree</td>
</tr>
<tr class="even">
<td style="text-align: left;">It doesn’t need much preprocessing as: <br> - It handles qualitative predictors. <br> - It doesn’t require feature scaling or normalization. <br> - It can handle missing values and outliers</td>
<td style="text-align: left;">They aren’t so very good predicting results as: <br> - It prones to overfitting. <br> - It presents low accuracy.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">It can be used for <strong>feature selection</strong> by defining the importance of a feature based on <strong>how early it appears</strong> in the tree and <strong>how often</strong> it is used for splitting</td>
<td style="text-align: left;">They can be biased towards the majority class in imbalanced datasets</td>
</tr>
</tbody>
</table></section><section id="regression-1" class="level4"><h4 class="anchored" data-anchor-id="regression-1">Regression</h4>
<p>To create a decision tree we need to find the regions <span class="math inline">\(R_1, \dots, R_j\)</span> that minimize the RSS where <span class="math inline">\(\hat{y}_{R_j}\)</span> represent the mean response for the training observations within the <em>j</em>th box:</p>
<p><span class="math display">\[
RSS = \sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</span></p>
<p>To define the regions we use the <strong>recursive binary splitting</strong>, which consist the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> leads to the greatest possible reduction in RSS. Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. The process continues until a <em>stopping criterion is reached</em> (no region contains more than five observations).</p>
<p>This method is:</p>
<ul>
<li><p><em>Top-down</em>: It begins at the <em>top of the tree</em> (where all observations belong to a single region) and then successively splits the predictor space.</p></li>
<li><p><em>Greedy</em>: At each step of the tree-building process, <strong>the best split is made at that particular step</strong>, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p></li>
</ul>
<p>As result, we could end with a very complex tree that <strong>overfits</strong> the data. To solve this, we need <strong>prune</strong> the original tree until getting a <strong>subtree</strong> that leads to the lowest test error rate by using the <strong>cost complexity pruning</strong> approach which creates differente trees based on <span class="math inline">\(\alpha\)</span>.</p>
<p><span class="math display">\[
\sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_m}) ^2  + \alpha|T|
\]</span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">\(\alpha\)</span>: Tunning parameter <span class="math inline">\([0,\infty]\)</span> selected using <em>k-cross validation</em>
</li>
<li>
<span class="math inline">\(|T|\)</span>: Number of terminal nodes of the tree <span class="math inline">\(T\)</span>.</li>
<li>
<span class="math inline">\(R_m\)</span>: The subset of predictor space corresponding to the <span class="math inline">\(m\)</span>th terminal node</li>
<li>
<span class="math inline">\(\hat{y}_{R_m}\)</span>: Predicted response associated with <span class="math inline">\(R_m\)</span>
</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/49-tree-best-number-leaves.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="classification" class="level4"><h4 class="anchored" data-anchor-id="classification">Classification</h4>
<p>For a classification tree, we predict that each observation belongs to the most <em>commonly occurring class</em> of training observations in the region to which it belongs.</p>
<p>As we can not use RSS as a criterion for making the binary splits, the <strong>classification error rate</strong> could the fraction of the training observations in that region that do not belong to the most common class (<span class="math inline">\(1 - \max_k(\hat{p}_{mk})\)</span>), but it turns out that classification error is not sufficiently sensitive for tree-growing and we use the next metrics as they are more sensitive to <strong>node purity</strong> (<em>proportion of the main class on each terminal node</em>):</p>
<table class="table">
<colgroup>
<col style="width: 36%">
<col style="width: 63%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Formula</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Gini index</strong></td>
<td style="text-align: center;"><span class="math inline">\(G = \sum_{k = 1}^K 1 - \hat{p}_{mk} (1 -\hat{p}_{mk})\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Entropy</strong></td>
<td style="text-align: center;"><span class="math inline">\(D = -\sum_{k = 1}^K 1 - \hat{p}_{mk} \log \hat{p}_{mk}\)</span></td>
</tr>
</tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/50-tree-classification-example.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="coding-example-1" class="level4"><h4 class="anchored" data-anchor-id="coding-example-1">Coding example</h4>
<p>https://app.datacamp.com/learn/tutorials/decision-trees-R</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># For data maninulation</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com">data.table</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># For modeling and visualization</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">#</span></span>
<span><span class="va">Boston</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="fu">MASS</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/MASS/man/Boston.html">Boston</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu">pillar</span><span class="fu">::</span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html">glimpse</a></span><span class="op">(</span><span class="va">Boston</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 506
Columns: 14
$ crim    &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,…
$ zn      &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1…
$ indus   &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.…
$ chas    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ nox     &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,…
$ rm      &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,…
$ age     &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9…
$ dis     &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505…
$ rad     &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,…
$ tax     &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31…
$ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15…
$ black   &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90…
$ lstat   &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10…
$ medv    &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15…</code></pre>
</div>
</div>
</section></section><section id="bagging-bootstrap-aggregation" class="level3" data-number="8.2.2"><h3 data-number="8.2.2" class="anchored" data-anchor-id="bagging-bootstrap-aggregation">
<span class="header-section-number">8.2.2</span> Bagging (bootstrap aggregation)</h3>
<p>As we said before, simple trees has a <em>high variance</em> problem <span class="math inline">\(Var(\hat{f}(x_{0}))\)</span> and <strong>bagging</strong> can help to mitigate this problem.</p>
<p>We know from the <em>Central Limit Theorem</em> a natural way to <em>reduce the variance</em> and <em>increase the test set accuracy</em> is taking many <strong>training sets from the population</strong>, build a separate prediction model using each training set, and average the resulting prediction, as for a given a set of <span class="math inline">\(n\)</span> <strong>independent observations</strong> <span class="math inline">\(Z_1, \dots, Z_n\)</span>, each with variance <span class="math inline">\(\sigma^2\)</span>, the variance of the mean <span class="math inline">\(\overline{Z}\)</span> of the observations is given by <span class="math inline">\(\sigma^2/n\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/66-bagging-concept.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As we generally do not have access to multiple training sets we use <strong>bootstrap</strong> to take repeated samples from the one training data set, train <span class="math inline">\(B\)</span> <strong>not pruned regression trees</strong> and <strong>average</strong> the resulting predictions or select the most commonly occurring class among the <span class="math inline">\(B\)</span> predictions in classification settings.</p>
<p><span class="math display">\[
\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)
\]</span></p>
<p>The <strong>number of trees is not a critical</strong> as <span class="math inline">\(B\)</span> will not lead to overfitting. Using <span class="math inline">\(B = 100\)</span> is sufficient to achieve good performance in this example.</p>
<section id="out-of-bag-error-estimation" class="level4"><h4 class="anchored" data-anchor-id="out-of-bag-error-estimation">Out-of-bag error estimation</h4>
<p>To estimate the test error as an approximation of the <em>Leave-one-out cross validation</em> when <span class="math inline">\(B\)</span> sufficiently large, we can take advantage of the <span class="math inline">\(1/3\)</span> of observation that were <strong>out-of-bag</strong> (OOB) on each re-sample and predict the response for the <span class="math inline">\(i\)</span>th observation using each of the trees in which that observation was OOB.</p>
<p>This will yield around <span class="math inline">\(B/3\)</span> predictions for each of the <span class="math inline">\(n\)</span> observation that we can <em>average</em> or take a <em>majority</em> vote to calculate the <em>test error</em>.</p>
</section><section id="variable-importance-measures" class="level4"><h4 class="anchored" data-anchor-id="variable-importance-measures">Variable importance measures</h4>
<p>After using this method, we can’t represent the statistical learning procedure using a single tree, instead we can use the <em>RSS</em> (or the <em>Gini index</em>) to record the total amount that the RSS is decreased due to splits over a given predictor, averaged(or added) over all <span class="math inline">\(B\)</span> trees where a large value indicates an important predictor.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/51-bagging-variable-importance.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section></section><section id="random-forests" class="level3" data-number="8.2.3"><h3 data-number="8.2.3" class="anchored" data-anchor-id="random-forests">
<span class="header-section-number">8.2.3</span> Random Forests</h3>
<p>Predictions from the bagged trees, has a big problem, there are <strong>highly correlated</strong> and averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many <strong>uncorrelated quantities</strong> (<em>independent</em>).</p>
<p>To solve this problem <strong>Random Forests</strong> provide an improvement over bagged trees by <strong>decorrelates the trees</strong>. As in bagging, we build many trees based on bootstrapped training samples. But when building these decision trees <em>random forest</em> sample <span class="math inline">\(m \approx \sqrt{p}\)</span> predictors to create <span class="math inline">\(B\)</span> *independent trees, making the average of the resulting trees less variable and hence more reliable.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/52-random-forest-effect-over-bagging.png" class="img-fluid figure-img"></p>
</figure>
</div>
<section id="parameters-to-tune" class="level4"><h4 class="anchored" data-anchor-id="parameters-to-tune">Parameters to tune</h4>
<p>Random forests have the least variability in their prediction accuracy when tuning, as the <strong>default values tend to produce good results</strong>.</p>
<ol type="1">
<li>
<strong>Number of trees</strong> (<span class="math inline">\(B\)</span>): A good rule of thumb is to <strong>start with 10 times the number of features</strong> as the error estimate converges after some trees and computation time increases linearly with the number of trees.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/71-ramdom-forest-tuning-number-of-trees.png" class="img-fluid figure-img"></p>
</figure>
</div>
<ol start="2" type="1">
<li>
<strong>Number of predictors</strong> (<span class="math inline">\(m_{try}\)</span>): In <code>ranger</code>, <span class="math inline">\(m_{try} = \text{floor} \left( \frac{p}{3} \right)\)</span> in regression problems and <span class="math inline">\(m_{try} = \text{floor} \left( \sqrt{p} \right)\)</span> in classifications problems, but we can explore in the range <span class="math inline">\([2,p]\)</span>.</li>
</ol>
<ul>
<li>With <strong>few</strong> relevant predictors (e.g., noisy data) a <strong>higher number tends to perform better</strong> because it makes it more likely to <em>select those features with the strongest signal</em>.</li>
<li>With <strong>many</strong> relevant predictors a <strong>lower number might perform better</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/72-ramdom-forest-tuning-number-of-predictors.png" class="img-fluid figure-img"></p>
</figure>
</div>
<ol start="3" type="1">
<li>
<strong>Tree complexity</strong> (<em>node size</em>): The default values of <span class="math inline">\(1\)</span> for classification and <span class="math inline">\(5\)</span> for regression as these values tend to produce good results, but:</li>
</ol>
<ul>
<li>If your data has <strong>many noisy predictors</strong> and a <strong>high number of trees</strong>, then performance may improve by <strong>increasing node size</strong> (i.e., decreasing tree depth and complexity).</li>
<li>If <strong>computation time is a concern</strong> then you can often decrease run time substantially by <strong>increasing the node size</strong>.</li>
</ul>
<p>Start with three values between 1–10 and adjust depending on impact to accuracy and run time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/73-ramdom-forest-tuning-node-size.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section></section><section id="boosting" class="level3" data-number="8.2.4"><h3 data-number="8.2.4" class="anchored" data-anchor-id="boosting">
<span class="header-section-number">8.2.4</span> Boosting</h3>
<p>As well as <strong>bagging</strong>, <strong>boosting</strong> is a general approach that can be be applied to many statistical learning methods for regression or classification. Both methods create many models in order to create a single prediction <span class="math inline">\(\hat{f}^1, \dots, \hat{f}^B\)</span>.</p>
<p>To create boosting trees, in general, we need to create then <strong><em>sequentially</em></strong> by using information from prior models and fit a new model with a modified version of the original data set. To be more specific we need to flow the following steps:</p>
<ol type="1">
<li><p>Set <span class="math inline">\(\hat{f}(x) = 0\)</span> and <span class="math inline">\(r_i=y_i\)</span> for all <span class="math inline">\(i\)</span> in the training set.</p></li>
<li><p>For <span class="math inline">\(b = 1, 2, \dots, B\)</span>, repeat the next process:</p></li>
</ol>
<ul>
<li>Fit a tree <span class="math inline">\(\hat{f}^b\)</span> with d splits (d + 1 terminal nodes) to the training data (<span class="math inline">\(X,r\)</span>). In this step is very import to see that we are fitting the new model based on the residuals.</li>
<li>Update <span class="math inline">\(\hat{f}\)</span> by adding in a shrunken version of the new tree:</li>
</ul>
<p><span class="math display">\[
\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)
\]</span></p>
<ul>
<li>Update the residuals</li>
</ul>
<p><span class="math display">\[
r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)
\]</span></p>
<ol start="3" type="1">
<li>Calculate the output of the boosting model by:</li>
</ol>
<p><span class="math display">\[
\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)
\]</span></p>
<p>As result, our model <em>learn slowly</em> by adding new decision trees into the fitted function in order to update the residuals on each step. As we are going to use many models, each individual model can be small by using a low <span class="math inline">\(d\)</span> parameter. In general, statistical learning approaches that learn slowly tend to perform well.</p>
<section id="parameters-to-tune-1" class="level4"><h4 class="anchored" data-anchor-id="parameters-to-tune-1">Parameters to tune</h4>
<ul>
<li><p><strong>Number of trees</strong> (<span class="math inline">\(B\)</span>): Unlike bagging and random forests, boosting can overfit if B is too large.</p></li>
<li><p><strong>shrinkage</strong> (<span class="math inline">\(\lambda\)</span>): Controls the rate at which boosting learns, it should be a positive value its values are <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.001\)</span>. Very small <span class="math inline">\(\lambda\)</span> can require using a very large value of B in order to achieve good performance.</p></li>
<li><p><strong>Number of splits or interaction depth</strong> (<span class="math inline">\(d\)</span>): It controls the complexity of the boosted ensemble. When <span class="math inline">\(d=1\)</span> is known as a <strong>stump tree</strong> as each term involves only <em>a single variable</em>. Some times, stump tree works well and are eraser to interpret, but as <span class="math inline">\(d\)</span> increases the <em>number of variables</em> used by each model increases, with <span class="math inline">\(d\)</span> as limit.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/53-boosting-vs-random-forest.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section></section><section id="bayesian-additive-regression-trees-bart" class="level3" data-number="8.2.5"><h3 data-number="8.2.5" class="anchored" data-anchor-id="bayesian-additive-regression-trees-bart">
<span class="header-section-number">8.2.5</span> Bayesian additive regression trees (BART)</h3>
<p>This method constructs trees in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.</p>
<p>To understand the method works we need to define some important notation:</p>
<ul>
<li>
<span class="math inline">\(K\)</span>: Number of regression trees. For example <span class="math inline">\(K\)</span> could be <span class="math inline">\(100\)</span>
</li>
<li>
<span class="math inline">\(B\)</span>: Number of iterations. For example <span class="math inline">\(B\)</span> could be <span class="math inline">\(1000\)</span>
</li>
<li>
<span class="math inline">\(\hat{f}_k^b(x)\)</span>: The prediction at <span class="math inline">\(x\)</span> for the <span class="math inline">\(k\)</span>th regression tree used in the <span class="math inline">\(b\)</span>th iteration</li>
<li>
<span class="math inline">\(\hat{f}^b(x) = \sum_{k=1}^K \hat{f}_k^b(x)\)</span>: Summed of the <span class="math inline">\(K\)</span> at the end of each iteration.</li>
</ul>
<p>To apply this method we need to follow the below steps:</p>
<ol type="1">
<li>In the first iteration all trees are initialized to have a single root node,<em>the mean of the response values divided by the total number of trees</em>, in other to predict the mean of <span class="math inline">\(y\)</span> in the first iteration <span class="math inline">\(\hat{f}^1(x)\)</span>.</li>
</ol>
<p><span class="math display">\[
\hat{f}_k^1(x) = \frac{1}{nK} \sum_{i=1}^n y_i
\]</span></p>
<ol start="2" type="1">
<li>Compute the predictions of the first iteration.</li>
</ol>
<p><span class="math display">\[
\hat{f}^1(x) = \sum_{k=1}^K \hat{f}_k^1(x) = \frac{1}{n} \sum_{i=1}^n y_i
\]</span></p>
<ol start="3" type="1">
<li>For each of the following iterations <span class="math inline">\(b = 2, \dots, B\)</span>.</li>
</ol>
<ul>
<li>
<p>Update each tree <span class="math inline">\(k = 1, 2, \dots, K\)</span> by:</p>
<ul>
<li>Computing a <strong>partial residual</strong> for each tree with all trees but the <span class="math inline">\(k\)</span>th tree.</li>
</ul>
</li>
</ul>
<p><span class="math display">\[
r_i = y_i - \sum_{k'&lt;k} \hat{f}_{k'}^b(x_i) - \sum_{k'&gt;k} \hat{f}_{k'}^{b-1}(x_i)
\]</span></p>
<ul>
<li><ul>
<li>Based on the <em>partial residual</em> BART <strong>randomly choosing a perturbation to the tree</strong> from the previous iteration <span class="math inline">\(\hat{f}_k^{b-1}\)</span> from a set of possible perturbations (adding branches, prunning branches or changing the prediction of terminal nodes) favoring ones that improve the fit to the partial residual. This guards against overfitting since it limits how “hard” we fit the data in each iteration.</li>
</ul></li>
<li>Compute <span class="math inline">\(\hat{f}^b(x) = \sum_{k=1}^K \hat{f}_k^b(x)\)</span>
</li>
</ul>
<ol start="4" type="1">
<li>Compute the <em>mean</em> or a <em>percentile</em> after <span class="math inline">\(L\)</span> burn-in iterations that don’t provide good results. For example <span class="math inline">\(L\)</span> could be <span class="math inline">\(200\)</span>
</li>
</ol>
<p><span class="math display">\[
\hat{f}(x) = \frac{1}{B-L} \sum_{b=L+1}^B \hat{f}(x)
\]</span></p>
<p>This models works really well even without a tuning process and the random process modifications protects the metho to overfit as we increase the number of iterations, as we can see in the next chart.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/54-bart-vs-boosting-trees.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section></section><section id="support-vector-machine" class="level2" data-number="8.3"><h2 data-number="8.3" class="anchored" data-anchor-id="support-vector-machine">
<span class="header-section-number">8.3</span> Support Vector Machine</h2>
<section id="maximal-margin-classifier" class="level3" data-number="8.3.1"><h3 data-number="8.3.1" class="anchored" data-anchor-id="maximal-margin-classifier">
<span class="header-section-number">8.3.1</span> Maximal Margin Classifier</h3>
<p>In a p-dimensional space, a <strong>hyperplane</strong> is a flat affine subspace of hyperplane dimension <span class="math inline">\(p − 1\)</span>.</p>
<p><span class="math display">\[
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0
\]</span></p>
<p>But if a point <span class="math inline">\(X = (X_1, X_3, \dots, X_p)^T\)</span> doesn’t satisfy that equation that equation the point would lies to one or other side the equation: - Over the <em>hyperplane</em> if <span class="math inline">\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p &gt; 0\)</span> - Under the <em>hyperplane</em> if <span class="math inline">\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p &lt; 0\)</span></p>
<p>But as we can see below there are several possible <em>hyperplane</em> that could do the job.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/97-hyperplane.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>To solve this problem, we need to find the <strong>maximal margin hyperplane</strong> by computing the perpendicular distance from each training observation to a given separating hyperplane to select the farthest <em>hyperplane</em> from the training observations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/98-maximal-margin-hyperplane.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In the last example the two blue points and the purple point that lie on the dashed lines are the <strong>support vectors</strong> as they “support” the maximal margin hyperplane.</p>
<p>The maximal margin hyperplane is the solution to the next optimization problem where the <strong>associated class labels</strong> <span class="math inline">\(y_1, \dots, y_n \in \{-1, 1\}\)</span>:</p>
<p><span class="math display">\[
\begin{split}
\underset{\beta_0, \beta_1, \dots, \beta_p, M}{\text{maximize}} &amp; \; M \\
\text{subject to } \sum_{j=1}^p \beta_{j}^2  &amp; = 1 , \\
y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) &amp; \geq M \; \; \forall \; i = 1, \dots, n
\end{split}
\]</span></p>
<p>This method has two disadvantages:</p>
<ol type="1">
<li>Sometimes there isn’t any possible <em>separating hyperplane</em>.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/99-no-posible-hyperplane.png" class="img-fluid figure-img"></p>
</figure>
</div>
<ol start="2" type="1">
<li>Classifying correctly all of the training can lead to sensitivity to individual observations, returning as a result a overfitted model.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/100-hyperplane-sensitivity.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="support-vector-classifiers-soft-margin-classifier" class="level3" data-number="8.3.2"><h3 data-number="8.3.2" class="anchored" data-anchor-id="support-vector-classifiers-soft-margin-classifier">
<span class="header-section-number">8.3.2</span> Support Vector Classifiers (soft margin classifier)</h3>
<p>To solve this problem we need to allow that:</p>
<ul>
<li>Some observations will be on the incorrect side of the <em>margin</em> like observations 1 and 8.</li>
<li>Some observations will be on the incorrect side of the <em>hyperplane</em> like observations 11 and 12.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/101-soft-margin-classifier.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>To get that result, we need to solve the next problem:</p>
<p><span class="math display">\[
\begin{split}
\underset{\beta_0, \beta_1, \dots, \beta_p, M}{\text{maximize}} &amp; \; M \\
\text{subject to } \sum_{j=1}^p \beta_{j}^2  &amp; = 1 , \\
y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}) &amp; \geq M (1 - \epsilon_i), \\
\epsilon_i \geq 0, \; \sum_{i=1}^n \epsilon_i \leq C,
\end{split}
\]</span></p>
<ul>
<li>Where:
<ul>
<li>
<span class="math inline">\(M\)</span> is the width of the margin.</li>
<li>
<span class="math inline">\(\epsilon_1, \dots, \epsilon_n\)</span> are slack variables that allow individual observations to be on the wrong side of the margin or the hyperplane.
<ul>
<li>If <span class="math inline">\(\epsilon_i = 0\)</span> then the <em>i</em>th observation is on the <strong>correct side of the margin</strong>.</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 0\)</span> then the <em>i</em>th observation is on the <strong>wrong side of the margin</strong>.</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 1\)</span> then the <em>i</em>th observation is on the <strong>wrong side of the hyperplane</strong>.</li>
</ul>
</li>
<li>
<span class="math inline">\(C\)</span> is a nonnegative tuning parameter that represents the <strong>budget for the amount that the margin can be violated</strong> by the n observations. For <span class="math inline">\(C &gt; 0\)</span> no more than <span class="math inline">\(C\)</span> observations can be on the wrong side of the hyperplane.</li>
</ul>
</li>
</ul>
<p>It’s important to point that only observations that either <strong>lie on the margin</strong> or <strong>violate the margin</strong> will <strong>affect the hyperplane</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/102-effects-of-changing-C.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="non-linear-boundaries" class="level3" data-number="8.3.3"><h3 data-number="8.3.3" class="anchored" data-anchor-id="non-linear-boundaries">
<span class="header-section-number">8.3.3</span> Non-linear boundaries</h3>
<p>To extend the <em>Support Vector Classifier</em> to non-lineal settings we need to use functions that quantifies the similarity of two observations, known as <strong>kernels</strong> <span class="math inline">\(K(x_i, x_{i'})\)</span> and implement it to the <strong>hyperplane</strong> function for the <em>support vectors</em> <span class="math inline">\(\mathcal{S}\)</span>.</p>
<p><span class="math display">\[
f(x) = \beta_0 + \sum_{i \in \mathcal{S}} \alpha_i K(x_i, x_{i'})
\]</span> And depending on the shape that we want to use there are 2 types of kernels to use.</p>
<section id="polynomial" class="level4"><h4 class="anchored" data-anchor-id="polynomial">Polynomial</h4>
<p>As the degree <span class="math inline">\(d\)</span> increases the fit becomes more non-linear</p>
<p><span class="math display">\[
K(x_i, x_{i'}) = (1 + \sum_{j=1}^p x_{ij} x_{i'j})^d
\]</span></p>
</section><section id="radial" class="level4"><h4 class="anchored" data-anchor-id="radial">Radial</h4>
<p>As <span class="math inline">\(\gamma\)</span> increases the fit becomes more non-linear.</p>
<p><span class="math display">\[
K(x_i, x_{i'}) = \exp(-\gamma \sum_{j=1}^p(x_{ij}-x_{i'j})^2)
\]</span></p>
</section></section><section id="extending-svms-to-the-k-class-case" class="level3" data-number="8.3.4"><h3 data-number="8.3.4" class="anchored" data-anchor-id="extending-svms-to-the-k-class-case">
<span class="header-section-number">8.3.4</span> Extending SVMs to the K-class case</h3>
<p>To extend this method we have 2 alternatives</p>
<ul>
<li><p><strong>One-Versus-One Classification</strong>: It constructs <span class="math inline">\(\left( \begin{array}{c} K \\ 2 \end{array} \right)\)</span>, tallys the number of times that the test observation is assigned to each of the <span class="math inline">\(K\)</span>. The final classification is performed by assigning the test observation to the class to which it was most frequently assigned.</p></li>
<li><p><strong>One-Versus-All Classification</strong>: We fit <span class="math inline">\(K\)</span> SVMs, each time comparing one of the <span class="math inline">\(K\)</span> classes to the remaining <span class="math inline">\(K − 1\)</span> classes. Let <span class="math inline">\(\beta = \beta_{0k}, \beta_{1k}, \dots, \beta_{pk}\)</span> denote the parameters that result from fitting an SVM comparing the <span class="math inline">\(k\)</span>th class (coded as <span class="math inline">\(+1\)</span>) to the others (coded as <span class="math inline">\(−1\)</span>). We assign the observation to the class for which the lineal combination of coefficients and the test observation <span class="math inline">\(\beta x^*\)</span> is largest.</p></li>
</ul></section><section id="coding-example-2" class="level3" data-number="8.3.5"><h3 data-number="8.3.5" class="anchored" data-anchor-id="coding-example-2">
<span class="header-section-number">8.3.5</span> Coding Example</h3>
<ol type="1">
<li>Load libraries</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">kernlab</span><span class="op">)</span></span>
<span><span class="fu">theme_set</span><span class="op">(</span><span class="fu">theme_light</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">sim_data</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span></span>
<span>  x1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">40</span><span class="op">)</span>,</span>
<span>  x2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">40</span><span class="op">)</span>,</span>
<span>  y  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>x1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">y</span> <span class="op">==</span> <span class="fl">1</span>, <span class="va">x1</span> <span class="op">+</span> <span class="fl">1.5</span>, <span class="va">x1</span><span class="op">)</span>,</span>
<span>         x2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">y</span> <span class="op">==</span> <span class="fl">1</span>, <span class="va">x2</span> <span class="op">+</span> <span class="fl">1.5</span>, <span class="va">x2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="va">sim_data</span>, <span class="fu">aes</span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, color <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="non-parametric-models_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<ol start="2" type="1">
<li>Define model specification</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">svm_linear_spec</span> <span class="op">&lt;-</span> <span class="fu">svm_poly</span><span class="op">(</span>degree <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"classification"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="co"># We don't need scaling for now</span></span>
<span>  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"kernlab"</span>, scaled <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Fitting the model and checking results</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">svm_linear_fit</span> <span class="op">&lt;-</span> <span class="va">svm_linear_spec</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_args</span><span class="op">(</span>cost <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">fit</span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">sim_data</span><span class="op">)</span></span>
<span></span>
<span><span class="va">svm_linear_fit</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">extract_fit_engine</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="non-parametric-models_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./flexible-regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Flexible Regression Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./survival-analysis.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Time-event (Survival) Analysis and Censored Data</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">Non official solitions of An Introduction to Statistical Learning Second Edition</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>


</body></html>