---
format:
  html:
    number-depth: 3
    css: summary-format.css
---

# Unsupervised Learning

It refers to a set of statistical tools intended for the setting in which we have only a set of features $X_1,X_2, \dots ,X_p$ measured on $n$ observations is to discover interesting things about the measurements like:

-   Finding an informative way to visualize and explore the data
-   Imputing missing values
-   Discovering subgroups among the variables or among the observations

As result, the exercise tends to be **more subjective**.

## Principal Components Analysis (PCA)

### Purpose

As the **number of variables increases** checking *two-dimensional scatterplots* **gets less insightful** since they each contain just a small fraction of the total information present in the data set.

For example, if we see correlations between features is easy to create more general categories known as **latent variable** as follow:

- Sandwich
  - cheese - mayonnaise
  - mayonnaise - bread
  - bread - cheese
  - bread -	lettuce
- Soda
  - pepsi - coke
  - 7up - coke
- Vegetables
  - spinach -	broccoli
  - peas - potatoes
  - peas - carrots


At the end this process can help to:

- Reduce the number of featured need to describe the data
- Remove multicollinearity between features

### Grafical description

The **first principal component loading vector** represent the **line** in $p$-dimensional space that is **closest to the** $\mathbf{n}$ **observations**, since that line could provide a good summary of the data.

![](img/74-pca-first-loading-vector.png){fig-align="center"}

But, if we add the **second principal component loading vector** we could span the **closest plane** to the $n$ observations, in terms of **average squared Euclidean distance**, to provide good summary of the data

![](img/75-pca-closest-plane.png){fig-align="center"}


### Mathematical Description

PCA finds a **low-dimensional representation** of a data set that **contains as much variation (information)** as possible. It assumes that all dimensions can be described as a **linear combination** of the $p$ original variables, known as **principal component scores**.

$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1} X_p
$$

Where:

- $\phi_1 = (\phi_{11} \phi_{21} \dots \phi_{p1})^T$ represent the loading vector for the first principal component.
- $\sum_{j=1}^p \phi_{j1}^2 = 1$


To perform a *principal components analysis* (PCA) need to:

1. Make any needed transformation to *tidy the data*.

2. Remove or impute any *missing value*.

3. Transform or variables to be numeric by using method like *one-hot encoding*.

4. **Center** and **scale** all the variables in $\mathbf{X}$  to have mean zero and standard deviation one. This step is important to ensure all variables are on the same scale, particularly if they were measured in different units or have outliers.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
library(data.table)

set.seed(1010)

# For example, we could two variables with different scales
data.table(
  x1 = rnorm(10, mean = 100, 5),
  x2 = rnorm(10, mean = 500, 2)

# Melt all the variable to check how scaling affect each one
)[, melt(.SD, measure.vars = c("x1", "x2"))

# Scale each variable
][, value := scale(value),
  by = "variable"

# Check how the distribution of each variable have changed
][, .(p25 = quantile(value, 0.25) |> round(2),
      mean = mean(value) |> round(2),
      p75 = quantile(value, 0.75) |> round(2)),
  by = "variable"]

```


5. The **loading vector** is determined by solving the following optimization problem using *eigen decomposition*. This identifies the **direction with the largest variance** in the feature space and reveals the **relative contributions of the original features** to the new PCs.

$$
\underset{\phi_{11}, \dots, \phi_{p1}}{\text{maximize}}
\left\{ \frac{1}{n} \sum_{i = 1}^n
\left( \sum_{j=1}^p \phi_{j1} x_j \right)^2 \right\} 
\text{ subject to }
\sum_{j=1}^p \phi_{j1}^2 = 1 .
$$

![](img/29-pca-data-example.png){fig-align="center" width=90% height=90%}

6. Repeat the process until having $\min(n-1, p)$ distinct principal components. Each new component must be orthogonal to all previously computed principal components to ensure that each new component captures a **new direction of variance** assuring a new **uncorrelated** new component.

### Proportion of Variance Explained (PVE)

To know how much of the **information** (variance) in a given data set is **not contained** after projecting the observations onto the **first few principal components** we can use **Proportion of Variance Explained (PVE)** for each component.

$$
\text{PVE}_m = \frac{\overbrace{\frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2 }^\text{Variance explained by the component}}
{\underbrace{\frac{1}{n} \sum_{j=1}^p \sum_{i=1}^n x_{ij}^2}_\text{Total variance present in a data set}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

As result, PVE can be interpret as the $R^2$ of the approximation for $\mathbf{X}$ given by the first $M$ principal components.

![](img/76-pca-pve-charts.png){fig-align="center" width=90% height=90%}

### Coding example

1. Importing useful packages -----

```{r}
library(ggplot2)
theme_set(theme_light())
```


2. Getting the data

```{r}
my_basket <- fread("https://koalaverse.github.io/homlr/data/my_basket.csv")
```


3. Importing library to use

::: {.panel-tabset group="library"}

#### stats

`broom` is really useful to extract model information.

```{r}
library(broom)
```

#### h2o

*Java is a prerequisite for H2O*.

``` {r}
#| output: false

library(h2o)

# turn off progress bars for brevity
h2o.no_progress()  

# connect to H2O instance with 5 gigabytes
h2o.init(max_mem_size = "5g")  

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
```

#### recipes

```{r}
library(recipes)
```


:::

4. Apply the PCA function

::: {.panel-tabset group="library"}

#### stats

```{r}
stats_pca <- prcomp(my_basket, scale = TRUE)
```

#### h2o

- `pca_method`: When your data contains mostly numeric data use **“GramSVD”**, but if the data contain many categorical variables (or just a few categorical variables with high cardinality) we recommend to use **“GLRM”**.
- `k`: Integer specifying how many PCs to compute.Use `ncol(data)`.
- `transform`: Character string specifying how (if at all) your data should be standardized.
- `impute_missing`: Logical specifying whether or not to impute missing values with the **corresponding column mean**.
- `max_runtime_secs`: Number specifying the max run time (in seconds) to limit the runtime for model training.


``` {r}
h2o_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000
)
```

#### recipes

```{r}
pca_rec <- recipe(~., data = my_basket) |>
  step_normalize(all_numeric()) |>
  step_pca(all_numeric(), id = "pca") |>
  prep()
```

:::


5. Extracting importance of each component

::: {.panel-tabset group="library"}

#### stats

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  head(5)
```


#### h2o

``` {r}
h2o_pca@model$importance[, 1:5]
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "variance") |>
  filter(component <= 5,
         terms != "cumulative variance") |>
  tidyr::pivot_wider(id_cols = component,
                     names_from = terms,
                     values_from = value)
```

:::


6. Identifying which of our original features contribute to the PCs by assessing the loadings. 

> Two different software packages will yield the same principal component loading vectors, although **the signs of those loading vectors may differ**.

- PC1

::: {.panel-tabset group="library"}

#### stats

  - PC1 can be interpreted as the **Unhealthy Lifestyle** component, as the higher weights are associated with less healthy behaviors, such as alcohol (bulmers, red.wine, fosters, kronenbourg), sweets (mars, twix, kitkat), tobacco (cigarettes), and potentially gambling (lottery).

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC == 1) |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(column, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


#### h2o

  - PC1 can be interpreted as the **Entertainment Lifestyle** component, as it seems to characterize individuals who might prefer leisure activities over more nutritious food choices.

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature"
  )[order(-pc1)
  ][1:20] |>
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component == "PC1") |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(terms, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


:::

- PC2

::: {.panel-tabset group="library"}

#### stats

  - PC2 can be interpreted as the a **Dine-in Food Choices** component, as  associated items are typically part of a main meal that one might consume for lunch or dinner.

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC == 2) |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(column, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


#### h2o

  - PC2 can be interpreted as the a **Breakfast and News** component, as it appears to characterize individuals who prefer sweet, breakfast, or caffeinated items, perhaps while reading the newspaper. They seem to avoid other types of food and drink.

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature"
  )[order(-pc2)
  ][1:20] |>
  ggplot(aes(pc2, reorder(feature, pc2))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component == "PC2") |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(terms, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

:::

7. Compare PCs against one another to see how the different features contribute to each PC.

::: {.panel-tabset group="library"}

#### stats

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC <= 2) |>
  tidyr::pivot_wider(id_cols = column,
                     names_from = PC,
                     values_from = value) |>
  ggplot(aes(`1`, `2`, label = column)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)+
  labs(x = "Unhealthy Lifestyle",
       y = "Dine-in Food Choices")
```


#### h2o

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature") |>
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)+
  labs(x = "Entertainment Lifestyle",
       y = "Breakfast and News")
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component %chin% paste0("PC",1:2)) |>
  tidyr::pivot_wider(id_cols = terms,
                     names_from = component,
                     values_from = value) |>
  ggplot(aes(PC1, PC2, label = terms)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)
```

:::

### Ways to select fewer components

#### Variance criterion

The **sum of the variance (eigenvalues) of all the components is equal to the number of variables** entered into the PCA. 

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  summarize(total_variance = sum(std.dev^2))
```

A variance of 1 means that the principal component would explain about one variable’s worth of the variability. In that sense we would just be interesting in selecting **components with variance 1 or greater**.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  filter(std.dev^2 >= 1)
```


#### Proportion of variance explained criterion

Depending of the use case the investigator might want to explain a particular proportion of variability. For example, to explain at least 75% of total variability we need to select the first 27 components.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  filter(cumulative >= 0.75) |>
  head(1L)
```


#### Scree plot criterion

The scree plot criterion looks for the “elbow” in the curve and **selects all components just before the line flattens out**, which looks like **8** in our example.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  ggplot(aes(PC, percent, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

#### Conclusion

There is no one best method for determining how many components to use: 

- If we performing an **Exploratory Data Analysis (EDA)** we would probably validate if the first few principal components are interesting and continue to look at subsequent principal components until **no further interesting patterns are found**.

- If we compute principal components for use in a **supervised analysis** we can define number of principal component score vectors as a **tuning parameter** to be selected via **cross-validation**.


## Matrix Completion

### Missing Values

Often datasets have **missing values**, but most of the statistical learning methods **cannot handle them**.

Some possible solutions could be to:

- Remove the rows that contain missing observations
- Replace missing values by the mean of the $j$th column
- Use **matrix completion** to impute missing values by using principal components after **confirming that missingness is at random**.

The good news, it is that we can *impute the missing values* and
*solve the principal component problem at the same time* by following the next approximation Algorithm:

![](img/77-matrix-complition.png){fig-align="center" width=60% height=80%}

After applying this method with with `USArrests` date set with 
**0.63 of correlation**.

![](img/78-missing-value-imputation.png){fig-align="center" height=50%}

### Coding example

Before being able to apply the Algorithm 12.1, we need to:

- Scale and transform the data.frame into a matrix

```{r}
arrests_m <- scale(USArrests) |> as.matrix()
```

- The **singular value decomposition (SVD)** algorithm breaks a matrix into three other matrices, in a way that can be taken back the next function matrix equation $\mathbf{X}= \mathbf{UDV'}$, where:
  - **U**: This is the matrix that transforms the original data into the new dimensional space spanned by the principal components. It represents **the coordinates of the original data after being projected onto these principal components**.
  - **D**: This is a **diagonal matrix** whose elements represent the amount of **variance** captured by each **principal component** in descending order.
  - **V**: This is the transpose of the matrix that represents the principal directions (or **loading vectors**) in the input space, defining how each original feature contributes to each principal component.

```{r}
arrests_svd <- svd(arrests_m)

# To calculate scored vectors
t(arrests_svd$d * t(arrests_svd$u)) |> head()
```

- Omitting 20 values

```{r}
arrests_na <- arrests_m

values_to_omit <- 20L

set.seed(15)

row_na <- 
  nrow(arrests_m) |>
  seq_len() |>
  sample(size = values_to_omit)

col_na <- 
  ncol(arrests_m) |>
  seq_len() |>
  sample(size = values_to_omit, replace = TRUE)

index_na <- cbind(row_na, col_na)

arrests_na[index_na] <- NA_real_

```


#### Manual implementation

1. **Substitute missing** values with the **mean** of each column.

```{r}
# Calculate the mean of each column
arrests_col_mean <- colMeans(arrests_na, na.rm = TRUE)

# Impute the mean of the column on each missing value
# save the result in a new variable
arrests_hat <- arrests_na
arrests_hat[index_na] <- arrests_col_mean[col_na]
```


2. Repeat steps (a)–(c) until the objective (12.14) fails to decrease.

```{r}

# Find a low-rank approximation of the resulting matrix
# It does the same as the next code but faster
# u[, 1:M, drop = FALSE] %*%
#   (if(M == 1L) matrix(d[1L]) else diag(d[1:M]))  %*%
#   t(v[, 1:M, drop = FALSE ])
aproximate_low_rank <- function(X, M = 1) {
  with(
    svd(X) ,
   u[, 1:M, drop = FALSE] %*% (d[1:M] * t(v[, 1:M, drop = FALSE ]))
  )
}

# Create a m x n matrix checking if missing values
# If TRUE then the value is missing
arrests_is_missing <- is.na(arrests_na)

# Mean of the squared non-missing elements
# of the old version of arrests_hat
mssold <-
  scale(arrests_na,
        center = arrests_col_mean,
        scale  = FALSE) |>
  (\(x) x[!arrests_is_missing]^2)() |>
  mean()

# Mean squared error of the non-missing elements
mss0 <- mean(arrests_na[!arrests_is_missing]^2)

iteration_count <- 0L
relative_error <- 1
stop_error <- 1e-7

while(relative_error > stop_error) {
  iteration_count <- iteration_count + 1L
  # Step 2(a)
  Xapp <- aproximate_low_rank(arrests_hat, M = 1)
  # Step 2(b)
  arrests_hat[arrests_is_missing] <- Xapp[arrests_is_missing]
  # Step 2(c)
  mss <- mean(((arrests_na - Xapp)[!arrests_is_missing])^2)
  relative_error <- (mssold - mss) / mss0
  mssold <- mss
  cat("Iter:", iteration_count,
      "MSS:", mss,
      "Rel. Err:", relative_error,"\n")
}

```

3. Return the estimated missing entries

```{r}
Xapp[arrests_is_missing]



# Compute the correlation between the imputed values
cor(
  Xapp[arrests_is_missing],
  arrests_m[arrests_is_missing]
)

```


#### `softImpute` package

```{r}
softImpute::softImpute(arrests_na,
                       rank.max = 1,
                       type = "svd") |>
  softImpute::complete(x = arrests_na) |>
  (\(x) x[arrests_is_missing])() |>
  cor(arrests_m[arrests_is_missing])

```

### Recommender Systems

Image a matrix of $n \times p$ with customer rating with scores from 1 to 5 for each movies they have seen, but on average each customer had seen around 200 movies, so **99% of the matrix had missing elements**

![](img/79-movie-rating-matrix.png){fig-align="center"}

In order **to suggest a movie** that a particular customer might like, we needed to **impute the missing values** of this data matrix.



## Clustering

It refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. Some applications could be to:

- Find few different unknown **subtypes of breast cancer**.
- Perform **market segmentation** by identify **subgroups of people** who might be more likely to purchase a particular product.


### K-means clustering

In *K-means clustering*, we seek to partition the observations into a **pre-specified number of non-overlapping clusters** $K$.


![](img/80-k-mean-clustering-example.png){fig-align="center"}

For this method, the main goal is to classify observations within clusters with **high intra-class similarity** *(low within-cluster variation)*, but with **low inter-class similarity**.


#### Mathematical Description

Let $C_1, \dots, C_K$ denote sets containing the **indices of the observations** in each cluster, where:

- Each observation belongs to at least one of the $K$ clusters. $C_1  \cup C_2 \cup \dots \cup C_K = \{1, \dots,n\}$

- No observation belongs to more than one cluster. $C_k \cap C_{k'} = \emptyset$ for all $k \neq k'$.

$$
\underset{C_1. \dots, C_K}{\text{minimize}} =
\left\{ \sum_{k=1}^k W(C_k) \right\}
$$

$W(C_k)$ represent the amount by which the observations within a cluster differ from each other. There are many possible ways to define this concept, but the most common choice involves **squared euclidean distance**, which is *sensitive to outliers* and works better with **gaussian distributed** features.

$$
W(C_k) = \frac{1}{| C_k|} \sum_{i,i' \in C_k} \sum_{j=1}^p (x_{ij} - x_{i'j})^2
$$
Where:

- $|C_k|$: Denotes the number of observations in the $k$th cluster


##### Distance alternatives

Some alternatives to the euclidean distance more robust to outliers and Non-normal distributions are:
- Manhattan distance
- Minkowski distance
- Gower distance

if you are analyzing unscaled data where observations may have large differences in magnitude but similar behavior then a **correlation-based distance** is preferred like:
- $1 - \text{Pearson correlation}$
- $1 - \text{Spearman correlation}$
- $1 - \text{Kendall Tau correlation}$

![](img/84-k-mean-clustering-scaling-problem.png){fig-align="center"}

#### Aproximation algorithm

As solving this problem would be very difficult, since there are almost $K^n$ ways to partition n observations into $K$ clusters, but we can use a very simple algorithm to find **local optimum**.

![](img/81-K-Means-Clustering-Algorithm.png){fig-align="center"}

![](img/82-K-Means-Clustering-Algorithm-Example.png){fig-align="center"}

Since the results obtained will depend on the **initial (random) cluster assignment** of each observation it is important to **run the algorithm multiple times** (10-20) from different random initial configurations *(random starts)*. Then one selects the solution with the **smallest objective**.

![](img/83-K-Means-Clustering-Algorithm-Example-Selection.png){fig-align="center"}

#### Coding example

To perform k-means clustering on mixed data we need to:

- Convert any ordinal categorical variables to numeric
- Convert nominal categorical variables to one-hot encode
- Scale all variables

#### Selecting the number of clusters

- $k$ **is predetermined** by external resources or knowledge.
- $k = \sqrt{n/2}$ rule of thumb
- **elbow method**
  - Compute k-means clustering for different values of $k$ (1-20)
  - For each  $k$, calculate the total within-cluster sum of squares (WSS).
  - Plot the curve of WSS according to the number of clusters $k$
  - The location of a bend (i.e., elbow) in the plot is generally considered as an indicator of the appropriate number of clusters.

```{r}
#The results show the “elbow” appears to happen when k = 5
factoextra::fviz_nbclust(
  my_basket, 
  kmeans, 
  k.max = 25,
  method = "wss",
  diss = factoextra::get_dist(my_basket, method = "spearman")
)
```

#### Partitioning around medians (PAM)

It has the same algorithmic steps as k-means but uses the **median** rather than the mean to determine the centroid; making it more robust to outliers. 

As your data becomes more sparse the performance of k-means and hierarchical clustering become *slow* and *ineffective*. An alternative is to use the **Gower distance**,  which applies a particular distance calculation that works well for each data type.

- **quantitative (interval)**: range-normalized Manhattan distance.
- **ordinal**: variable is first ranked, then Manhattan distance is used with a special adjustment for ties.
- **nominal**: variables with $k$ categories are first converted into $k$ binary columns (i.e., one-hot encoded) and then the **Dice coefficient** is used. To compute the dice metric for two observations $(X,Y)$ the algorithm looks across all one-hot encoded categorical variables and scores them as:
  - **a** — number of dummies 1 for both observations
  - **b** — number of dummies 1 for $X$ and 0 for $Y$
  - **c** - number of dummies 0 for $X$ and 1 for $Y$
  - **d** — number of dummies 0 for both
and then uses the following formula:

$$
D = \frac{2a}{2a + b +c}
$$

```{r}
# Original data minus Sale_Price
ames_full <- 
  AmesHousing::make_ames() |> 
  subset(select = -Sale_Price)

# Compute Gower distance for original data
gower_dst <- cluster::daisy(ames_full, metric = "gower")

# You can supply the Gower distance matrix to several clustering algos
pam_gower <- cluster::pam(x = gower_dst, k = 8, diss = TRUE)
```


##### clustering large applications (CLARA)

It uses the same algorithmic process as PAM; however, instead of finding the medoids for the entire data set it considers a small sample size and applies k-means or PAM.

```{r}
system.time(kmeans(my_basket, centers = 10))

system.time(cluster::clara(my_basket, k = 10))
```

### Hierarchical clustering

we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a **dendrogram**.

> It allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to n.





