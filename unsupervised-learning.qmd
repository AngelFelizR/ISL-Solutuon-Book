---
format:
  html:
    number-depth: 3
    css: summary-format.css
---

# Unsupervised Learning

It refers to a set of statistical tools intended for the setting in which we have only a set of features $X_1,X_2, \dots ,X_p$ measured on $n$ observations is to discover interesting things about the measurements like:

-   Finding an informative way to visualize and explore the data
-   Imputing missing values
-   Discovering subgroups among the variables or among the observations

As result, the exercise tends to be **more subjective**.

## Principal Components Analysis (PCA)

### Purpose

As the **number of variables increases** checking *two-dimensional scatterplots* **gets less insightful** since they each contain just a small fraction of the total information present in the data set.

For example, if we see correlations between features is easy to create more general categories known as **latent variable** as follow:

- Sandwich
  - cheese - mayonnaise
  - mayonnaise - bread
  - bread - cheese
  - bread -	lettuce
- Soda
  - pepsi - coke
  - 7up - coke
- Vegetables
  - spinach -	broccoli
  - peas - potatoes
  - peas - carrots


At the end this process can help to:

- Reduce the number of featured need to describe the data
- Remove multicollinearity between features

### Grafical description

The **first principal component loading vector** represent the **line** in $p$-dimensional space that is **closest to the** $\mathbf{n}$ **observations**, since that line could provide a good summary of the data.

![](img/74-pca-first-loading-vector.png){fig-align="center"}

But, if we add the **second principal component loading vector** we could span the **closest plane** to the $n$ observations, in terms of **average squared Euclidean distance**, to provide good summary of the data

![](img/75-pca-closest-plane.png){fig-align="center"}


### Mathematical Description

PCA finds a **low-dimensional representation** of a data set that **contains as much variation (information)** as possible. It assumes that all dimensions can be described as a **linear combination** of the $p$ original variables, known as **principal component scores**.

$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1} X_p
$$

Where:

- $\phi_1 = (\phi_{11} \phi_{21} \dots \phi_{p1})^T$ represent the loading vector for the first principal component.
- $\sum_{j=1}^p \phi_{j1}^2 = 1$


To perform a *principal components analysis* (PCA) need to:

(@) Make any needed transformation to *tidy the data*.

(@) Remove or impute any *missing value*.

(@) Transform or variables to be numeric by using method like *one-hot encoding*.

(@) **Center** and **scale** all the variables in $\mathbf{X}$  to have mean zero and standard deviation one. This step is important to ensure all variables are on the same scale, particularly if they were measured in different units or have outliers.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
library(data.table)

set.seed(1010)

# For example, we could two variables with different scales
data.table(
  x1 = rnorm(10, mean = 100, 5),
  x2 = rnorm(10, mean = 500, 2)

# Melt all the variable to check how scaling affect each one
)[, melt(.SD, measure.vars = c("x1", "x2"))

# Scale each variable
][, value := scale(value),
  by = "variable"

# Check how the distribution of each variable have changed
][, .(p25 = quantile(value, 0.25) |> round(2),
      mean = mean(value) |> round(2),
      p75 = quantile(value, 0.75) |> round(2)),
  by = "variable"]

```


(@) The **loading vector** is determined by solving the following optimization problem using *eigen decomposition*. This identifies the **direction with the largest variance** in the feature space and reveals the **relative contributions of the original features** to the new PCs.

$$
\underset{\phi_{11}, \dots, \phi_{p1}}{\text{maximize}}
\left\{ \frac{1}{n} \sum_{i = 1}^n
\left( \sum_{j=1}^p \phi_{j1} x_j \right)^2 \right\} 
\text{ subject to }
\sum_{j=1}^p \phi_{j1}^2 = 1 .
$$

![](img/29-pca-data-example.png){fig-align="center" width=90% height=90%}

(@) Repeat the process until having $\min(n-1, p)$ distinct principal components. Each new component must be orthogonal to all previously computed principal components to ensure that each new component captures a **new direction of variance** assuring a new **uncorrelated** new component.

### Proportion of Variance Explained (PVE)

To know how much of the **information** (variance) in a given data set is **not contained** after projecting the observations onto the **first few principal components** we can use **Proportion of Variance Explained (PVE)** for each component.

$$
\text{PVE}_m = \frac{\overbrace{\frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm} x_{ij} \right)^2 }^\text{Variance explained by the component}}
{\underbrace{\frac{1}{n} \sum_{j=1}^p \sum_{i=1}^n x_{ij}^2}_\text{Total variance present in a data set}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

As result, PVE can be interpret as the $R^2$ of the approximation for $\mathbf{X}$ given by the first $M$ principal components.

![](img/76-pca-pve-charts.png){fig-align="center" width=90% height=90%}

### Coding example

1. Importing useful packages -----

```{r}
library(ggplot2)
theme_set(theme_light())
```


2. Getting the data

```{r}
my_basket <- fread("https://koalaverse.github.io/homlr/data/my_basket.csv")
```


3. Importing library to use

::: {.panel-tabset group="library"}

## stats

`broom` is really useful to extract model information.

```{r}
library(broom)
```

## h2o

*Java is a prerequisite for H2O*.

``` {r}
#| output: false

library(h2o)

# turn off progress bars for brevity
h2o.no_progress()  

# connect to H2O instance with 5 gigabytes
h2o.init(max_mem_size = "5g")  

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
```

## recipes

```{r}
library(recipes)
```


:::

4. Apply the PCA function

::: {.panel-tabset group="library"}

## stats

```{r}
stats_pca <- prcomp(my_basket, scale = TRUE)
```

## h2o

- `pca_method`: When your data contains mostly numeric data use **“GramSVD”**, but if the data contain many categorical variables (or just a few categorical variables with high cardinality) we recommend to use **“GLRM”**.
- `k`: Integer specifying how many PCs to compute.Use `ncol(data)`.
- `transform`: Character string specifying how (if at all) your data should be standardized.
- `impute_missing`: Logical specifying whether or not to impute missing values with the **corresponding column mean**.
- `max_runtime_secs`: Number specifying the max run time (in seconds) to limit the runtime for model training.


``` {r}
h2o_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000
)
```

## recipes

```{r}
pca_rec <- recipe(~., data = my_basket) |>
  step_normalize(all_numeric()) |>
  step_pca(all_numeric(), id = "pca") |>
  prep()
```

:::


5. Extracting importance of each component

::: {.panel-tabset group="library"}

## stats

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  head(5)
```


## h2o

``` {r}
h2o_pca@model$importance[, 1:5]
```

## recipes

```{r}
tidy(pca_rec, id = "pca", type = "variance") |>
  filter(component <= 5,
         terms != "cumulative variance") |>
  tidyr::pivot_wider(id_cols = component,
                     names_from = terms,
                     values_from = value)
```

:::


6. Identifying which of our original features contribute to the PCs by assessing the loadings. 

> Two different software packages will yield the same principal component loading vectors, although **the signs of those loading vectors may differ**.

- PC1

::: {.panel-tabset group="library"}

## stats

  - PC1 can be interpreted as the **Unhealthy Lifestyle** component, as the higher weights are associated with less healthy behaviors, such as alcohol (bulmers, red.wine, fosters, kronenbourg), sweets (mars, twix, kitkat), tobacco (cigarettes), and potentially gambling (lottery).

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC == 1) |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(column, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


## h2o

  - PC1 can be interpreted as the **Entertainment Lifestyle** component, as it seems to characterize individuals who might prefer leisure activities over more nutritious food choices.

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature"
  )[order(-pc1)
  ][1:20] |>
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

## recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component == "PC1") |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(terms, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


:::

- PC2

::: {.panel-tabset group="library"}

## stats

  - PC2 can be interpreted as the a **Dine-in Food Choices** component, as  associated items are typically part of a main meal that one might consume for lunch or dinner.

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC == 2) |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(column, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


## h2o

  - PC2 can be interpreted as the a **Breakfast and News** component, as it appears to characterize individuals who prefer sweet, breakfast, or caffeinated items, perhaps while reading the newspaper. They seem to avoid other types of food and drink.

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature"
  )[order(-pc2)
  ][1:20] |>
  ggplot(aes(pc2, reorder(feature, pc2))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

## recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component == "PC2") |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(terms, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

:::

7. Compare PCs against one another to see how the different features contribute to each PC.

::: {.panel-tabset group="library"}

## stats

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC <= 2) |>
  tidyr::pivot_wider(id_cols = column,
                     names_from = PC,
                     values_from = value) |>
  ggplot(aes(`1`, `2`, label = column)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)+
  labs(x = "Unhealthy Lifestyle",
       y = "Dine-in Food Choices")
```


## h2o

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature") |>
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)+
  labs(x = "Entertainment Lifestyle",
       y = "Breakfast and News")
```

## recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component %chin% paste0("PC",1:2)) |>
  tidyr::pivot_wider(id_cols = terms,
                     names_from = component,
                     values_from = value) |>
  ggplot(aes(PC1, PC2, label = terms)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)
```

:::

### Ways to select fewer components

#### Variance criterion

The **sum of the variance (eigenvalues) of all the components is equal to the number of variables** entered into the PCA. 

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  summarize(total_variance = sum(std.dev^2))
```

A variance of 1 means that the principal component would explain about one variable’s worth of the variability. In that sense we would just be interesting in selecting **components with variance 1 or greater**.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  filter(std.dev^2 >= 1)
```


#### Proportion of variance explained criterion

Depending of the use case the investigator might want to explain a particular proportion of variability. For example, to explain at least 75% of total variability we need to select the first 27 components.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  filter(cumulative >= 0.75) |>
  head(1L)
```


#### Scree plot criterion

The scree plot criterion looks for the “elbow” in the curve and **selects all components just before the line flattens out**, which looks like **8** in our example.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  ggplot(aes(PC, percent, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

#### Conclusion

There is no one best method for determining how many components to use: 

- If we performing an **Exploratory Data Analysis (EDA)** we would probably validate if the first few principal components are interesting and continue to look at subsequent principal components until **no further interesting patterns are found**.

- If we compute principal components for use in a **supervised analysis** we can define number of principal component score vectors as a **tuning parameter** to be selected via **cross-validation**.


## Matrix Completion

### Missing Values

Often datasets have **missing values**, but most of the statistical learning methods **cannot handle them**.

Some possible solutions could be to:

- Remove the rows that contain missing observations
- Replace missing values by the mean of the $j$th column
- Use **matrix completion** to impute missing values by using principal components after **confirming that missingness is at random**.

The good news, it is that we can *impute the missing values* and
*solve the principal component problem at the same time* by following the next approximation Algorithm:

![](img/77-matrix-complition.png){fig-align="center" width=60% height=80%}

After applying this method with with `USArrests` date set with 
**0.63 of correlation**.

![](img/78-missing-value-imputation.png){fig-align="center" height=50%}

### Coding example

Before being able to apply the Algorithm 12.1, we need to:

- Scale and transform the data.frame into a matrix

```{r}
arrests_m <- scale(USArrests) |> as.matrix()
```

- The **singular value decomposition (SVD)** algorithm breaks a matrix into three other matrices, in a way that can be taken back the next function matrix equation $\mathbf{X}= \mathbf{UDV'}$, where:
  - **U**: This is the matrix that transforms the original data into the new dimensional space spanned by the principal components. It represents **the coordinates of the original data after being projected onto these principal components**.
  - **D**: This is a **diagonal matrix** whose elements represent the amount of **variance** captured by each **principal component** in descending order.
  - **V**: This is the transpose of the matrix that represents the principal directions (or **loading vectors**) in the input space, defining how each original feature contributes to each principal component.

```{r}
arrests_svd <- svd(arrests_m)

# To calculate scored vectors
t(arrests_svd$d * t(arrests_svd$u)) |> head()
```

- Omitting 20 values

```{r}
arrests_na <- arrests_m

values_to_omit <- 20L

set.seed(15)

row_na <- 
  nrow(arrests_m) |>
  seq_len() |>
  sample(size = values_to_omit)

col_na <- 
  ncol(arrests_m) |>
  seq_len() |>
  sample(size = values_to_omit, replace = TRUE)

index_na <- cbind(row_na, col_na)

arrests_na[index_na] <- NA_real_

```


#### Manual implementation

1. **Substitute missing** values with the **mean** of each column.

```{r}
# Calculate the mean of each column
arrests_col_mean <- colMeans(arrests_na, na.rm = TRUE)

# Impute the mean of the column on each missing value
# save the result in a new variable
arrests_hat <- arrests_na
arrests_hat[index_na] <- arrests_col_mean[col_na]
```


2. Repeat steps (a)–(c) until the objective (12.14) fails to decrease.

```{r}

# Find a low-rank approximation of the resulting matrix
# It does the same as the next code but faster
# u[, 1:M, drop = FALSE] %*%
#   (if(M == 1L) matrix(d[1L]) else diag(d[1:M]))  %*%
#   t(v[, 1:M, drop = FALSE ])
aproximate_low_rank <- function(X, M = 1) {
  with(
    svd(X) ,
   u[, 1:M, drop = FALSE] %*% (d[1:M] * t(v[, 1:M, drop = FALSE ]))
  )
}

# Create a m x n matrix checking if missing values
# If TRUE then the value is missing
arrests_is_missing <- is.na(arrests_na)

# Mean of the squared non-missing elements
# of the old version of arrests_hat
mssold <-
  scale(arrests_na,
        center = arrests_col_mean,
        scale  = FALSE) |>
  (\(x) x[!arrests_is_missing]^2)() |>
  mean()

# Mean squared error of the non-missing elements
mss0 <- mean(arrests_na[!arrests_is_missing]^2)

iteration_count <- 0L
relative_error <- 1
stop_error <- 1e-7

while(relative_error > stop_error) {
  iteration_count <- iteration_count + 1L
  # Step 2(a)
  Xapp <- aproximate_low_rank(arrests_hat, M = 1)
  # Step 2(b)
  arrests_hat[arrests_is_missing] <- Xapp[arrests_is_missing]
  # Step 2(c)
  mss <- mean(((arrests_na - Xapp)[!arrests_is_missing])^2)
  relative_error <- (mssold - mss) / mss0
  mssold <- mss
  cat("Iter:", iteration_count,
      "MSS:", mss,
      "Rel. Err:", relative_error,"\n")
}

```

3. Return the estimated missing entries

```{r}
Xapp[arrests_is_missing]



# Compute the correlation between the imputed values
cor(
  Xapp[arrests_is_missing],
  arrests_m[arrests_is_missing]
)

```


#### `softImpute` package

```{r}
softImpute::softImpute(arrests_na,
                       rank.max = 1,
                       type = "svd") |>
  softImpute::complete(x = arrests_na) |>
  (\(x) x[arrests_is_missing])() |>
  cor(arrests_m[arrests_is_missing])

```

### Recommender Systems

Image a matrix of $n \times p$ with customer rating with scores from 1 to 5 for each movies they have seen, but on average each customer had seen around 200 movies, so **99% of the matrix had missing elements**

![](img/79-movie-rating-matrix.png){fig-align="center"}

In order **to suggest a movie** that a particular customer might like, we needed to **impute the missing values** of this data matrix.


## Clustering



