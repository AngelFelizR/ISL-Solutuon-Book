---
format:
  html:
    number-depth: 3
    css: summary-format.css
---

# Unsupervised Learning

It refers to a set of statistical tools intended for the setting in which we have only a set of features $X_1,X_2, \dots ,X_p$ measured on $n$ observations is to discover interesting things about the measurements like:

-   Finding an informative way to visualize and explore the data
-   Imputing missing values
-   Discovering subgroups among the variables or among the observations

As result, the exercise tends to be **more subjective**.

## Libraries to use

```{r}
#| output: false

# Helper packages
library(dplyr)       # for data manipulation
library(ggplot2)     # for data visualization
theme_set(theme_light())
library(factoextra)
library(data.table)
library(recipes)
# Really useful to extract model information.
library(broom)

library(h2o)
library(cluster)


```


## Principal Components Analysis (PCA)

### Purpose

As the **number of variables increases** checking *two-dimensional scatterplots* **gets less insightful** since they each contain just a small fraction of the total information present in the data set.

For example, if we see correlations between features is easy to create more general categories known as **latent variable** as follow:

- Sandwich
  - cheese - mayonnaise
  - mayonnaise - bread
  - bread - cheese
  - bread -	lettuce
- Soda
  - pepsi - coke
  - 7up - coke
- Vegetables
  - spinach -	broccoli
  - peas - potatoes
  - peas - carrots


At the end this process can help to:

- Reduce the number of featured need to describe the data
- Remove multicollinearity between features
  

### Mathematical Description

PCA finds a **low-dimensional representation** of a data set that **contains as much variation (information)** as possible. It assumes that all dimensions can be described as a **linear combination** of the $p$ original variables, known as **principal component scores**.

$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1} X_p
$$

Where:

- $\phi_1 = (\phi_{11} \phi_{21} \dots \phi_{p1})^T$ represent the loading vector for the first principal component.
- $\sum_{j=1}^p \phi_{j1}^2 = 1$


To perform a *principal components analysis* (PCA) need to:

(@) Make any needed transformation to *tidy the data*.

(@) Remove or impute any *missing value*.

(@) Transform or variables to be numeric by using method like *one-hot encoding*.

(@) **Center** and **scale** all the variables in $\mathbf{X}$  to have mean zero and standard deviation one. This step is important to ensure all variables are on the same scale, particularly if they were measured in different units or have outliers.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1010)

# For example, we could two variables with different scales
data.table(
  x1 = rnorm(10, mean = 100, 5),
  x2 = rnorm(10, mean = 500, 2)

# Melt all the variable to check how scaling affect each one
)[, melt(.SD, measure.vars = c("x1", "x2"))

# Scale each variable
][, value := scale(value),
  by = "variable"

# Check how the distribution of each variable have changed
][, .(p25 = quantile(value, 0.25) |> round(2),
      mean = mean(value) |> round(2),
      p75 = quantile(value, 0.75) |> round(2)),
  by = "variable"]

```


(@) The **loading vector** is determined by solving the following optimization problem using *eigen decomposition*. This identifies the **direction with the largest variance** in the feature space and reveals the **relative contributions of the original features** to the new PCs.

$$
\underset{\phi_{11}, \dots, \phi_{p1}}{\text{maximize}}
\left\{ \frac{1}{n} \sum_{i = 1}^n
\left( \sum_{j=1}^p \phi_{j1} x_j \right)^2 \right\} 
\text{ subject to }
\sum_{j=1}^p \phi_{j1}^2 = 1 .
$$

![](img/29-pca-data-example.png){fig-align="center" width=90% height=90%}

(@) Repeat the process until having $\min(n-1, p)$ distinct principal components. Each new component must be orthogonal to all previously computed principal components to ensure that each new component captures a **new direction of variance** assuring a new **uncorrelated** new component.


### Coding example

1. Getting the data

```{r}
my_basket <- fread("https://koalaverse.github.io/homlr/data/my_basket.csv")
```


2. Importing library to use

::: {.panel-tabset group="library"}

#### stats



```{r}

```

#### h2o

*Java is a prerequisite for H2O*.

``` {r}


# turn off progress bars for brevity
h2o.no_progress()  

# connect to H2O instance with 5 gigabytes
h2o.init(max_mem_size = "5g")  

# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
```

#### recipes

```{r}

```


:::

4. Apply the PCA function

::: {.panel-tabset group="library"}

#### stats

```{r}
stats_pca <- prcomp(my_basket, scale = TRUE)
```

#### h2o

- `pca_method`: When your data contains mostly numeric data use **“GramSVD”**, but if the data contain many categorical variables (or just a few categorical variables with high cardinality) we recommend to use **“GLRM”**.
- `k`: Integer specifying how many PCs to compute.Use `ncol(data)`.
- `transform`: Character string specifying how (if at all) your data should be standardized.
- `impute_missing`: Logical specifying whether or not to impute missing values with the **corresponding column mean**.
- `max_runtime_secs`: Number specifying the max run time (in seconds) to limit the runtime for model training.


``` {r}
h2o_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000
)

h2o.shutdown(prompt = FALSE)
```

#### recipes

```{r}
pca_rec <- recipe(~., data = my_basket) |>
  step_normalize(all_numeric()) |>
  step_pca(all_numeric(), id = "pca") |>
  prep()
```

:::


5. Extracting importance of each component

::: {.panel-tabset group="library"}

#### stats

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  head(5)
```


#### h2o

``` {r}
h2o_pca@model$importance[, 1:5]
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "variance") |>
  filter(component <= 5,
         terms != "cumulative variance") |>
  tidyr::pivot_wider(id_cols = component,
                     names_from = terms,
                     values_from = value)
```

:::


6. Identifying which of our original features contribute to the PCs by assessing the loadings.

- PC1

::: {.panel-tabset group="library"}

#### stats

  - PC1 can be interpreted as the **Unhealthy Lifestyle** component, as the higher weights are associated with less healthy behaviors, such as alcohol (bulmers, red.wine, fosters, kronenbourg), sweets (mars, twix, kitkat), tobacco (cigarettes), and potentially gambling (lottery).

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC == 1) |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(column, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


#### h2o

  - PC1 can be interpreted as the **Entertainment Lifestyle** component, as it seems to characterize individuals who might prefer leisure activities over more nutritious food choices.

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature"
  )[order(-pc1)
  ][1:20] |>
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component == "PC1") |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(terms, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


:::

- PC2

::: {.panel-tabset group="library"}

#### stats

  - PC2 can be interpreted as the a **Dine-in Food Choices** component, as  associated items are typically part of a main meal that one might consume for lunch or dinner.

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC == 2) |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(column, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```


#### h2o

  - PC2 can be interpreted as the a **Breakfast and News** component, as it appears to characterize individuals who prefer sweet, breakfast, or caffeinated items, perhaps while reading the newspaper. They seem to avoid other types of food and drink.

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature"
  )[order(-pc2)
  ][1:20] |>
  ggplot(aes(pc2, reorder(feature, pc2))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component == "PC2") |>
  arrange(-value) |>
  head(20) |>
  ggplot(aes(value, reorder(terms, value))) +
  geom_point()+
  geom_vline(xintercept = 0)
```

:::

7. Compare PCs against one another to see how the different features contribute to each PC.

::: {.panel-tabset group="library"}

#### stats

```{r}
tidy(stats_pca, matrix = "loadings") |>
  filter(PC <= 2) |>
  tidyr::pivot_wider(id_cols = column,
                     names_from = PC,
                     values_from = value) |>
  ggplot(aes(`1`, `2`, label = column)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)+
  labs(x = "Unhealthy Lifestyle",
       y = "Dine-in Food Choices")
```


#### h2o

``` {r}
as.data.table(h2o_pca@model$eigenvectors,
              keep.rownames = "feature") |>
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)+
  labs(x = "Entertainment Lifestyle",
       y = "Breakfast and News")
```

#### recipes

```{r}
tidy(pca_rec, id = "pca", type = "coef") |>
  filter(component %chin% paste0("PC",1:2)) |>
  tidyr::pivot_wider(id_cols = terms,
                     names_from = component,
                     values_from = value) |>
  ggplot(aes(PC1, PC2, label = terms)) +
  geom_text(check_overlap = TRUE)+
  geom_vline(xintercept = 0)+
  geom_hline(yintercept = 0)
```

:::

### Ways to select fewer components

#### Variance criterion

The **sum of the variance (eigenvalues) of all the components is equal to the number of variables** entered into the PCA. 

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  summarize(total_variance = sum(std.dev^2))
```

A variance of 1 means that the principal component would explain about one variable’s worth of the variability. In that sense we would just be interesting in selecting **components with variance 1 or greater**.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  filter(std.dev^2 >= 1)
```


#### Proportion of variance explained criterion

Depending of the use case the investigator might want to explain a particular proportion of variability. For example, to explain at least 75% of total variability we need to select the first 27 components.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  filter(cumulative >= 0.75) |>
  head(1L)
```


#### Scree plot criterion

The scree plot criterion looks for the “elbow” in the curve and **selects all components just before the line flattens out**, which looks like **8** in our example.

```{r}
tidy(stats_pca, matrix = "eigenvalues") |>
  ggplot(aes(PC, percent, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

#### Conclusion

The frank answer is that **there is no one best method for determining how many components to use**. If we were merely trying to profile customers we would probably use 8 or 10, if we were performing dimension reduction to feed into a downstream predictive model we would likely retain 26 or more based on cross-validation.

## Clustering

It refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. Some applications could be to:

- Find few different unknown **subtypes of breast cancer**.
- Perform **market segmentation** by identify **subgroups of people** who might be more likely to purchase a particular product.


### K-means clustering

In *K-means clustering*, we seek to partition the observations into a **pre-specified number of non-overlapping clusters** $K$.


![](img/80-k-mean-clustering-example.png){fig-align="center"}

For this method, the main goal is to classify observations within clusters with **high intra-class similarity** *(low within-cluster variation)*, but with **low inter-class similarity**.


#### Mathematical Description

Let $C_1, \dots, C_K$ denote sets containing the **indices of the observations** in each cluster, where:

- Each observation belongs to at least one of the $K$ clusters. $C_1  \cup C_2 \cup \dots \cup C_K = \{1, \dots,n\}$

- No observation belongs to more than one cluster. $C_k \cap C_{k'} = \emptyset$ for all $k \neq k'$.

$$
\underset{C_1. \dots, C_K}{\text{minimize}} =
\left\{ \sum_{k=1}^k W(C_k) \right\}
$$

$W(C_k)$ represent the amount by which the observations within a cluster differ from each other. There are many possible ways to define this concept, but the most common choice involves **squared euclidean distance**, which is *sensitive to outliers* and works better with **gaussian distributed** features.

$$
W(C_k) = \frac{1}{| C_k|} \sum_{i,i' \in C_k} \sum_{j=1}^p (x_{ij} - x_{i'j})^2
$$
Where:

- $|C_k|$: Denotes the number of observations in the $k$th cluster


##### Distance alternatives

Some alternatives to the euclidean distance more robust to outliers and Non-normal distributions are:

- Manhattan distance
- Minkowski distance
- Gower distance

if you are analyzing unscaled data where observations may have large differences in magnitude but similar behavior then a **correlation-based distance** is preferred like:

- $1 - \text{Pearson correlation}$
- $1 - \text{Spearman correlation}$
- $1 - \text{Kendall Tau correlation}$

![](img/84-k-mean-clustering-scaling-problem.png){fig-align="center"}

#### Aproximation algorithm

As solving this problem would be very difficult, since there are almost $K^n$ ways to partition n observations into $K$ clusters, but we can use a very simple algorithm to find **local optimum**.

![](img/81-K-Means-Clustering-Algorithm.png){fig-align="center"}

![](img/82-K-Means-Clustering-Algorithm-Example.png){fig-align="center"}

Since the results obtained will depend on the **initial (random) cluster assignment** of each observation it is important to **run the algorithm multiple times** (10-20) from different random initial configurations *(random starts)*. Then one selects the solution with the **smallest objective**.

![](img/83-K-Means-Clustering-Algorithm-Example-Selection.png){fig-align="center"}

#### Coding example

To perform k-means clustering on mixed data we need to:

- Convert any ordinal categorical variables to numeric
- Convert nominal categorical variables to one-hot encode
- Scale all variables

#### Selecting the number of clusters

- $k$ **is predetermined** by external resources or knowledge.
- $k = \sqrt{n/2}$ rule of thumb
- **elbow method**
  - Compute k-means clustering for different values of $k$ (1-20)
  - For each  $k$, calculate the total within-cluster sum of squares (WSS).
  - Plot the curve of WSS according to the number of clusters $k$
  - The location of a bend (i.e., elbow) in the plot is generally considered as an indicator of the appropriate number of clusters.

```{r}
my_basket <- fread("https://koalaverse.github.io/homlr/data/my_basket.csv")

#The results show the “elbow” appears to happen when k = 5
fviz_nbclust(
  my_basket, 
  kmeans, 
  k.max = 25,
  method = "wss",
  diss = get_dist(my_basket, method = "spearman")
)
```

#### Partitioning around medians (PAM)

It has the same algorithmic steps as k-means but uses the **median** rather than the mean to determine the centroid; making it more robust to outliers. 

As your data becomes more sparse the performance of k-means and hierarchical clustering become *slow* and *ineffective*. An alternative is to use the **Gower distance**,  which applies a particular distance calculation that works well for each data type.

- **quantitative (interval)**: range-normalized Manhattan distance.
- **ordinal**: variable is first ranked, then Manhattan distance is used with a special adjustment for ties.
- **nominal**: variables with $k$ categories are first converted into $k$ binary columns (i.e., one-hot encoded) and then the **Dice coefficient** is used. To compute the dice metric for two observations $(X,Y)$ the algorithm looks across all one-hot encoded categorical variables and scores them as:
  - **a** — number of dummies 1 for both observations
  - **b** — number of dummies 1 for $X$ and 0 for $Y$
  - **c** - number of dummies 0 for $X$ and 1 for $Y$
  - **d** — number of dummies 0 for both
and then uses the following formula:

$$
D = \frac{2a}{2a + b +c}
$$

```{r}
# Original data minus Sale_Price
ames_full <- 
  AmesHousing::make_ames() |> 
  subset(select = -Sale_Price)

# Compute Gower distance for original data
gower_dst <- daisy(ames_full, metric = "gower")

# You can supply the Gower distance matrix to several clustering algos
pam_gower <- pam(x = gower_dst, k = 8, diss = TRUE)
```


##### clustering large applications (CLARA)

It uses the same algorithmic process as PAM; however, instead of finding the medoids for the entire data set it considers a small sample size and applies k-means or PAM.

```{r}
system.time(kmeans(my_basket, centers = 10))

system.time(clara(my_basket, k = 10))
```


### Hierarchical clustering

- It doesn't require to define the number of clusters.
- It returns an attractive tree-based representation (**dendrogram**).

> It assumes that clusters are nested, but that isn't true k-means clustering coud yield better.


#### Understanding dendrograms

In general we can say that:

- Each **leaf** represent an observation
- **Similar** the groups of observations are **lower** in the tree
- **Different** the groups of observations are near the **top** of the tree
- The **height of the cut** controls the **number of clusters** obtained.

![](img/85-hierarchical-clustering-clusters.png){fig-align="center"}

In the next example:

- {1,6} and {5,7} are close observations

![](img/86-hierarchical-clustering-simularity-example1.png){fig-align="center"}

- Observation 9 is no more similar to observation 2 than it
is to observations 8, 5, and 7, as it was **fused at higher height of the cut**.

![](img/87-hierarchical-clustering-9-similarity.png){fig-align="center"}


#### Linkage

It measures the dissimilarity between two clusters. To do so we have the next methods:

- AGNES clustering
  - **Complete (maximal intercluster dissimilarity)**: Record the **largest dissimilarity** between cluster $A$ and $B$. It tends to produce more **compact clusters**.
  - **Ward’s minimum variance**: Minimizes the total within-cluster variance. At each step the pair of clusters with the smallest between-cluster distance are merged. Tends to **produce more compact clusters**.

- DIANA clustering
  - **Average (mean intercluster dissimilarity)**: Record the **average dissimilarity** between cluster $A$ and $B$. It can **vary** in the compactness of the clusters it creates.

- **Single (minimal intercluster dissimilarity)**: Record the **smallest dissimilarity** between cluster $A$ and $B$. It tends to produce more **extended clusters**.

- **Centroid**: Computes the dissimilarity between the centroid for cluster $A$ (a mean vector of length $p$, one element for each variable) and the centroid for cluster $B$. It is often used in genomics, but *inversions* can lead to **difficulties** in visualizing and interpreting of the dendrogram.

![](img/92-hierarchical-clustering-linkage-examples.png){fig-align="center"}

#### Hierarchical Clustering Types 

![AGNES (bottom-up) versus DIANA (top-down) clustering](img/93-hierarchical-clustering-types.png){fig-align="center"}

##### Agglomerative Clustering (AGNES, Bottom-up)

1. Defining a **dissimilarity measure** between
each pair of observations, like **Euclidean distance** and **correlation-based distance**.
2. Defining each of the $n$ observations as a *cluster*.
3. **Fusing the most similar 2 clusters** and repeating the process until all the observations belong to **one single cluster**.

- Step 1

![](img/88-hierarchical-clustering-step1.png){fig-align="center"}

- Step 2

![](img/89-hierarchical-clustering-step2.png){fig-align="center"}

- Step 3

![](img/90-hierarchical-clustering-step3.png){fig-align="center"}

> It is good at identifying **small** clusters.

###### Coding example

```{r}
ames_scale <- AmesHousing::make_ames() |>
  select_if(is.numeric) |>  # select numeric columns
  select(-Sale_Price) |>    # remove target column
  mutate_all(as.double) |>  # coerce to double type
  scale()                   # center & scale the resulting columns

# For reproducibility
set.seed(123)

# Dissimilarity matrix
d <- dist(
  ames_scale,
  method = "euclidean"
)

cluster_methods <- c(
  "average",
  "single",
  "complete",
  "ward.D"
)

setattr(cluster_methods,
        "names",
        cluster_methods)

sapply(cluster_methods,
       \(x) fastcluster::hclust(d, method = x) |>
         coef.hclust() ) |> 
  sort(decreasing = TRUE)

```

##### Divisive Clustering (DIANA, top-down)

1. Defining a **dissimilarity measure** between
each pair of observations, like **Euclidean distance** and **correlation-based distance**.
2. Defining the root, in which all observations are included in a single cluster. 
3. The current cluster is split into two clusters that are considered most heterogeneous. The process is iterated until all observations are in their own cluster.

> It is good at identifying **large** clusters.

#### Defining number of clusters

```{r}
p1 <- fviz_nbclust(
  ames_scale,
  FUN = hcut,
  method = "wss",
  k.max = 10
) +
  ggtitle("(A) Elbow method")


p2 <- fviz_nbclust(
  ames_scale,
  FUN = hcut,
  method = "silhouette",
  k.max = 10
) +
  ggtitle("(B) Silhouette method")


# Display plots side by side
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

####  Working with dendrograms

https://uc-r.github.io/hc_clustering

