---
format:
  html:
    css: summary-format.css
---

# Flexible Regression Models {.unnumbered}

In this book the main goal is to provide more tools to explore the relation of our response and a single predictor, that could be useful when we are are performing an **EDA** and the extend that power using **generalized additive models**.

## Based on linear regression

### Polynomial regression 

It extends the *linear model* by adding extra predictors, obtained by **raising each of the original predictors to a power**. 

As result, if the response is a numeric variable we can fit our model to follow the next form:

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d + \epsilon_i
$$


![](img/35-polynomial-4degree-regression.png){fig-align="center"}

On the other hand, we can use the *logistic regression* and apply the same structure to predict the probability of particular class:

$$
\Pr(y_i > 250|x_i) = \frac{\exp(\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d)}
                          {1 + \exp(\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d)}
$$

![](img/36-polynomial-4degree-logistic-regression.png){fig-align="center"}

### Piecewise constant regression

They **cut the range of a variable into K distinct regions** (known as *bins*) in order to produce a qualitative variable. This has the eﬀect of ﬁtting a piecewise constant function.

If we define the cutpoints as $c_1, c_2, \dots, c_K$ in the range of *X*, we can create *dummy variables* to represent each range. For example, if $c_1 \leq x_i < c_2$ is `TRUE` then $C_1(x_i) = 1$ and then we need to repeat that process for each value of $X$ and range. As result we can fit a *lineal regression* based on the new variables.

$$
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i)  \dots + \beta_K C_K(x_i) + \epsilon_i
$$

![](img/37-step-function-regression.png){fig-align="center"}

On the other hand, we can use the *logistic regression* and apply the same structure to predict the probability of particular class:

$$
\Pr(y_i > 250|x_i) = \frac{\exp(\beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i)  \dots + \beta_K C_K(x_i)}
                          {1 + \exp(\beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i)  \dots + \beta_K C_K(x_i))}
$$

![](img/38-step-function-logistic-regression.png){fig-align="center"}

### Piecewise polynomials regression

It consist in ﬁtting separate low-degree polynomials over diﬀerent regions of *X*. For example, a **piecewise cubic polynomial** with a single knot at a point *c* takes the form.

$$
y_i =
  \begin{cases}
    \beta_{01} + \beta_{11} x_i + \beta_{21} x_i^2 + \beta_{31} x_i^3 + \epsilon_i &  \text{if } x_i<c\\
    \beta_{02} + \beta_{12} x_i + \beta_{22} x_i^2 + \beta_{32} x_i^3 + \epsilon_i &  \text{if } x_i \geq c
  \end{cases}
$$

As each polynomial has four parameters, we are using a total of **8 degrees of freedom** in ﬁtting that model. By using that model with `Wage`data, we can see a problem as the model used was **too flexible** and to solve it we need to **constrain** it to be continuous at `age = 50`. 

![](img/39-piecewise-cubic-example.png){fig-align="center"}
But as you could see after applying the **continuity constraint** the plot still present an unnatural V-shape that can solve by apply the **continuity constraint** to the *first* and *second* derivative of the function, the end with **5 degrees of freedom** model.

![](img/40-constrained-piecewise-cubic-example.png){fig-align="center"}
 
In this context, a **natural spline** refers to a *regression spline* with the additional constraints of maintaining **linearity at the boundaries**. 


## Smoothing splines

They arise as a result of minimizing a residual sum of squares criterion subject to a smoothness penalty.

In this method rather than  trying to minimize $\text{RSS} = \sum_{i=1}^n (y_i - g(x_i))^2$, we try to find a function $g(x)$, known as *smoothing spline*, which could minimize the following expression based on the $\lambda$ *nonnegative tuning parameter*.

$$
\underbrace{\sum_{i=1}^n (y_i - g(x_i))^2 }_{\text{loss function (data fitting)}} + 
\underbrace{\lambda \int g''(t)^2dt}_{\text{penalty term (g varibility)}}
$$

The second derivative of $g(t)$ measure how **wiggly** is the function near $t$, where its value is $0$ when the function is a *straight line* as a line is *perfectly smooth*. The we can use *integral* to get total change in the function $g'(t)$, over its *entire range*. As consequence, **the larger the value of** $\mathbf{\lambda}$ **, the smoother** $\mathbf{g}$ **will be**.


::: {.callout-note}
The function $g(x)$ is a **natural cubic spline** with knots at $x_1, \dots ,x_n$
:::


## Local regression

They arise as a result of minimizing a residual sum of squares criterion subject to a smoothness penalty and the possibility that the regions defined can overlap others.


